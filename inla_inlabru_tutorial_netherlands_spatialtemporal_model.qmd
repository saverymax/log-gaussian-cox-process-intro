
# Spatialtemporal modelling with INLA and SPDE

TODO: Started writing this on January 15th, but compied it from inla_inlabru_tutorial_netherlands_data.qmd.

In the previous post only a spatial model was fit. Now, let's expand that to a spatialtemporal model.

That means we need covariate data with a spatial extent and temporal extent. The response data, air pollution, is already spatialtemporal in nature, we were just using a single time point to reduce the temporal dimension. 

So we need some spatial-temporal covariate data if we are to model a spatial-temporal response. @wang-2023 use temperature, precipitation and vapour pressure (though reduced to monthly intervals), elevation, and population density. @cameletti-2013 use daily mean wind speed, daily maximum mixing height, daily precipitation, daily temperature, daily emissions, altitude, and spatial coordinates. @shetty-2024 uses even more covariates including the NO2 column from Tropomi, Night light radiance and NDVI. @Belconi-2021 and @Belconi-2018 have even more. There are probably a lot of these papers. Distance to roads and sea are interesting ones.

Some links for that I've tracked down for the NO2 tropomi data:

These are for processing Sentinel5p
1. <https://www.earthdata.nasa.gov/news/feature-articles/how-find-visualize-nitrogen-dioxide-satellite-data>
2. <https://disc.gsfc.nasa.gov/datasets/S5P_L2__NO2____HiR_2/summary>
3. <https://daac.gsfc.nasa.gov/information/howto?title=How%20to%20Subset%20Level-2%20Data>
4. <https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20Access%20GES%20DISC%20Data%20Using%20wget%20and%20curl>
5. There is the copernicus browser: <https://dataspace.copernicus.eu/> and more info at the sentinel site: <https://sentinels.copernicus.eu/web/sentinel/home> and <https://sentiwiki.copernicus.eu/web/s5p-mission>.

And a good resource for learning about stars: <https://tmieno2.github.io/R-as-GIS-for-Economists-Quarto/>

Some other MODIS products:
1. <https://modis.gsfc.nasa.gov/data/>
2. <https://modis.gsfc.nasa.gov/data/dataprod/index.php#atmosphere>

$$
Y_i \sim N(u_i, \sigma^2)\\
u_i = \beta_0\cdot\text{temp} + \beta_1\cdot\text{precipitation}_i + S(x_i).
$$
So it's a typical Gaussian distributed variable with underlying latent structure. I'm not sure what the most accurate statistical way to say this is; potentially a spatial random effect "modelled as a Gaussian process". This is what Paula Moraga calls "geostatistical data", that is, measurements taken at discrete locations but used to describe or estimate a spatially continuous process, such as air pollution. It is very similar to a mixed model with a random effect. In this case the random effect is Gaussian distributed and spatially indexed. 

It's a simple model, so let's get straight to data downloading and processing. I download air pollution data in the Netherlands for the year of 2023 from <https://eeadmz1-downloads-webapp.azurewebsites.net/>.
```{r set-up}
library(INLA)
library(inlabru)
library(sf)
library(rnaturalearth)
library(ggplot2)
library(viridis)
library(terra)
library(arrow)
library(dplyr)
library(stringr)
library(readr)
library(exactextractr)
#data_path <- file.path("D:", "data", "air_quality_data", "belgium_eea", "E1a")
data_path <- file.path("D:", "data", "air_quality_data", "netherlands_eea", "E1a")
d <- open_dataset(data_path)
d
# 8	Nitrogen dioxide
# 5	Particulate matter < 10 µm
# 6001	Particulate matter < 2.5 µm
d %>% group_by(Samplingpoint) |> filter(Pollutant==6001, Value>-1) |> collect() -> df
head(df)
```
Add locations to the sampling stations.
```{r processing}
df <- df %>%
   #mutate(Samplingpoint = str_remove(Samplingpoint, "BE/"))
   mutate(Samplingpoint = str_remove(Samplingpoint, "NL/"))
#station_location_path <- file.path("D:", "data", "air_quality_data", "eea_stations_2023", "belgium_stations_2023.csv")
station_location_path <- file.path("D:", "data", "air_quality_data", "eea_stations_2023", "nl_stations_2023.csv")
station_locations <- read_csv(station_location_path)
station_locations <- rename(station_locations, Samplingpoint = 'Sampling Point Id')
station_locations %>% 
  distinct(Samplingpoint, Longitude, Latitude) -> unique_locations
unique_locations
merged_df <- df %>% 
  left_join(unique_locations %>% select(Samplingpoint, Longitude, Latitude), by = "Samplingpoint")
merged_df
station_locations_sf <- st_as_sf(station_locations, coords = c("Longitude", "Latitude"))
air_sf <- st_as_sf(merged_df, coords = c("Longitude", "Latitude"))
st_crs(air_sf) <- "EPSG:4326"

# Let's also select the amount of time that we will be working with
air_sf <- air_sf %>% filter(Start<=as.POSIXct("2023-01-15"))
head(air_sf)
```
Then examine the time series
```{r plot-time-series}
distinct_stations <- unique(air_sf$Samplingpoint)[1:12]
distinct_stations
plotting_stations <- air_sf[air_sf$Samplingpoint%in%distinct_stations,]
plotting_stations
plotting_stations %>% 
  ggplot(aes(x=Start, y=Value)) + 
    geom_line() + 
    facet_wrap(~ Samplingpoint) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Time series of monitoring stations", x = "Time, hourly", y = "Conc (μg/m³)")
```
It ends up being practical to model the daily time series, even if hourly time is more useful.

One question for my overall work is if working at an hourly resolution is useful, and then I can easily limit the time that goes into my model, and perhaps use day just as a covariate. On the other hand, I can work at the daily resolution, but how useful are predictions at the resolution?
```{r agg-time}
# TODO: 1-23-2025 debugging: Using floor_date works to fit the model with a subset of the data
# but once I do the grouping operation it does not work
#air_sf %>% mutate(date=lubridate::floor_date(Start, "day"))
# This is working
#air_sf %>% mutate(date=lubridate::floor_date(Start, "day")) -> daily_air_sf #%>% group_by(date, Samplingpoint) -> daily_air_sf
# This is what I want to work:
air_sf %>% mutate(date=lubridate::floor_date(Start, "day")) %>% group_by(Samplingpoint, date) %>% 
              summarise(Value=mean(Value)) -> daily_air_sf
daily_air_sf

#daily_air_sf <- air_sf
# Don't need this with the above "working" code except to change class to date
#daily_air_sf$date <- as.Date(daily_air_sf$Start)
plotting_stations <- daily_air_sf[daily_air_sf$Samplingpoint%in%distinct_stations[c(1:2, 4)],]
plotting_stations
plotting_stations %>% 
  ggplot(aes(x=date, y=Value)) + 
    geom_line() + 
    facet_wrap(~ Samplingpoint) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Time series of monitoring stations", x = "Time, daily", y = "Conc (μg/m³)")
```
Not all stations have measurements for all days. Let's filter out the stations that are missing some days.

```{r filter-stations}
station_counts <- daily_air_sf %>% 
  group_by(Samplingpoint) %>% 
  summarise(n_obs = n())
station_counts

most_frequent_n <-  station_counts %>% 
  count(n_obs) %>% 
  slice_max(n, n = 1) %>% 
  pull(n_obs)
most_frequent_n

filtered_stations <- station_counts %>% 
  filter(n_obs == most_frequent_n) %>% 
  pull(Samplingpoint)
filtered_stations

daily_air_sf <- daily_air_sf %>% 
  filter(Samplingpoint %in% filtered_stations) 

dim(daily_air_sf)

distinct_stations_filtered <- unique(daily_air_sf$Samplingpoint)[1:15]
plotting_stations <- daily_air_sf[daily_air_sf$Samplingpoint%in%distinct_stations_filtered,]
plotting_stations %>% 
  ggplot(aes(x=date, y=Value)) + 
    geom_line() + 
    facet_wrap(~ Samplingpoint) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Time series of monitoring stations", x = "Time, daily", y = "Conc (μg/m³)")
```
Then let's load a map of the Netherlands. The border is from <https://service.pdok.nl/kadaster/bestuurlijkegrenzen/atom/bestuurlijke_grenzen.xml>.

```{r process-maps}
map <- st_read(file.path("D:", "data", "maps", "netherlands_bestuurlijkegrenzen_2021", "bestuurlijkegrenzen.gpkg"))
map <- st_union(map)
map <- st_as_sf(map)
map <- st_transform(map, crs = st_crs(daily_air_sf))
# raster grid covering map
grid <- terra::rast(map, nrows = 10, ncols = 10)
grid
# coordinates of all cells
xy <- terra::xyFromCell(grid, 1:ncell(grid))
# transform points to a sf object
dp <- st_as_sf(as.data.frame(xy), coords = c("x", "y"),
                 crs = st_crs(map))

# indices points within the map
indicespointswithin <- which(st_intersects(dp, map,
                                           sparse = FALSE))
length(indicespointswithin)

# points within the map
dp <- st_filter(dp, map)

# plot
ggplot() + geom_sf(data = map) +
  geom_sf(data = dp)
```
Next let's load the spatio-temporal covariates
```{r no2-load}
nc_dir <- file.path("D:", "data", "air_quality_data",  
                      "sentinel_download",  "processed_no2_nl.nc")
no2_rast <- terra::rast(nc_dir)
plot(no2_rast)
no2_rast
```
We need to prep it for the INLA model. I need to extract the raster values for every station location and time point.
# Would be very helpful to first get the station ids and add into loc_sf 
```{r rast-process}
all_stations <- daily_air_sf %>% select(Samplingpoint)
loc_sf <- unique(all_stations)
# We are also only going to use data for the period of January
#air_sf$date <- as.Date(air_sf$Start) 
daily_air_sf <- daily_air_sf %>% filter(date<as.Date("2023-02-01"))

ggplot() +
  geom_sf(data = map, color=alpha("white",0.9)) +
  geom_sf(data=loc_sf, color=alpha("orange", 0.3)) +
  labs(title = "NL Monitoring Stations")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme_minimal()
nlyr(no2_rast)
no2_rast[[1]]
extracted_values <- lapply(1:nlyr(no2_rast), function(i) {
  raster_layer <- no2_rast[[i]]
  terra::extract(raster_layer, loc_sf, 'mean') 
})
extracted_values[[1]]

# 1. Add the date as a column to each dataframe
for (i in 1:length(extracted_values)) {
  extracted_values[[i]]$station_id <- loc_sf$Samplingpoint
}
extracted_values[[1]]
library(reshape2)
time_values <- terra::time(no2_rast)
time_values
names(extracted_values) <- time_values
melted_df <- melt(extracted_values, id.vars = c("ID", "station_id"))
melted_df
# 3. Clean up the column names
names(melted_df) <- c("ID", "Samplingpoint", "variable", "no2_column", "date")
melted_df$date <- as.Date(melted_df$date)
colnames(melted_df)
colnames(daily_air_sf)
sum(is.na(melted_df$no2_column))
# Then merge this with the air_sf
air_sf_joined <- left_join(daily_air_sf, melted_df %>% select(Samplingpoint, date, no2_column), by = c("Samplingpoint", "date"))
sum(is.na(air_sf_joined$no2_column))
sum(is.na(air_sf_joined))
air_sf_joined
```

```{r convert-mol}
air_sf_joined$no2_column_ugm3 <- air_sf_joined$no2_column * 46.0055 * 1000
sum(is.na(air_sf_joined$no2_column))
```
Hmm, how different are the stations per day? They are not exactly the same, but only a few change per day out of 53 total.
```{r check-merge}
station_list <- list()
as.Date(air_sf_joined$date)
air_sf_joined[which(as.Date(air_sf_joined$date)=="2023-01-04"),]

for (di in seq_along(time_values)){
  print(time_values[di])
  single_day <- air_sf_joined %>% filter(date==as.Date(time_values[di]))
  station_list[[di]] <- unique(single_day$Samplingpoint)
  p <- ggplot() + geom_sf(data = map) +
    geom_sf(data = single_day, aes(col = no2_column_ugm3)) +
    scale_color_viridis()
  print(p)
}
station_list
# Missing some stations per day.
all(sapply(station_list, identical, station_list[[1]]))

#library(gganimate)
#p <- ggplot() +
#  geom_sf(data = map, color=alpha("black",0.9),linewidth=1.5, fill='transparent') +
#  geom_sf(data = air_sf_joined, aes(col = no2_column_ugm3)) +
#  scale_fill_viridis() +
#  coord_sf() +
#  labs(title = "Tropospheric NO2 Column - {date}",  # Dynamic title
#       x = "Longitude",
#       y = "Latitude") +
#  transition_states(
#    date, transition_length = 2, state_length = 1
#  ) +
#  enter_fade() +
#  exit_fade() +
#  ease_aes('linear')
#animate(p, renderer = gifski_renderer())
```
More data process
```{r process-data-2}
library(geodata)
# With geodata library
save_path <- file.path("D:", "data", "air_quality_data", "aux_variables")
covtemp <- worldclim_global(var = "tavg", res = 10,
                            path = save_path)
covprec <- worldclim_global(var = "prec", res = 10,
                            path = save_path)
# Extract at observed locations
air_sf_joined$covtemp <- extract(mean(covtemp), st_coordinates(air_sf_joined))[, 1]
air_sf_joined$covprec <- extract(mean(covprec), st_coordinates(air_sf_joined))[, 1]
# Extract at prediction locations
dp$covtemp <- extract(mean(covtemp), st_coordinates(dp))[, 1]
dp$covprec <- extract(mean(covprec), st_coordinates(dp))[, 1]

ggplot() + geom_sf(data = map) +
  geom_sf(data = air_sf_joined, aes(col = Value)) +
  scale_color_viridis()
ggplot() + geom_sf(data = map) +
  geom_sf(data = air_sf_joined, aes(col = covtemp)) +
  scale_color_viridis()
ggplot() + geom_sf(data = map) +
  geom_sf(data = air_sf_joined, aes(col = covprec)) +
  scale_color_viridis()
ggplot() + geom_sf(data = map) +
  geom_sf(data = dp, aes(col = covtemp)) +
  scale_color_viridis()
ggplot() + geom_sf(data = map) +
  geom_sf(data = dp, aes(col = covprec)) +
  scale_color_viridis()
```
Then do the extraction for prediction points

TODO: Oh, wait, this only makes sense using future time points. So then, I need to split 
my dataset of get future data. Let's split it
```{r extract-for-pred}
extracted_pred_values <- lapply(1:nlyr(no2_rast), function(i) {
  raster_layer <- no2_rast[[i]]
  terra::extract(raster_layer, dp, 'mean') 
})
extracted_pred_values[[1]]

```

Looks like there is a couple of nans in there. Let's impute using the mean.
```{r imputation}
dp <- dp %>% mutate(covprec=ifelse(is.na(covprec), mean(covprec, na.rm=TRUE), covprec),
                    covtemp=ifelse(is.na(covtemp), mean(covtemp, na.rm=TRUE), covtemp))
ggplot() + geom_sf(data = map) +
  geom_sf(data = dp, aes(col = covtemp)) +
  scale_color_viridis()
ggplot() + geom_sf(data = map) +
  geom_sf(data = dp, aes(col = covprec)) +
  scale_color_viridis()
```
Saw in the forum <https://groups.google.com/g/r-inla-discussion-group/c/z_v2oIh2egs> it's good to work with units of kilometers which is intuitive from previous spatial data work.
```{r projection}
st_crs("EPSG:3857")$proj4string
projMercator<-"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0
+x_0=0 +y_0=0 +k=1 +units=km +nadgrids=@null +wktext +no_defs"
air_sf_project <- st_transform(air_sf_joined, crs = projMercator)
dp <- st_transform(dp, crs = projMercator)
# Observed coordinates
coo <- st_coordinates(air_sf_project)

# Predicted coordinates
coop <- st_coordinates(dp)
summary(dist(coo)) # summary of distances between locations
```
``max.edge``` controls the largest triangle edge length, and providing it with a vector ```c(inner, outer)``` sets the max edge for inside the boundary and outside the boundary. The purpose of this is to avoid boundary effects in the estimation of the model, where boundary values have high variance. Lindgren and Rue (2015) suggest to extend the domain by some amount. This is briefly discussed in the book by Blangiardo (2015).

```cutoff``` is the minimum allowed distance between points. Otherwise, points are replaced by a single vertex. For areas of high clusters of points, this could be useful to reduce redundancy. If no ```boundary``` is set the mesh is created on the convex hull of the observations.

```{r inla-mesh}
mesh <-inla.mesh.2d(loc = coo, max.edge=c(200, 500), crs=st_crs(air_sf_project))
mesh$n
```

Next we construct the A matrix that "A that projects the GRF from the observations to the vertices of the triangulated mesh." (Moraga 2024). 
This A matrix has a row for each ovservation, and columns equal to the number of vertices in the mesh. That is shown above, with ```mesh$n```. Below, two different meshes are generated. One for the observation locations, and one for the prediction locations. We need to set these both at once, because to make predictions in INLA we have to have that all pre-specified, unlike with the typical modelling style in R.

Additionally, the index allows us to extract fitted values from the model.

The smoothness parameter is set to 1, where in the spatial case $d=2$ and $\alpha=\nu + d/2 = 2$, seen in the code below.

```{r inla-A}
plot(mesh)
points(coo, col = "red")
axis(1)
axis(2)
spde <- inla.spde2.matern(mesh = mesh, alpha = 2, constr = TRUE)
indexs <- inla.spde.make.index("s", spde$n.spde)
lengths(indexs)
# Make the projection matrices
A <- inla.spde.make.A(mesh = mesh, loc = coo)
Ap <- inla.spde.make.A(mesh = mesh, loc = coop)
dim(A)
dim(Ap)
```
Then we have to make the INLA stack. This is done because we need to combine the data for estimation with the prediction data, as well as the projection matrices. It contains the response data, the list of covariate data--here the temperature and precipitation--the projection matrices, and the indices. In the next section about LGCPs, I'll describe the stack in a some more detail.
```{r inla-stack}
# stack for estimation stk.e
stk.e <- inla.stack(tag = "est",
data = list(y = air_sf_project$Value), A = list(1, A),
effects = list(data.frame(b0 = rep(1, nrow(A)),
covtemp = air_sf_project$covtemp, covprec = air_sf_project$covprec),
s = indexs))
#stk.e

# stack for prediction stk.p
stk.p <- inla.stack(tag = "pred",
data = list(y = NA), A = list(1, Ap),
effects = list(data.frame(b0 = rep(1, nrow(Ap)),
covtemp = dp$covtemp, covprec = dp$covprec),
s = indexs))
#stk.p

# stk.full has stk.e and stk.p
stk.full <- inla.stack(stk.e, stk.p)
```
Finally, we can specify the model in INLA. All the hard work has been done above and at least the model specification in INLA is easier. This model has mean specified by
$$
\mu_i = \beta_0 + \beta_1 \times \text{temp}_i + \beta_2 \times \text{prec}_i + S(x_i),
$$
so there is some contribution from fixed effects as well as a unknown latent process modelled as a zero-mean Gaussian Random Field with Matern covariance function. This puts us well within INLA territory. The model equation can be seen quite clearly in the ```formula``` variable below.

TODO: Fit the cameletti model in INLA.

```{r inla-model}
formula <- y ~ 0 + b0 + covtemp + covprec + f(s, model = spde)
res <- inla(formula, family = "gaussian",
       data = inla.stack.data(stk.full),
       control.predictor = list(compute = TRUE,
                                A = inla.stack.A(stk.full)),
       control.compute = list(return.marginals.predictor = TRUE))
```
Notice that I am passing a few options to the ```inla``` call. Importantly, ```control.compute```. We have a nice description of the control options at <https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#sec:controlops>. It controls what quantities are actually computed and returned during the INLA estimation. For example, there are a few different information criteria that it can return.

```control.predictor``` will compute the posterior marginals of the parameters.

Once the model is fit, we can inspect the fixed parameters and estimated latent field, as well as the hyperparameters for the latent field.
```{r inspect-fit}
res$summary.fixed
# Latent field is here
# res$summary.random$s
# res$summary.hyperpar

index <- inla.stack.index(stack = stk.full, tag = "pred")$data
pred_mean <- res$summary.fitted.values[index, "mean"]
pred_ll <- res$summary.fitted.values[index, "0.025quant"]
pred_ul <- res$summary.fitted.values[index, "0.975quant"]
grid$mean <- NA
grid$ll <- NA
grid$ul <- NA

length(pred_mean)
length(indicespointswithin)
grid$mean[indicespointswithin] <- pred_mean
grid$ll[indicespointswithin] <- pred_ll
grid$ul[indicespointswithin] <- pred_ul
  
summary(grid) # negative values for the lower limit
library(rasterVis)
levelplot(grid, layout = c(1, 3),
names.attr = c("Mean", "2.5 percentile", "97.5 percentile"))
```

When ```compute=TRUE``` in ```control.predictor```, we can also obtain the output below. <https://www.paulamoraga.com/book-geospatial/sec-inla.html> is good for info about this.
```{r posterior-marginals}
#| eval: false
res$summary.fitted.values
res$summary.linear.predictor
```

Another useful feature is the set of functions that can be used to manipulate the marginal distributions. Might be useful to compute posterior quantities such as KL divergence: <https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#sec:marginals>

Again from <https://www.paulamoraga.com/book-geospatial/sec-inla.html> is helpful.

```{r marginal}
inla.mmarginal(res$marginals.fixed$b0)
# Should be the same?
res$summary.fixed$mode[1]
```

Fit the model in ```inlabru``` now. I will be using the @cameletti-2013 model, also desribed in @blangiardo-2015, 
but with temporally varying covariate.

Let's understand these priors a bit more: pcmatern vs matern.
```{r inlabru-temporal}
mesh <-inla.mesh.2d(loc = coo, max.edge=c(200, 500), crs=st_crs(air_sf_project))
spde <- inla.spde2.pcmatern(mesh = mesh, alpha = 2, constr = TRUE, prior.range = c(10, 0.01), prior.sigma = c(1, 0.01))

air_sf_subset <- air_sf_project #%>% filter(Samplingpoint%in%distinct_stations)
#air_sf_subset <- air_sf_project %>% filter(Samplingpoint%in%distinct_stations, date<=as.Date("2023-01-05"))
air_sf_subset
dim(air_sf_subset)
air_sf_subset$date_index <- as.numeric(as.factor(air_sf_subset$date))
fm_mesh_1d(sort(unique(air_sf_subset$date_index)))
day_mapper <- bru_mapper(fm_mesh_1d(sort(unique(air_sf_subset$date_index))), indexed = TRUE)
day_mapper$mesh
#formula <- Value ~ Intercept(1) + 
#  field(geometry, model = spde, group = date_index, control.group=list(model="ar1", order=1))
formula <- Value ~ Intercept(1) + covtemp + covprec + no2_column_ugm3 + 
  field(geometry, model = spde, group = date_index, control.group=list(model="ar1", order=1))
fit <- bru(formula, data = air_sf_subset, family = "gaussian")#, options = list(control.inla=list(int.strategy = 'grid', strategy="gaussian")))
# Summarize the results
summary(fit)
```

```{r some-fake-data}
# OK, let's create a fake date
# January 24: Just creating the data from scratch sort of
# Amazing. Right now I have 15 days of data, one day for each station. But 
# I accidentally did the index wrong, so that it is 1:5 so each station has multiple observations for one 
# day. In this case, the code works. So there is really something about that. 
# Maybe try to add good priors to the code. But there was also a problem with that.
# January 27th: Code is working when the pcmatern function is used above. 
# However, not sure how to use the rho priors below. See <https://becarioprecario.bitbucket.io/spde-gitbook/ch-stapp.html>
fake_date_index <- as.integer(as.factor(rep(1:15, nrow(air_sf_subset)/15)))
fake_date_index
fake_stations <- air_sf_subset$Samplingpoint
fake_stations
length(air_sf_subset$date_index)
fake_geo <- st_geometry(air_sf_subset)
fake_value <- air_sf_subset$Value

fake_data <- data.frame(Value=fake_value, geometry=fake_geo, Samplingpoint=fake_stations, date_index=fake_date_index)
fake_data
dim(fake_data)
class(fake_data)
day_mapper <- bru_mapper(fm_mesh_1d(sort(unique(fake_data$date))), indexed = TRUE)
# The field is the realization of the spatio-temporal latent process.
#What's this prior do?
rho1p <- list(rho = list(prior = 'pc.cor1', param = c(0, 0.9))) 
formula <- Value ~ Intercept(1) +
  field(geometry, model = spde, group = date_index, control.group=list(model="ar1", order=1, hyper=rho1p))
  #field(geometry, model = spde, group = date_index, group_mapper = day_mapper, control.group=list(model="ar1", order=1))
# Fit the model for inlabru
precprior <- list(prec = list(prior = 'pc.prec', param = c(0.2, 0.5)))
fit <- bru(formula, data = fake_data, family = "gaussian", options = list(
  control.inla=list(int.strategy = 'grid', strategy="gaussian"), 
  control.family=list(list(hyper=precprior))))
# Summarize the results
summary(fit)
```
TODO: Does the latent process change over time?
Just seeing how the temporal model actually looks, or how it is supposed to look even. Might be useful to just fit 
a temporal only one.

Inspect the marginals
```{r posterior-marginals}
fit$summary.hyperpar
mesh$n

# This should be the number of marginals
mesh$n * length(unique(air_sf_subset$date))
length(fit$marginals.random[[1]])
mean(fit$marginals.random[[1]]$index.1)
mean(fit$marginals.random[[1]]$index.299)
rho_post <- fit$marginals.hyperpar$`GroupRho for field`
plot(rho_post, type = "l", xlab = "Rho", ylab = "Density")
inla_obj <- bru_info(fit)
inla_obj$model
plot(inla.smarginal(fit$marginals.random$field$index.1),type="l")
plot(fit$marginals.random$field$index.1, type = "l") 
plot(fit, "field", type="l")
```

TODO: If we want to do future predictions, need future satellite data. The below code 
work without using the covariates. But need to properly set it up with the satellite data.
```{r spatiotemporal-pred}
map_prj <- st_transform(map, crs = projMercator)

#Generate lattice points. Generates lattice evenly over region.
lattice_over_mesh <- fm_pixels(mesh, mask=map_prj, dims=c(20,20), format = "sf")
ggplot() +
geom_sf(data=as(lattice_over_mesh, "sf"))
# Predict ten days ahead from the last day in the data. But also include some of the "already seen" days.
day_lattice <- fm_cprod(lattice_over_mesh, data.frame(date_index = seq(max(air_sf_subset$date_index) + 1, by=1, length.out=4)))
#day_lattice <- fm_cprod(lattice_over_mesh, data.frame(daily_concentration=rep(NA, 14), day_index = seq(max(daily_air$day_index) - 3, by=1, length.out=14)))
day_lattice

#predictions <- predict(fit, day_lattice, formula= ~ Intercept + field)
# TODO: Not sure if this is working right or not. Predict times are all the same.
predictions <- predict(fit, day_lattice, formula = ~ Intercept)
predictions <- predict(fit, day_lattice, formula = ~ Intercept + field_eval(geometry, group=date_index))
#predictions1 <- predict(fit, newdata=dp, formula = ~ Intercept + covtemp + covprec + no2_column_ugm3 + f)
#predictions2 <- predict(fit, newdata=dp, formula = ~ f)

ggplot() +
geom_sf(data=predictions, aes(color=mean)) +
  facet_wrap(~date_index) +
  scale_colour_gradient(low = "blue", high = "yellow") 
```
That concludes this section, showing how to fit spatial model in ```INLA``` and ```inlabru```. There are a few things more to do: simulating data and seeing how well INLA recovers the parameters, and fitting a temporal model in both packages. But I'll leave these for later.
