[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Log Gaussian Cox Processes and INLA: Theory and Application",
    "section": "",
    "text": "Preface\nHello!\nThis is a blog post by Max Savery. Go see the next section to learn what it is about.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a blog post about Log-Gaussian Cox Processes (LGCP), mainly. It is a follow up to the earlier blog post I made about Poisson Point Process models. In fact, the LGCP is an extension of the Poisson Point Process, in that a latent correlation structure is included in the point process model. Specifically, the LGCP uses a Gaussian Random Field to characterize the intensity of the point process. Doing so allows us to take into account spatial or temporal correlation across our region or period of interest. Statistically, this is quite advantageous compared to the Poisson Point Process where spatial correlation could only be accounted for via covariates. However, the inclusion of the random field presents mathematical and computational challenges. These challenges are addressed by making use of the Integrated Nested Laplace Approximation (INLA) approach to posterior approximation.\nThe general structure of this post is as follows: I will first introduce the LGCP and accompanying theory, but only briefly, as most of the theory is the same as the Poisson Process. Then, I’ll spend some time covering the INLA method–which involves much more theory–and the accompanying R packages INLA and inlabru. I’ll compare these packages for modelling point processes as well as for continuous measures. At some point I will end the post and continue later with another about fitting multiple likelihood models in INLA, such as for the purpose of accounting for preferential sampling or including multiple datasets. Perhaps, at the end of this blog, you will be in a similar state as represented in the cartoon below.\n\n\n\nA very happy face\n\n\nI am writing about this because having a model that incorporates spatial (and/or temporal) correlation is necessary to properly model both point processes (such as the Presence-Only data of the last blog) or continuous measures in spatiotemporal domains (such as air pollution). Not including correlation means that you then assume the covariates included in your model are sufficient to account for spatial fluctuations in the response variable. The problem with this is that is there no direct component in the model that relates the behavior of the process at one location to the behavior at another location. The Gaussian field in the LGCP does exactly this by taking into account the spatial dependence between measurement locations.\nEven though the only thing we want to do is include spatial dependence, this quickly becomes a more technical model than the Poisson process. We have to first learn a whole new method of approximation (INLA) and a new software framework to be able to efficiently implement the LGCP. However, the advantage is the INLA method/software is sufficiently general so that we can tackle a whole set of spatial-temporal modelling problems using the same approach. In fact, the INLA approach allows us to (easily???) work with any continuous, discrete, or point-type data that has latent Gaussian structure, and makes possible the combination of likelihoods of different datasets as long as the underlying processes share a set of parameters. Pretty useful stuff.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "theory.html",
    "href": "theory.html",
    "title": "2  Theory",
    "section": "",
    "text": "2.1 LGCP\nA Poisson point process is a stochastic set of points indexed in a bounded n-dimensional space, where the number of points in the set is a Poisson distributed random variable. The LGCP extends the process by explicitly incorporating spatial or temporal correlation into the definition of the intensity of the process. Diggle et al. (2013) is a good reference for these type of models. I’ll give some details below, and then discuss the INLA approach for actually estimating the model.\nThe Poisson Point Process is characterized by an intensity that can be defined as the expected number of observations per unit area. For a given region \\(A\\), the number of individuals occurring within the region will be Poisson distributed. The mean of the distribution can be defined by the integral of the intensity over the region: \\[\n\\Delta(A) = \\int_A \\lambda(s)ds\n\\] Sadly, the intensity is an unobservable and unknown process, and therefore must be modelled as a function of covariates. The intensity takes the general parameterization as \\[\n\\lambda(s) = \\exp[\\alpha + \\beta'x(s)]\n\\] In the previous blog I discussed incorporating sampling bias, but I will leave that out now. Some version of this will come back when discussing preferential sampling.\nNow, in the LGCP we incorporate a Gaussian Random Field (also described as Gaussian spatial field, spatial random effect, or latent spatial field. I’m not sure exactly when the difference in description leads to a distinctly different spatial model) to incorporate spatial correlation in the model of intensity. \\[\n\\lambda(s) = \\exp[\\alpha + \\beta'x(s) + f_s(s)]\n\\] The spatial effect is denoted as \\(f_s(s)\\) in order to distinguish from temporal contributions. \\(f_s(s)\\) can be thought of as a continuous spatial effect that is evaluated at certain locations.\nThe main thing is that, once we incorporate \\(f_s(s)\\), we are forced to use a covariance structure that depends on the resolution of the grid or mesh that will be used to approximate the continuous spatial effect. Furthermore, we are still stuck with the problem of an intractable integral in the likelihood of the LGCP (as in the case of the Poisson process). Thus, the computational complexity makes this approach prohibitively costly when using MCMC approaches to estimate the model in any region of practical interest. Hence, other approximations to the posterior of the parameters in the LGCP is necessary.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theory</span>"
    ]
  },
  {
    "objectID": "theory.html#inla",
    "href": "theory.html#inla",
    "title": "2  Theory",
    "section": "2.2 INLA",
    "text": "2.2 INLA\nThe Integrated Nested Laplace Approximation has been proposed by Rue, Martino, and Chopin (2009) as a fast way to estimate the posterior of models with latent Gaussian structure. The LGCP is certainly an instance of models of this class. In the following description of the model, I present it nearly identically to the original paper, and/or from other well-done sources, including:\n\nhttps://becarioprecario.bitbucket.io/spde-gitbook really has a lot of detail, specifically for the SPDE innovation to INLA.\nhttps://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html is also good for focusing just on INLA.\nThe book of Paula Moraga https://www.paulamoraga.com/book-spatial/sec-geostatisticaldataSPDE.html is concise and clear for an initial tutorial, though not a detailed.\nThis book is also a very good resource: Spatial and Spatio-temporal Bayesian Models with R-INLA. Blangiardo, Marta, and Michela Cameletti. 2015.\n\nThere is nothing new under the sun here. I’m just condensing some resources for my own reference. So, taking notation directly from the Rue paper, INLA is concerned with models with an additive predictor that takes the general form \\[\n\\eta_i = \\alpha + \\sum^{n_f}_{j=1}f^{(j)}(u_{ij}) + \\sum^{n_{\\beta}}_{k=1} \\beta_kz_{ki} + \\epsilon_i\n\\] where \\(\\alpha\\) is an intercept, \\(f\\) are unknown functions of \\(\\mathbf{u}\\), \\(\\beta\\) are the typical fixed effects for covariates \\(\\mathbf{z}\\), and \\(\\epsilon_i\\) is the additional noise in the process. Additionally, \\(\\mathbf{x}\\) will refer to the vector of latent Gaussian variables (Gaussian by definition of the prior), and \\(\\boldsymbol{\\theta}\\) the vector of hyperparameters that we usually deal with in a Bayesian model. These need not be inherently Gaussian. In the book of Blangiardo and Cameletti (2015), hyperparameters and latent field are denoted by \\(\\psi\\) and \\(\\theta\\) respectively, which is clearer than using \\(\\mathbf{x}\\) to describe the latent field, but here I’m sticking to the original notation.\nStill taking from the notation of the Rue paper, the joint posterior of hyperparameters and latent variables is written as \\[\n\\begin{aligned}\n\\pi(\\mathbf{x},\\mathbf{\\theta}|\\mathbf{y})\\propto\\pi(\\mathbf{\\theta})\\pi(\\mathbf{x}|\\mathbf{\\theta})\\prod_{i\\in I}\\pi(y_i|x_i,\\mathbf{\\theta})\\\\\n\\propto\\pi(\\mathbf{\\theta})|Q(\\mathbf{\\theta})|^{1/2}\\exp\\bigg[-\\frac{1}{2}\\mathbf{x}^TQ(\\mathbf{\\theta})\\mathbf{x}+\\sum_{i\\in I}\\log\\{\\pi(y_i|x_i,\\mathbf{\\theta})\\}\\bigg],\n\\end{aligned}\n\\] where \\(\\pi(y_i|x_i,\\mathbf{\\theta})\\) is the distribution of the response variable and \\(|Q(\\mathbf{\\theta})|^{1/2}\\exp\\bigg[-\\frac{1}{2}\\mathbf{x}^TQ(\\mathbf{\\theta})\\mathbf{x}\\bigg]\\) is the Gaussian prior on the GRMF with a mean of 0 and precision matrix \\(Q\\), conditional on \\(\\mathbf{\\theta}\\).\nThe main point of INLA is to make approximations of the posterior marginals of the hyperparameters and (even more importantly) the latent field. The posterior marginal of one component of the hyperparameters is \\[\n\\pi(\\theta_j|\\mathbf{y}) = \\int \\pi(\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta}_{-j}.\n\\] Notice the \\(-j\\) components are integrated out. The posterior marginal of a component of the latent field is \\[\n\\pi(x_i|\\mathbf{y}) = \\int \\pi(x_i|\\mathbf{\\theta},\\mathbf{y})\\pi(\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta},\n\\] again, sticking to the \\(\\mathbf{x}\\) notation.\nFollowing the procedure of INLA, first \\(\\pi(\\theta|y)\\) will need to be approximated, and then \\(\\pi(x_i|\\theta,y)\\). The integration over \\(\\pi(x_i|y)\\) will be done numerically, using points from (|y). The approximations themselves are done with the Laplace Approximation, or a Gaussian-type approximation. To approximate the first quantity, \\(\\pi(\\theta|y)\\), a Laplace approximation is used from Tierney and Kadane, 1986: \\[\n\\tilde{\\pi}(\\theta|y) \\propto \\frac{\\pi(x,\\theta,y)}{\\tilde{\\pi}_G(x|\\theta,y)}\\bigg|_{x=x^*(\\theta)} = \\frac{\\pi(y|x,\\theta)\\pi(x|\\theta)\\pi(\\theta)}{\\tilde{\\pi}_G(x|\\theta,y)}\\bigg|_{x=x^*(\\theta)},\n\\] where \\(x=x^*(\\theta)\\) of is the mode of \\(\\tilde{\\pi}_G(x|\\theta,y)\\). As per the name, IN(ested)LA, this approximation uses a NESTED approximation, \\(\\tilde{\\pi}_G(x|\\theta,y)\\). \\(\\tilde{\\pi}_G\\) is the Gaussian approximation to the latent field, which works because we already know the latent field to be prior-ly distributed as Gaussian. In fact, the contribution of this paper is to then to improve on the approximation of \\(\\pi(x_i|, \\theta, y)\\) with another approximation, the Laplace Approximation (LA), beyond just a simple Gaussian. The paper discusses 3 approximations to \\(\\pi(x_i|\\theta, y)\\): the Gaussian, LA, and a simplified LA.\nThe LA approximation is written as \\[\n\\tilde{\\pi}_{LA}(x_i|\\theta,y) \\propto \\frac{\\pi(x,\\theta,y)}{\\tilde{\\pi}_{GG}(x_{-i}|x_i,\\theta,y)}\\bigg|_{x_{-i}=x^*_{-i}(x_i,\\theta)},\n\\] where \\(x^{*}_{-i}(x_i,\\theta)\\) is the mode of \\(\\tilde{\\pi}_{GG}(x_{-i}|x_i,\\theta,y)\\). This approximation can be computationally intensive since it is necessary to recompute \\(\\tilde{\\pi}_{GG}(x_{-i}|x_i,\\theta,y)\\) for each element of \\(x\\) and \\(\\theta\\).\nOnce the approximations of \\(\\tilde{\\pi}(\\theta|y)\\) of \\(\\tilde{\\pi}(x_i|\\theta,y)\\) are found, the marginals for each element \\(x_i\\) can be approximated:\n\\[\n\\tilde{\\pi}(x_i|y) \\approx\\sum\\tilde{\\pi}(x_i|\\theta^{(i)},y)\\tilde{\\pi}(\\theta^{(j)}|y)\\Delta_j\n\\] for integration points \\(\\theta^{(i)}\\) which are found by the exploration of the posterior for \\(\\theta\\). This is done in a few different ways, for example by creating a standardized \\(Z\\) variable around the mode and inverse hessian of \\(\\log \\tilde{\\pi}(\\theta|y)\\). There are quite a few more details to this part that I won’t discuss here, but may come up when up using the INLA software itself (choosing the integration scheme, for instance).\nAgain, from the math above it is not possible to fully understand the INLA approach, but it should enough of a technical description to understand the main ideas (approximate the hyperparameters and latent conditionals, then numerically integrate to approximate the marginals).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theory</span>"
    ]
  },
  {
    "objectID": "theory.html#spde-connection",
    "href": "theory.html#spde-connection",
    "title": "2  Theory",
    "section": "2.3 SPDE connection",
    "text": "2.3 SPDE connection\nThe Stochastic Partial Differential Equation (SPDE) approach is an innovation from Lindgren, Rue, and Lindström (2011) on the INLA method. The approach allows the representation of a continuous Gaussian Field with a Gaussian Markov Random Field. The connection to the Rue, Martino, and Chopin (2009) paper is that Lindgren, Rue, and Lindström (2011) connected the GRMF models of INLA to specific (continuous) GFs. As spatial statisticians, we care about the continuous case more than approximations in discrete space, and SPDE connected the two.\nTo be slightly more specific, it turns out a GF with Matern covariance is a solution to a specific SPDE. The solution to the SPDE is found via a set of basis functions defined on a triangulation of the spatial region. The resulting GRMF is an approximation to the SPDE. So this approach allows us to approximate the solution of the SPDE with a GRMF that corresponds to a specific GF with matern covariance.\n\n2.3.1 Mesh\nIn the SPDE approach a triangulated mesh (the finite element representation) is used to approximate the solution to the SPDE using spatial weights defined at the vertices of the mesh. \\[\nx(u) = \\sum^n_{k=1}\\psi_k(u)w_k,\n\\] where \\(w_k\\) are Gaussian weights and \\(\\psi_k\\) are the piecewise linear basis functions, and \\(n\\) is the number of vertices in the entire triangulation of the domain. This means that the weights represent the values of the field at each vertex, and the basis functions are used to interpolate the value of the observation in the interior of the triangle, where \\(\\psi_k(u)=1\\) only at vertex \\(k\\) of the triangle containing \\(u\\). A result of Lindgren, Rue, and Lindström (2011) is to show that the weights follow a GRMF and to derive the precision matrix.\nThe practical implication of this the SPDE approach is that you will need to define the mesh before modelling. The results will be sensitive to this choice, so it is important to explore the impact of the mesh on the model estimation. I’ll show this a little once I get to the coding.\nIn INLA software, a projection matrix maps the GMRF from the observations to the triangulation nodes. The observation will be represented as the weighted average using the weights and values from the triangulation and projection matrix. For example, in the picture below, for observation \\(s\\) the value Z\\((s)\\) is calculated as \\(Z(s)\\approx \\frac{T_1}{T}Z_1+\\frac{T_2}{T}Z_2+\\frac{T_3}{T}Z_3\\). This is taken from https://www.paulamoraga.com/book-spatial/sec-geostatisticaldataSPDE.html \n\n\n2.3.2 Off the grid\nSimpson et al. (2016) developed an approach for approximating the likelihood of the LGCP within the INLA and SPDE framework, using a finite-dimensional random field \\[\nZ(s) = \\sum^n_{i=1}z_i\\phi_i(s)\n\\] where \\(z=(z_1...,z_n)^T\\) is a multivariate Gaussian vector and \\(\\{\\phi_i(s)\\}^n_{i=1}\\) is a set of basis functions. Using the random field, they propose an approximation to the LGCP that does not rely on the discretization of the domain into a grid. Furthermore, they are able to leverage the triangulation of the SPDE approach to compute the LGCP likelihood when using their random field approximation. The implication of this for our LGCP modelling is that the INLA framework can conveniently be used approximate the LGCP likelihood and posterior.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theory</span>"
    ]
  },
  {
    "objectID": "theory.html#in-conclusion",
    "href": "theory.html#in-conclusion",
    "title": "2  Theory",
    "section": "2.4 In conclusion…",
    "text": "2.4 In conclusion…\nThat was a bit of a longer-than-intended dive into INLA. But INLA is a more mathematically complicated method than other approximations, and I wanted to solidify the details for myself. It’s not always that way, sometimes it’s nice just to jump straight into the application, but with INLA it seemed prudent to pick over the details. Now, onto using the software with some of my own data and use-cases. I’ll now proceed to discuss the modelling procedure using INLA and inlabru. I’ll compare the software and details of the implementation, first for a continuous measurement and then for point data using an LGCP.\n\n\n\n\nBlangiardo, Marta, and Michela Cameletti. 2015. Spatial and Spatio-Temporal Bayesian Models with r-INLA. John Wiley & Sons, Ltd. https://doi.org/https://doi.org/10.1002/9781118950203.\n\n\nDiggle, Peter J., Paula Moraga, Barry Rowlingson, and Benjamin M. Taylor. 2013. “Spatial and Spatio-Temporal Log-Gaussian Cox Processes: Extending the Geostatistical Paradigm.” Statistical Science 28 (4): 542–63. https://doi.org/10.1214/13-STS441.\n\n\nLindgren, Finn, Håvard Rue, and Johan Lindström. 2011. “An Explicit Link Between Gaussian Fields and Gaussian Markov Random Fields: The Stochastic Partial Differential Equation Approach.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 73 (4): 423–98. https://doi.org/10.1111/j.1467-9868.2011.00777.x.\n\n\nRue, Håvard, Sara Martino, and Nicolas Chopin. 2009. “Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 71 (2): 319–92. https://doi.org/10.1111/j.1467-9868.2008.00700.x.\n\n\nSimpson, D., J. B. Illian, F. Lindgren, S. H. Sørbye, and H. Rue. 2016. “Going Off Grid: Computationally Efficient Inference for Log-Gaussian Cox Processes.” Biometrika 103 (1): 49–70. https://doi.org/10.1093/biomet/asv064.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theory</span>"
    ]
  },
  {
    "objectID": "inla_inlabru_tutorial_netherlands_data.html",
    "href": "inla_inlabru_tutorial_netherlands_data.html",
    "title": "3  Spatial modelling with INLA and SPDE",
    "section": "",
    "text": "3.1 INLA\nHaving covered the need-to-know material for INLA and SPDE, next I’m going to build a spatial model using both INLA and inlabru. Comparing the two packages is useful because, though they can be expected to give the same results in most cases, the implementation is a bit different. Knowing when using the more complicated INLA can be justified is a useful exercise, I think. The template for this code using INLA follows the air pollution example from the geostatistical chapter in Paula Moraga’s Spatial Statistics book, https://www.paulamoraga.com/book-spatial/sec-geostatisticaldataSPDE.html. The difference is that I will use air pollution data from the Netherlands, compare INLA and inlabru, and consider a few extra technical details. For the inlabru code I follow the examples the authors provide at their site for the package: https://inlabru-org.github.io/inlabru/articles/.\nThe model fit here is a simple geostatistical one. I’ll be using air pollution data in the Netherlands. The model can be written as \\[\nY_i \\sim N(u_i, \\sigma^2)\\\\\nu_i = \\beta_0 + \\beta_1\\cdot\\text{temperature}_i + \\beta_2\\cdot\\text{precipitation}_i + S(x_i).\n\\] So it’s a typical Gaussian distributed variable with underlying latent structure $S(x_i), otherwise referred to as a random effect modelled as a Gaussian process that is spatially indexed. In either case, it represents measurements taken at discrete locations but used to describe or estimate a spatially continuous process, such as air pollution.\nLet’s get straight to data downloading and processing. I downloaded air pollution data in the Belgium and Netherlands for the year of 2023 from https://eeadmz1-downloads-webapp.azurewebsites.net/. For now I just focus on the Netherlands data.\nlibrary(INLA)\nlibrary(inlabru)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(terra)\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\nIn the dataset, 8 is the id for Nitrogen dioxide, 5 Particulate matter &lt; 10 µm, and 6001 for Particulate matter &lt; 2.5 µm. I focused on Particulate matter &lt; 2.5 µm, referred to as PM2.5.\n#data_path &lt;- file.path(\"D:\", \"data\", \"air_quality_data\", \"belgium_eea\", \"E1a\")\ndata_path &lt;- file.path(\"D:\", \"data\", \"air_quality_data\", \"netherlands_eea\", \"E1a\")\nd &lt;- open_dataset(data_path)\nd %&gt;% group_by(Samplingpoint) |&gt; filter(Pollutant==6001, Value&gt;-1) |&gt; collect() -&gt; df\ndf\n\n# A tibble: 442,873 × 12\n# Groups:   Samplingpoint [53]\n   Samplingpoint   Pollutant Start               End                 Value Unit \n   &lt;chr&gt;               &lt;int&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;\n 1 NL/SPO-NL00007…      6001 2023-01-01 01:00:00 2023-01-01 02:00:00 131.  ug.m…\n 2 NL/SPO-NL00007…      6001 2023-01-01 02:00:00 2023-01-01 03:00:00  86.2 ug.m…\n 3 NL/SPO-NL00007…      6001 2023-01-01 03:00:00 2023-01-01 04:00:00  30.7 ug.m…\n 4 NL/SPO-NL00007…      6001 2023-01-01 04:00:00 2023-01-01 05:00:00  11   ug.m…\n 5 NL/SPO-NL00007…      6001 2023-01-01 05:00:00 2023-01-01 06:00:00   9.1 ug.m…\n 6 NL/SPO-NL00007…      6001 2023-01-01 06:00:00 2023-01-01 07:00:00   5.3 ug.m…\n 7 NL/SPO-NL00007…      6001 2023-01-01 07:00:00 2023-01-01 08:00:00   2.8 ug.m…\n 8 NL/SPO-NL00007…      6001 2023-01-01 08:00:00 2023-01-01 09:00:00   2.3 ug.m…\n 9 NL/SPO-NL00007…      6001 2023-01-01 10:00:00 2023-01-01 11:00:00  -0.6 ug.m…\n10 NL/SPO-NL00007…      6001 2023-01-01 11:00:00 2023-01-01 12:00:00   1.4 ug.m…\n# ℹ 442,863 more rows\n# ℹ 6 more variables: AggType &lt;chr&gt;, Validity &lt;int&gt;, Verification &lt;int&gt;,\n#   ResultTime &lt;dttm&gt;, DataCapture &lt;dbl&gt;, FkObservationLog &lt;chr&gt;\nNext add the the locations of the measuring stations to the dataset. The air pollution observations include an station identifier, but the coordinates of each of these stations is not included. It comes in a separate dataset.\ndf &lt;- df %&gt;%\n   #mutate(Samplingpoint = str_remove(Samplingpoint, \"BE/\"))\n   mutate(Samplingpoint = str_remove(Samplingpoint, \"NL/\"))\n#station_location_path &lt;- file.path(\"D:\", \"data\", \"air_quality_data\", \"eea_stations_2023\", \"belgium_stations_2023.csv\")\nstation_location_path &lt;- file.path(\"D:\", \"data\", \"air_quality_data\", \"eea_stations_2023\", \"nl_stations_2023.csv\")\nstation_locations &lt;- read_csv(station_location_path)\n\nRows: 4685 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (19): Country, Air Quality Network, Air Quality Network Name, Air Qualit...\ndbl  (8): Year, Air Pollution Level, Data Coverage, Verification, Longitude,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstation_locations &lt;- rename(station_locations, Samplingpoint = 'Sampling Point Id')\nstation_locations %&gt;% \n  distinct(Samplingpoint, Longitude, Latitude) -&gt; unique_locations\nmerged_df &lt;- df %&gt;% \n  left_join(unique_locations %&gt;% select(Samplingpoint, Longitude, Latitude), by = \"Samplingpoint\")\nstation_locations_sf &lt;- st_as_sf(station_locations, coords = c(\"Longitude\", \"Latitude\"))\nair_sf &lt;- st_as_sf(merged_df, coords = c(\"Longitude\", \"Latitude\"))\nst_crs(air_sf) &lt;- \"EPSG:4326\"\nair_sf$station_locations\n\nWarning: Unknown or uninitialised column: `station_locations`.\n\n\nNULL\nThen examine the time series\ndistinct_stations &lt;- unique(air_sf$Samplingpoint)[1:12]\nplotting_stations &lt;- air_sf[air_sf$Samplingpoint%in%distinct_stations,]\nplotting_stations %&gt;% \n  ggplot(aes(x=Start, y=Value)) + \n    geom_line() + \n    facet_wrap(~ Samplingpoint) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(title = \"Time series of monitoring stations\", x = \"Time, hourly\", y = \"Conc (μg/m³)\")\nThe above figure shows 12 of the stations observations over 2023. The data is currently at the hourly resolution.\nNext, load a map of the Netherlands. The border is from https://service.pdok.nl/kadaster/bestuurlijkegrenzen/atom/bestuurlijke_grenzen.xml. A number of processing steps are done here, namely to generate some grid points within the map border. These points will be used as locations to make spatial predictions later on.\nmap &lt;- st_read(file.path(\"D:\", \"data\", \"maps\", \"netherlands_bestuurlijkegrenzen_2021\", \"bestuurlijkegrenzen.gpkg\"))\nmap &lt;- st_union(map)\nmap &lt;- st_as_sf(map)\nmap &lt;- st_transform(map, crs = st_crs(air_sf))\n# raster grid covering map\ngrid &lt;- terra::rast(map, nrows = 100, ncols = 100)\n# coordinates of all cells\nxy &lt;- terra::xyFromCell(grid, 1:ncell(grid))\n# transform points to a sf object\ndp &lt;- st_as_sf(as.data.frame(xy), coords = c(\"x\", \"y\"),\n                 crs = st_crs(map))\n\n# indices points within the map\nindicespointswithin &lt;- which(st_intersects(dp, map,\n                                           sparse = FALSE))\n# points within the map\ndp &lt;- st_filter(dp, map)\nLet’s check the map and grid of prediction points.\n# plot\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp)\nWe can see the grid of spatial points is at quite a high resolution.\nThe next step is to download temperature and precipitation data using the geodata package.\nlibrary(geodata)\n# With geodata library\nsave_path &lt;- file.path(\"D:\", \"data\", \"air_quality_data\", \"aux_variables\")\ncovtemp &lt;- worldclim_global(var = \"tavg\", res = 10,\n                            path = save_path)\ncovprec &lt;- worldclim_global(var = \"prec\", res = 10,\n                            path = save_path)\n# Extract at observed locations\nair_sf$covtemp &lt;- extract(mean(covtemp), st_coordinates(air_sf))[, 1]\nair_sf$covprec &lt;- extract(mean(covprec), st_coordinates(air_sf))[, 1]\n# Extract at prediction locations\ndp$covtemp &lt;- extract(mean(covtemp), st_coordinates(dp))[, 1]\ndp$covprec &lt;- extract(mean(covprec), st_coordinates(dp))[, 1]\nWe can then take a look at the air pollution and the covariates at the measuring stations locations.\nggplot() + geom_sf(data = map) +\n  geom_sf(data = air_sf, aes(col = Value)) +\n  ggtitle(\"Air pollution at measuring stations\") +\n  scale_color_viridis()\n\n\n\n\n\n\n\nggplot() + geom_sf(data = map) +\n  geom_sf(data = air_sf, aes(col = covtemp)) +\n  ggtitle(\"Temperature at measuring stations\") +\n  scale_color_viridis()\n\n\n\n\n\n\n\nggplot() + geom_sf(data = map) +\n  geom_sf(data = air_sf, aes(col = covprec)) +\n  ggtitle(\"Precipitation at measuring stations\") +\n  scale_color_viridis()\nThe above code also extracted the covariate data at each prediction point. We can see what that looks like as well:\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp, aes(col = covtemp)) +\n  ggtitle(\"Temperature at prediction points\") +\n  scale_color_viridis()\n\n\n\n\n\n\n\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp, aes(col = covprec)) +\n  ggtitle(\"Precipitation at prediction points\") +\n  scale_color_viridis()\nNext mean impute out the NANs in the data. A better imputation method would certainly be better, wouldn’t it?\ndp &lt;- dp %&gt;% mutate(covprec=ifelse(is.na(covprec), mean(covprec, na.rm=TRUE), covprec),\n                    covtemp=ifelse(is.na(covtemp), mean(covtemp, na.rm=TRUE), covtemp))\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp, aes(col = covtemp)) +\n  scale_color_viridis()\n\n\n\n\n\n\n\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp, aes(col = covprec)) +\n  scale_color_viridis()\nI saw in the INLA forum, https://groups.google.com/g/r-inla-discussion-group/c/z_v2oIh2egs, that it can be helpful to work with units of kilometers. In my experience, this has also held true.\nprojMercator&lt;-\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0\n+x_0=0 +y_0=0 +k=1 +units=km +nadgrids=@null +wktext +no_defs\"\nair_sf_project &lt;- st_transform(air_sf, crs = projMercator)\ndp &lt;- st_transform(dp, crs = projMercator)\n# Observed coordinates\ncoo &lt;- st_coordinates(air_sf_project)\n\n# Predicted coordinates\ncoop &lt;- st_coordinates(dp)\n#summary(dist(coo)) # summary of distances between locations\nThat gives us enough data and more importantly, motivation, to make a model. Now I can get into the INLA implementation. First: the mesh. I mentioned the mesh in the theory discussion. The mesh enables us to compute the discretized approximation to the GF using a GRMF.\nINLA provides the function to create the mesh. Here I’m using inla.mesh.2d. Let’s take a look at a few meshes created with different parameter options.\nmesh0 &lt;- inla.mesh.2d(loc = coo, max.edge=c(200, 500))\nmesh1 &lt;- inla.mesh.2d(loc = coo, max.edge=c(200, 500), cutoff=1)\nmesh2 &lt;- inla.mesh.2d(loc = coo, max.edge=c(200, 500), cutoff=0.3)\nmesh3 &lt;- inla.mesh.2d(loc = coo, max.edge=c(100, 150), cutoff=1)\nmesh4 &lt;- inla.mesh.2d(loc = coo, max.edge=c(100, 150), cutoff=0.3)\nmesh5 &lt;- inla.mesh.2d(loc = coo, max.edge=c(100), cutoff=1)\nmesh6 &lt;- inla.mesh.2d(loc = coo, max.edge=c(100), cutoff=0.3)\nplot(mesh0)\n\n\n\n\n\n\n\nplot(mesh1)\n\n\n\n\n\n\n\nplot(mesh2)\n\n\n\n\n\n\n\nplot(mesh3)\n\n\n\n\n\n\n\nplot(mesh4)\n\n\n\n\n\n\n\nplot(mesh5)\n\n\n\n\n\n\n\nplot(mesh6)\n\n\n\n\n\n\n\n# Using mesh0\nmesh &lt;-inla.mesh.2d(loc = coo, max.edge=c(200, 500), crs=st_crs(air_sf_project))\nThere are a number of options to play with: max.edge controls the largest triangle edge length, and providing it with a vector c(inner, outer) sets the max edge for inside the boundary and outside the boundary. The purpose of this is to avoid boundary effects in the estimation of the model, where boundary values have high variance. (lindgren2015?) suggests to extend the domain by some amount.\ncutoff is the minimum allowed distance between points. Otherwise, points are replaced by a single vertex. For areas of high clusters of points, this could be useful to reduce redundancy. If no boundary is set the mesh is created on the convex hull of the observations.\nhttps://punama.github.io/BDI_INLA/ is also a good source for this. Thanks!\nNext we construct the A matrix that “A that projects the GRF from the observations to the vertices of the triangulated mesh” (Moraga (2023)). The A matrix has a row for each observation, and columns equal to the number of vertices in the mesh. The number of vertices in our mesh can be checked with mesh$n. Below, two different meshes are generated. One for the observation locations, and one for the prediction locations. We need to set these both at once, because to make predictions in INLA we have to have that all pre-specified, unlike with the typical modelling style in R. That was why I created the grid of prediction points initially.\nThe inla.spde2.matern function is used to build the SPDE model with matern covariance, using the mesh. The smoothness parameter \\(\\nu\\) for the SPDE is implicitly set to 1, where in the spatial case \\(d=2\\) and \\(\\alpha=\\nu + d/2 = 2\\), seen in the code below. In the model itself the Matern covariance function parameters will be estimated; more on that later.\nplot(mesh)\npoints(coo, col = \"red\")\naxis(1)\naxis(2)\n\n\n\n\n\n\n\nspde &lt;- inla.spde2.matern(mesh = mesh, alpha = 2, constr = TRUE)\nindexs &lt;- inla.spde.make.index(\"s\", spde$n.spde)\nlengths(indexs)\n\n      s s.group  s.repl \n    280     280     280 \n\n# Make the projection matrices\nA &lt;- inla.spde.make.A(mesh = mesh, loc = coo)\nAp &lt;- inla.spde.make.A(mesh = mesh, loc = coop)\ndim(A)\n\n[1]  52 280\n\ndim(Ap)\n\n[1] 4944  280\nThe index allows us to extract fitted values from the model, which has a length equal to the number of vertices in the mesh. s.group and s.repl (replicate) are related how the dependence structure is specified in the model.\nThen we have to make the INLA stack. This is done because we need to combine the data for estimation with the prediction data, as well as the projection matrices. It contains the response data, the list of covariate data–here the temperature and precipitation, the projection matrices, and the indices. Though this is a bit technical, it actually adds a lot of flexibility to INLA because we can manipulate the stack based on the data structure that we want. For example, when modelling multiple likelihoods in a single model, these will be combined all within the stack.\n# stack for estimation stk.e\nstk.e &lt;- inla.stack(tag = \"est\",\ndata = list(y = air_sf_project$Value), A = list(1, A),\neffects = list(data.frame(b0 = rep(1, nrow(A)),\ncovtemp = air_sf_project$covtemp, covprec = air_sf_project$covprec),\ns = indexs))\n#stk.e\n\n# stack for prediction stk.p\nstk.p &lt;- inla.stack(tag = \"pred\",\ndata = list(y = NA), A = list(1, Ap),\neffects = list(data.frame(b0 = rep(1, nrow(Ap)),\ncovtemp = dp$covtemp, covprec = dp$covprec),\ns = indexs))\n#stk.p\n\n# stk.full has stk.e and stk.p\nstk.full &lt;- inla.stack(stk.e, stk.p)\nFinally, we can specify the model in INLA. All the hard work has been done above and at least the model specification in INLA is easier. This model has its mean specified by \\[\n\\mu_i = \\beta_0 + \\beta_1 \\cdot \\text{temp}_i + \\beta_2 \\cdot \\text{prec}_i + S(x_i),\n\\] so there is some contribution from fixed effects as well as a latent process modelled as a zero-mean Gaussian Random Field with Matern covariance function. This puts us well within INLA territory. The model equation can be seen quite clearly in the formula variable below. We include 0 to remove the intercept that is added by default because we specifically refer to \\(\\beta_0\\) in the stack and then need to do so in the model equation.\nformula &lt;- y ~ 0 + b0 + covtemp + covprec + f(s, model = spde)\nres &lt;- inla(formula, family = \"gaussian\",\n       data = inla.stack.data(stk.full),\n       control.inla=list(lincomb.derived.correlation.matrix=TRUE),\n       control.predictor = list(compute = TRUE,\n                                A = inla.stack.A(stk.full)),\n       control.compute = list(return.marginals.predictor = TRUE))\nNotice that I am passing a few options to the INLA call. Importantly, control.compute. We have a nice description of the control options at https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#sec:controlops. It controls what quantities are actually computed and returned during the INLA estimation. For example, there are a few different information criteria that it can returned. control.predictor will compute the posterior marginals of the parameters. Also useful is lincomb.derived.correlation.matrix=TRUE or control.fixed = list(correlation.matrix=TRUE) to retrieve the covariance matrix for the fixed effects. All the options can be seen with the command inla.set.control.compute.default()\nOnce the model is fit, we can inspect the fixed parameters, the estimated latent field, as well as the hyperparameters for the latent field.\nres$summary.fixed\n\n                mean         sd  0.025quant     0.5quant 0.975quant\nb0      -15.29769449 21.3472624 -57.7595559 -15.17245556 26.4583704\ncovtemp   2.17003514  2.0949918  -1.8002534   2.09513247  6.5392273\ncovprec  -0.01550201  0.2034683  -0.4214077  -0.01404086  0.3818577\n                mode          kld\nb0      -15.17140288 4.819848e-08\ncovtemp   2.09359434 3.231952e-07\ncovprec  -0.01411427 5.717319e-08\n\n# Latent field is here but it will print out a lot of values\n# res$summary.random$s\nres$summary.hyperpar\n\n                                              mean         sd 0.025quant\nPrecision for the Gaussian observations  0.3076177 0.07974476  0.1774270\nTheta1 for s                             1.9282907 0.53127489  0.9078608\nTheta2 for s                            -4.1344657 0.69219969 -5.5329805\n                                          0.5quant 0.975quant       mode\nPrecision for the Gaussian observations  0.2986888  0.4890943  0.2825065\nTheta1 for s                             1.9197803  2.9993397  1.8830630\nTheta2 for s                            -4.1223427 -2.8081708 -4.0698031\nThis shows us the relevant statistics for our intercept and parameters for the covariates.\nOur predictions at the points in our spatial field have already been made, so we need to extract the estimated values:\nindex &lt;- inla.stack.index(stack = stk.full, tag = \"pred\")$data\npred_mean &lt;- res$summary.fitted.values[index, \"mean\"]\npred_ll &lt;- res$summary.fitted.values[index, \"0.025quant\"]\npred_ul &lt;- res$summary.fitted.values[index, \"0.975quant\"]\ngrid$mean &lt;- NA\ngrid$ll &lt;- NA\ngrid$ul &lt;- NA\ngrid$mean[indicespointswithin] &lt;- pred_mean\ngrid$ll[indicespointswithin] &lt;- pred_ll\ngrid$ul[indicespointswithin] &lt;- pred_ul\n  \nsummary(grid) # negative values for the lower limit\n\n      mean             ll               ul        \n Min.   :1.338   Min.   :-4.722   Min.   : 3.733  \n 1st Qu.:3.626   1st Qu.: 0.359   1st Qu.: 6.530  \n Median :4.665   Median : 1.462   Median : 7.813  \n Mean   :4.595   Mean   : 1.368   Mean   : 7.874  \n 3rd Qu.:5.553   3rd Qu.: 2.405   3rd Qu.: 9.087  \n Max.   :8.316   Max.   : 6.753   Max.   :12.860  \n NA's   :5056    NA's   :5056     NA's   :5056\nThen create a plot of the predictions:\nlibrary(rasterVis)\n\nLoading required package: lattice\n\nlevelplot(grid, layout = c(1, 3),\nnames.attr = c(\"Mean\", \"2.5 percentile\", \"97.5 percentile\"))\nThis shows the mean and the 95% credible interval surrounding it.\nWhen compute=TRUE in control.predictor, we can also obtain the quantities below. https://www.paulamoraga.com/book-geospatial/sec-inla.html describes this.\nres$summary.fitted.values\nres$summary.linear.predictor\nmarginals.linear.predictor\nmarginals.fitted.values\nbut it’s a lot of output so I won’t show it.\nFrom https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#model-fitting-strategies, we can also see the differences using the different fitting strategies, set by the control.inla argument. We can change both the Gaussian approximation strategy for the posterior full conditional distributions, as well as the integration strategy used to integrate out the \\(\\theta_{-k}\\) parameters to get the marginal distribution for \\(\\theta_k\\). The \"grid\" option is the most costly, compared to the central composite design (\"ccd\"). In the empirical bayes option (\"eb\"), the posterior mode is used as the integration point.\napprox_strategy &lt;- c(\"gaussian\", \"simplified.laplace\", \"laplace\")\nint_strategy &lt;- c(\"ccd\", \"grid\", \"eb\")\nmodels &lt;- c(\"iid\", \"matern\")\nfits &lt;- matrix(nrow=length(approx_strategy)*length(int_strategy)*length(models), ncol=3)\nfits_marginals_b &lt;- vector(mode=\"list\", length=length(approx_strategy)*length(int_strategy)*length(models))\nfits_marginals_temp &lt;- vector(mode=\"list\", length=length(approx_strategy)*length(int_strategy)*length(models))\nfits_marginals_prec &lt;- vector(mode=\"list\", length=length(approx_strategy)*length(int_strategy)*length(models))\nindex_f &lt;- 0\nmodel_names &lt;- c()\nfor (m in models){\n  for(a in approx_strategy){\n    for (i in int_strategy){\n      index_f &lt;- index_f + 1\n      if (m==\"matern\"){\n        formula_approx &lt;- y ~ 0 + b0 + covtemp + covprec + f(s, model = spde)\n      }\n      else{\n        formula_approx &lt;- y ~ 0 + b0 + covtemp + covprec + f(s, model = \"iid\")\n      }\n      print(paste(a, \", \", i, \", \", m))\n      model_names &lt;- c(model_names, paste(a, \", \", i, \", \", m))\n      fit_approx &lt;- inla(formula, family = \"gaussian\",\n         data = inla.stack.data(stk.full),\n         control.inla = list(strategy = a, int.strategy = i),\n         control.compute = list(cpo = TRUE, dic = TRUE, waic = TRUE),\n         control.predictor = list(compute = TRUE, A = inla.stack.A(stk.full)))\n      fits[index_f,] &lt;- fit_approx$summary.fixed$mean\n      fits_marginals_b[[index_f]] &lt;- fit_approx$marginals.fixed$b0\n      fits_marginals_temp[[index_f]] &lt;- fit_approx$marginals.fixed$covtemp\n      fits_marginals_prec[[index_f]] &lt;- fit_approx$marginals.fixed$covprec\n  }\n  }\n}\n\n[1] \"gaussian ,  ccd ,  iid\"\n[1] \"gaussian ,  grid ,  iid\"\n[1] \"gaussian ,  eb ,  iid\"\n[1] \"simplified.laplace ,  ccd ,  iid\"\n[1] \"simplified.laplace ,  grid ,  iid\"\n[1] \"simplified.laplace ,  eb ,  iid\"\n[1] \"laplace ,  ccd ,  iid\"\n[1] \"laplace ,  grid ,  iid\"\n[1] \"laplace ,  eb ,  iid\"\n[1] \"gaussian ,  ccd ,  matern\"\n[1] \"gaussian ,  grid ,  matern\"\n[1] \"gaussian ,  eb ,  matern\"\n[1] \"simplified.laplace ,  ccd ,  matern\"\n[1] \"simplified.laplace ,  grid ,  matern\"\n[1] \"simplified.laplace ,  eb ,  matern\"\n[1] \"laplace ,  ccd ,  matern\"\n[1] \"laplace ,  grid ,  matern\"\n[1] \"laplace ,  eb ,  matern\"\nThis prints out all the combinations of approximation, integration strategy and correlation structure.\nNow we can compare the means of the parameters per different model set-up.\nfits_df &lt;- as.data.frame(fits)\nnames(fits_df) &lt;- c(\"b0\", \"covtemp\", \"covprec\")\nfits_df$models &lt;- model_names\nfits_df\n\n          b0  covtemp     covprec                               models\n1  -13.85789 2.049883 -0.01973532               gaussian ,  ccd ,  iid\n2  -15.29769 2.170035 -0.01550201              gaussian ,  grid ,  iid\n3  -13.11329 1.992509 -0.02262044                gaussian ,  eb ,  iid\n4  -13.85789 2.049883 -0.01973532     simplified.laplace ,  ccd ,  iid\n5  -15.29769 2.170035 -0.01550201    simplified.laplace ,  grid ,  iid\n6  -13.11329 1.992509 -0.02262044      simplified.laplace ,  eb ,  iid\n7  -13.85789 2.049883 -0.01973532                laplace ,  ccd ,  iid\n8  -15.29769 2.170035 -0.01550201               laplace ,  grid ,  iid\n9  -13.11329 1.992509 -0.02262044                 laplace ,  eb ,  iid\n10 -13.85789 2.049883 -0.01973532            gaussian ,  ccd ,  matern\n11 -15.29769 2.170035 -0.01550201           gaussian ,  grid ,  matern\n12 -13.11329 1.992509 -0.02262044             gaussian ,  eb ,  matern\n13 -13.85789 2.049883 -0.01973532  simplified.laplace ,  ccd ,  matern\n14 -15.29769 2.170035 -0.01550201 simplified.laplace ,  grid ,  matern\n15 -13.11329 1.992509 -0.02262044   simplified.laplace ,  eb ,  matern\n16 -13.85789 2.049883 -0.01973532             laplace ,  ccd ,  matern\n17 -15.29769 2.170035 -0.01550201            laplace ,  grid ,  matern\n18 -13.11329 1.992509 -0.02262044              laplace ,  eb ,  matern\nIt’s also possible to compare the posteriors themselves.\n# Combine dataframes\ndf_list_1 &lt;- lapply(fits_marginals_b, function(m) {\n  data.frame(x = m[,1], y = m[,2]) \n})\ndf_list_2 &lt;- lapply(fits_marginals_temp, function(m) {\n  data.frame(x = m[,1], y = m[,2]) \n})\ndf_list_3 &lt;- lapply(fits_marginals_prec, function(m) {\n  data.frame(x = m[,1], y = m[,2]) \n})\nnew_df_1 &lt;- do.call(rbind, df_list_1) \nnew_df_2 &lt;- do.call(rbind, df_list_2) \nnew_df_3 &lt;- do.call(rbind, df_list_3) \n# Add an ID column\nrow_counts &lt;- sapply(fits_marginals_b, nrow)\nids &lt;- factor(rep(1:length(fits_marginals_b), each = row_counts))\n\nWarning in rep(1:length(fits_marginals_b), each = row_counts): first element\nused of 'each' argument\n\nnew_df_1$id &lt;- ids\nnew_df_2$id &lt;- ids\nnew_df_3$id &lt;- ids\nggplot(new_df_1, aes(x = x, y = y, colour = id)) + \n  geom_line() + ggtitle(\"Posterior of B0\")\n\n\n\n\n\n\n\nggplot(new_df_2, aes(x = x, y = y, colour = id)) + \n  geom_line() + ggtitle(\"Posterior of Temperature parameter\")\n\n\n\n\n\n\n\nggplot(new_df_3, aes(x = x, y = y, colour = id)) + \n  geom_line() + ggtitle(\"Posterior of Precipitation parameter\")\nSo with this data set there is not really much different between estimation methods, regardless of the options used. I suppose that is a good sign?\nAnother useful feature is the set of functions that can be used to manipulate the marginal distributions: &lt;https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#sec:marginals. for example,\ninla.mmarginal(res$marginals.fixed$b0)\n\n[1] -14.90955\n\nplot(res$marginals.fixed$b0, type = \"l\", xlab = \"b0\", ylab = \"Density\")\n\n\n\n\n\n\n\n# This should give about the same answer:\nres$summary.fixed$mode[1]\n\n[1] -15.1714",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial modelling with INLA and SPDE</span>"
    ]
  },
  {
    "objectID": "inla_inlabru_tutorial_netherlands_data.html#inlabru",
    "href": "inla_inlabru_tutorial_netherlands_data.html#inlabru",
    "title": "3  Spatial modelling with INLA and SPDE",
    "section": "3.2 Inlabru",
    "text": "3.2 Inlabru\nNext, let’s write that model in inlabru and check if the estimates are the same. We can use the same mesh and spde function as we used before. And also the same formula! But what is especially nice about inlabru is that we don’t have to set up the stack with the projection matrices. We just model the response variable as a function of the covariates in the dataset we set up earlier and the locations of the observations. In inlabru we can use the sf dataset object directly. Because of this, it’s amazingly simple to fit the model, compared to INLA.\n\nformula &lt;- Value ~ Intercept(1) + covtemp + covprec + f(geometry, model = spde)\n# Fit the model for inlabru\nfit &lt;- bru(formula, data = air_sf_project, family = \"gaussian\")\n# Summarize the results\nsummary(fit)\n\ninlabru version: 2.12.0\nINLA version: 24.05.01-1\nComponents:\nIntercept: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\ncovtemp: main = linear(covtemp), group = exchangeable(1L), replicate = iid(1L), NULL\ncovprec: main = linear(covprec), group = exchangeable(1L), replicate = iid(1L), NULL\nf: main = spde(geometry), group = exchangeable(1L), replicate = iid(1L), NULL\nLikelihoods:\n  Family: 'gaussian'\n    Tag: ''\n    Data class: 'sf', 'grouped_df', 'tbl_df', 'tbl', 'data.frame'\n    Response class: 'numeric'\n    Predictor: Value ~ .\n    Used components: effects[Intercept, covtemp, covprec, f], latent[]\nTime used:\n    Pre = 1.68, Running = 0.699, Post = 0.363, Total = 2.75 \nFixed effects:\n             mean     sd 0.025quant 0.5quant 0.975quant    mode kld\nIntercept -15.298 21.347    -57.760  -15.172     26.458 -15.171   0\ncovtemp     2.170  2.095     -1.800    2.095      6.539   2.094   0\ncovprec    -0.016  0.203     -0.421   -0.014      0.382  -0.014   0\n\nRandom effects:\n  Name    Model\n    f SPDE2 model\n\nModel hyperparameters:\n                                          mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations  0.308 0.080      0.177    0.299\nTheta1 for f                             1.928 0.531      0.908    1.920\nTheta2 for f                            -4.134 0.692     -5.533   -4.122\n                                        0.975quant   mode\nPrecision for the Gaussian observations      0.489  0.283\nTheta1 for f                                 2.999  1.883\nTheta2 for f                                -2.808 -4.070\n\nDeviance Information Criterion (DIC) ...............: 231.65\nDeviance Information Criterion (DIC, saturated) ....: 71.69\nEffective number of parameters .....................: 16.69\n\nWatanabe-Akaike information criterion (WAIC) ...: 232.14\nEffective number of parameters .................: 14.00\n\nMarginal log-Likelihood:  -145.00 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nFortunately, that gives nearly the same estimates as from INLA There’s also a lot of additional useful output, including the spatial variance \\(\\tau\\) and spatial range \\(\\kappa\\) for the spatial field, respectively referred to as theta1 and theta2 by default (which are a log transformation of the parameters). We can also see the information criteria. We could have gotten this same output from INLA with summary(fit).\nThen let’s plot the predictions. In inlabru, we don’t have to make predictions at the same time as the model fitting, which is convenient.\n\nmap_prj &lt;- st_transform(map, crs = projMercator)\npredictions1 &lt;- predict(fit, newdata=dp, formula = ~ Intercept + covtemp + covprec + f)\npredictions2 &lt;- predict(fit, newdata=dp, formula = ~ f)\nggplot() +\ngeom_sf(data=predictions1, aes(color=mean)) +\n  scale_colour_gradient(low = \"blue\", high = \"yellow\")\n\n\n\n\n\n\n\n# Check the contribution of just the spatial field\nggplot() +\ngeom_sf(data=predictions2, aes(color=mean)) +\n  scale_colour_gradient(low = \"blue\", high = \"yellow\")\n\n\n\n\n\n\n\n\nLooks very nice. Of course, there are a lot more diagnostics we could consider, including looking at the posterior distributions of all parameters and hyperparameters and checking the information criteria. We could also consider the effect of setting priors on the SPDE parameters. This will end up making a difference for more complex models. I could also simulate data and see how well INLA recovers the parameters. There is also fitting a temporal model in both packages.\nBut for now, that concludes this section, showing how to fit spatial model in INLA and inlabru. The next section shows how to do this for a point process.\n\n\n\n\nMoraga, Paula. 2023. Spatial Statistics for Data Science: Theory and Practice with R. Chapman & Hall/CRC Data Science Series.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial modelling with INLA and SPDE</span>"
    ]
  },
  {
    "objectID": "inla_inlabru_lgcp_tutorial_netherlands_data.html",
    "href": "inla_inlabru_lgcp_tutorial_netherlands_data.html",
    "title": "4  Implementation of LGCP in INLA and INLABRU",
    "section": "",
    "text": "4.1 LGCP in INLA\nHaving completed air pollution modelling in INLA and inlabru, it is time to move on to the LGCP and the point process model. A couple of online resources about point process modelling in INLA include: https://becarioprecario.bitbucket.io/spde-gitbook/ch-stapp.html#sec:burkitt and https://www.paulamoraga.com/book-spatial/point-process-modeling.html. I’ll again be relying on the Moraga book for template code.\nThe model fit here has intensity specified as \\[\n\\lambda(s) = \\beta_0 + f(s)\n\\] Here I only use an intercept and spatial field to model the intensity, so we won’t have to worry about downloading any covariates.\nlibrary(INLA)\nlibrary(inlabru)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(rasterVis)\nNow for the data. I’ll use butterfly data from citizen science observations made in the Netherlands in 2023. The data is available in the GBIF repository https://doi.org/10.15468/dl.p5cy6n, which includes all butterfly observations uploaded to observation.org. So it’s necessary to filter the dataset down to just the Netherlands. I filter for the species Lasiommata megera. This leaves us with 2705 observations.\nfile_name &lt;- file.path(\"D:\", \"data\", \"gbif_observation_org_butterflies\", \"gbif_butterfly_observation-org\", \"gbif_subset_netherlands_lepidoptera.csv\")\nd &lt;- read_delim(file_name, delim='\\t', col_types=cols(infraspecificEpithet=col_character()))\nd %&gt;% filter(species==\"Lasiommata megera\") -&gt; d\ndim(d)\n\n[1] 2705   50\n\nd &lt;- st_as_sf(d, coords = c(\"decimalLongitude\", \"decimalLatitude\"))\nst_crs(d) &lt;- \"EPSG:4326\"\n#projMercator &lt;- \"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0\n#+x_0=0 +y_0=0 +k=1 +units=km +nadgrids=@null +wktext +no_defs\"\nprojMercator &lt;- st_crs(\"EPSG:3857\")$proj4string\n# Observed coordinates\nd &lt;- st_transform(d, crs = projMercator)\nBefore, I used the Netherlands map directly, but now there is some extra processing to do, due to little isolated boundaries within the main polygon of the Netherlands border.\nlayers &lt;- st_layers(file.path(\"D:\", \"data\", \"maps\", \"netherlands_bestuurlijkegrenzen_2021\", \"bestuurlijkegrenzen.gpkg\"))\n#print(str(layers))\nmap &lt;- st_read(file.path(\"D:\", \"data\", \"maps\", \"netherlands_bestuurlijkegrenzen_2021\", \"bestuurlijkegrenzen.gpkg\"), layer = \"landsgrens\")\n\nReading layer `landsgrens' from data source \n  `D:\\data\\maps\\netherlands_bestuurlijkegrenzen_2021\\bestuurlijkegrenzen.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 10425.16 ymin: 306846.2 xmax: 278026.1 ymax: 621876.3\nProjected CRS: Amersfoort / RD New\n\nmap &lt;- st_union(map)\nmap &lt;- st_as_sf(map)\n# there's a little isolated spec in the map!\nborder_polygon &lt;- st_cast(map, \"POLYGON\")\nborder_polygon &lt;- st_as_sfc(border_polygon)\ngeos &lt;- lapply(border_polygon, function(x) x[1])\n#for (g in geos){\n#  plot(st_polygon(g))\n#}\n# Get the border polygon\nborder_final &lt;- st_polygon(geos[[1]])\n# We still need sf object\nborder_final &lt;- st_sfc(border_final, crs=st_crs(map))\nborder_final &lt;- st_as_sf(border_final)\nplot(border_final)\n\n\n\n\n\n\n\nmap &lt;- border_final\nmap &lt;- st_transform(map, crs = projMercator)\ncoo &lt;- st_coordinates(d)\nggplot() + geom_sf(data = map) +\n  geom_sf(data = d) + coord_sf(datum = projMercator)\n\n\n\n\n\n\n\n# Save this for later\nst_write(map, file.path(\"D:\", \"data\", \"maps\", \"netherlands_bestuurlijkegrenzen_2021\", \"clean_nl_boundary.gpkg\"), append=F)\n\nDeleting layer `clean_nl_boundary' using driver `GPKG'\nWriting layer `clean_nl_boundary' to data source \n  `D:/data/maps/netherlands_bestuurlijkegrenzen_2021/clean_nl_boundary.gpkg' using driver `GPKG'\nWriting 1 features with 0 fields and geometry type Polygon.\nSo that gives us a very clean map, with the location of Lasiommata megera observations.\nAs in the previous example for air pollution data, we again create prediction points across the spatial region. These will be the locations at which the intensity is predicted.\n# raster grid covering map\ngrid &lt;- terra::rast(map, nrows = 50, ncols = 50)\n# coordinates of all cells\nxy &lt;- terra::xyFromCell(grid, 1:ncell(grid))\n# transform points to a sf object\ndp &lt;- st_as_sf(as.data.frame(xy), coords = c(\"x\", \"y\"),\n               crs = st_crs(map))\n\n# indices points within the map\nindicespointswithin &lt;- which(st_intersects(dp, map,\n                                           sparse = FALSE))\n\n# points within the map\ndp &lt;- st_filter(dp, map)\n\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp) + coord_sf(datum = projMercator)\n\n\n\n\n\n\n\ncoop &lt;- st_coordinates(dp)\nNext, build the mesh.\nloc.d &lt;- cbind(st_coordinates(map)[, 1], st_coordinates(map)[, 2])\n#mesh &lt;- inla.mesh.2d(loc=coo, max.edge = c(50000, 100000))\n#mesh &lt;- inla.mesh.2d(loc.domain=loc.d)\n#mesh &lt;- inla.mesh.2d(loc.domain = loc.d, max.edge = c(50, 100), crs=crs(d))\nmesh &lt;- inla.mesh.2d(loc.domain = loc.d, max.edge = c(50000, 100000), crs=crs(d))\nplot(mesh)\npoints(coo, col = \"red\")\naxis(1)\naxis(2)\n\n\n\n\n\n\n\n(nv &lt;- mesh$n)\n\n[1] 326\n\n(n &lt;- nrow(coo))\n\n[1] 2705\nUsing spde &lt;- inla.spde2.matern(mesh = mesh, alpha = 2, constr = TRUE) led to some differences in the inla and INLABRU models, mainly in that the intercept was quite different between the two. So I started to use the Penalized Complexity priors on the matern function instead:\nspde &lt;- inla.spde2.pcmatern(mesh = mesh, alpha = 2, constr = TRUE, prior.range = c(10, 0.01), prior.sigma = c(1, 0.01))\nIn the point process setting, it is necessary to use the method of Simpson et al. (2016). This uses a dual mesh to approximate the point process likelihood. It is called a dual mesh because a mesh of polygons is generated, each centered around the vertices of the triangles in the first mesh. The code to create the dual mesh is available in both the SPDE book and Moraga book.\nbook.mesh.dual &lt;- function(mesh) {\n    if (mesh$manifold=='R2') {\n        ce &lt;- t(sapply(1:nrow(mesh$graph$tv), function(i)\n            colMeans(mesh$loc[mesh$graph$tv[i, ], 1:2])))\n        library(parallel)\n        pls &lt;- mclapply(1:mesh$n, function(i) {\n            p &lt;- unique(Reduce('rbind', lapply(1:3, function(k) {\n            j &lt;- which(mesh$graph$tv[,k]==i)\n            if (length(j)&gt;0) \n            return(rbind(ce[j, , drop=FALSE],\n            cbind(mesh$loc[mesh$graph$tv[j, k], 1] +\n            mesh$loc[mesh$graph$tv[j, c(2:3,1)[k]], 1], \n            mesh$loc[mesh$graph$tv[j, k], 2] +\n            mesh$loc[mesh$graph$tv[j, c(2:3,1)[k]], 2])/2))\n            else return(ce[j, , drop=FALSE])\n            })))\n            j1 &lt;- which(mesh$segm$bnd$idx[,1]==i)\n            j2 &lt;- which(mesh$segm$bnd$idx[,2]==i)\n            if ((length(j1)&gt;0) | (length(j2)&gt;0)) {\n            p &lt;- unique(rbind(mesh$loc[i, 1:2], p,\n            mesh$loc[mesh$segm$bnd$idx[j1, 1], 1:2]/2 +\n            mesh$loc[mesh$segm$bnd$idx[j1, 2], 1:2]/2, \n            mesh$loc[mesh$segm$bnd$idx[j2, 1], 1:2]/2 +\n            mesh$loc[mesh$segm$bnd$idx[j2, 2], 1:2]/2))\n            yy &lt;- p[,2]-mean(p[,2])/2-mesh$loc[i, 2]/2\n            xx &lt;- p[,1]-mean(p[,1])/2-mesh$loc[i, 1]/2\n            }\n            else {\n            yy &lt;- p[,2]-mesh$loc[i, 2]\n            xx &lt;- p[,1]-mesh$loc[i, 1]\n            }\n            Polygon(p[order(atan2(yy,xx)), ])\n        })\n        return(SpatialPolygons(lapply(1:mesh$n, function(i)\n            Polygons(list(pls[[i]]), i))))\n    }\n    else stop(\"It only works for R2!\")\n}\ndmesh &lt;- book.mesh.dual(mesh)\nplot(dmesh)\naxis(1)\naxis(2)\nWe then do something a little tricky. The mesh is larger than the domain that the points were observed in or the study region. So the intersections between the polygons in the mesh and the locations in \\(D\\) are computed.\ndomain.polys &lt;- Polygons(list(Polygon(loc.d)), '0')\ndomainSP &lt;- SpatialPolygons(list(domain.polys))\ndomain_sf &lt;- st_as_sf(domainSP)\ndomain_sf &lt;- st_set_crs(domain_sf, projMercator)\nmesh_sf &lt;- st_as_sf(dmesh)\nmesh_sf &lt;- st_set_crs(mesh_sf, projMercator)\n# Check if the mesh polygons overlap with any of the locations \nw &lt;- sapply(1:length(dmesh), function(i) {\n  if(length(st_intersects(mesh_sf[i,], domain_sf)[[1]])&gt;0){\n    return(sf::st_area(sf::st_intersection(mesh_sf[i, ], domain_sf)))\n  }\n  else return(0)\n})\nsum(w)\n\n[1] 111060620373\n\nst_area(map)\n\n111060620373 [m^2]\n\n# Fun little exercise as an alternative way to calculate the weights.\n# dp &lt;- fm_pixels(mesh, dims = c(50, 50), mask = domain_sf)\n# # Project mesh basis functions to pixel locations and multiply by pixel weights (area)\n# # Sum these contributions for each basis function (mesh vertex)\n# A_pixels &lt;- fm_basis(mesh, loc = dp)\n# pixel_weights &lt;- st_area(dp)\n# # Ensure weights are simple numeric vector\n# pixel_weights &lt;- as.numeric(pixel_weights)\n# A_weighted &lt;- Diagonal(length(pixel_weights), pixel_weights) %*% A_pixels\n# w &lt;- Matrix::colSums(A_weighted)\n# w &lt;- as.vector(w)\n# sum(w)\nNotice that the weights and and the area of the domain are the same.\nWe can see in the plot below that we are left with a very nice looking mesh, where the black integration points fall within the Netherlands domain and red fall outside of it.\nplot(mesh)\nplot(domain_sf, add=T, col=\"green\")\npoints(mesh$loc[which(w &gt; 0), 1:2], col = \"black\", pch = 20)\npoints(mesh$loc[which(w == 0), 1:2], col = \"red\", pch = 20)\nNext, create the INLA stack, for both observations and for prediction points. In the first section I didn’t provide that much detail about the stack, so I’ll explain it in more detail here.\nFirst we create vectors for the observed response and for estimation. nv is the number of mesh nodes and n is the number of observations, where there will be a 0 for element of length nv and a 1 for each of length n. e.pp will consist of the integration weights w of length nv and a set of 0s of length n. This is for the numerical integration in the LGCP likelihood. Just as in the point process likelihood, we have two components: An intractable integral to be approximated, and a product of point locations.\ny.pp &lt;- rep(0:1, c(nv, n))\nlength(y.pp)\n\n[1] 3031\n\ne.pp &lt;- c(w, rep(0, n))\nlength(e.pp)\n\n[1] 3031\nNext set up the projection matrix. This will be used to compute the linear predictor, generally written \\[\n\\boldsymbol{\\eta} = \\boldsymbol{1}\\beta_0 + \\boldsymbol{Az}.\n\\] where \\(\\boldsymbol{z}=\\{z_1,z_2,,,z_{n_v}\\}\\) will be the values of the spatial field at the nodes.\nIn the model here, we have both nodes and observed locations, and the points for prediction. So we can write that all out as \\[\n\\begin{aligned}\n\\begin{pmatrix}\n\\boldsymbol{\\eta}_{node} \\\\\n\\boldsymbol{\\eta}_{obs}\n\\end{pmatrix} &=\n\\begin{pmatrix}\n\\boldsymbol{1}_{n_v}\\\\\n\\boldsymbol{1}_n\n\\end{pmatrix}\\beta_0 +\n\\begin{pmatrix}\n\\boldsymbol{A}_{n_v}\\\\\n\\boldsymbol{A}_n\n\\end{pmatrix}\\boldsymbol{z}\\\\\n\\boldsymbol{\\eta}_{npred} &=\n\\boldsymbol{1}_{npred}\n\\beta_0 +\n\\boldsymbol{A}_{npred}\n\\boldsymbol{z}\n\\end{aligned}\n\\] so that there is a response for each component, an intercept for each, and a projection matrix for each. The below code reflects this. Notice that for A.y and A.pp, we use the function inla.spde.make.A(). This function interpolates the values of the spatial field at the observed location using the triangles in the mesh that each location falls within.\n# Projection matrix for the integration points (mesh vertices)\n# It is diagonal here because the values of the mesh vertices are the integration points themselves.\nA.int &lt;- Diagonal(nv, rep(1, nv))\n# Projection matrix for observed points (event locations)\nA.y &lt;- inla.spde.make.A(mesh = mesh, loc = coo)\n# Projection matrix for mesh vertices and event locations\nA.pp &lt;- rbind(A.int, A.y)\n\n# We also create the projection matrix Ap.pp for the prediction locations.\nAp.pp &lt;- inla.spde.make.A(mesh = mesh, loc = coop)\nThen create the stacks themselves. We provide the response and estimation vectors to the data argument and the A projection matrices to the A argument. The effect argument takes the intercept and the index of spatial effects. If we had covariates, this is where we would provide them, also in a list.\n# stack for estimation\nstk.e.pp &lt;- inla.stack(tag = \"est.pp\",\ndata = list(y = y.pp, e = e.pp), \nA = list(1, A.pp),\neffects = list(list(b0 = rep(1, nv + n)), list(s = 1:nv)))\n\n# stack for prediction stk.p\nstk.p.pp &lt;- inla.stack(tag = \"pred.pp\",\ndata = list(y = rep(NA, nrow(coop)), e = rep(0, nrow(coop))),\nA = list(1, Ap.pp),\neffects = list(data.frame(b0 = rep(1, nrow(coop))),\n               list(s = 1:nv)))\n\n# stk.full has stk.e and stk.p\nstk.full.pp &lt;- inla.stack(stk.e.pp, stk.p.pp)\nFinally, we can fit the model. This looks about the same as in the continuous setting. Notice the different strategy options for both the integration strategy and for the posterior approximation itself. control.predictor returns the posterior marginals for the observed and node locations, with link=1 using the same link function as given in family = 'poisson'.\nformula &lt;- y ~ -1 + b0 + f(s, model = spde)\nres &lt;- inla(formula,  family = 'poisson',\n  data = inla.stack.data(stk.full.pp),\n  control.inla=list(int.strategy = 'grid', strategy=\"laplace\"),\n  control.predictor = list(compute = TRUE, link = 1,\n    A = inla.stack.A(stk.full.pp)),\n    E = inla.stack.data(stk.full.pp)$e,\n  control.fixed=list(prec = 0.001^2))\nWe can check the estimate for the intercept, as well as the parameters for the spatial field.\nres$summary.fixed\n\n        mean         sd 0.025quant  0.5quant 0.975quant      mode kld\nb0 -17.53067 0.01922721  -17.56835 -17.53067  -17.49299 -17.53067   0\n\nres$summary.hyperpar\n\n                   mean          sd  0.025quant    0.5quant  0.975quant\nRange for s 445.1015377 786.4644284 30.36464796 224.4312938 2262.621054\nStdev for s   0.7494826   0.9236917  0.05813083   0.4678162    3.150424\n                 mode\nRange for s 75.391472\nStdev for s  0.157495\nThen extract the predictions\nindex &lt;- inla.stack.index(stk.full.pp, tag = \"pred.pp\")$data\npred_mean &lt;- res$summary.fitted.values[index, \"mean\"]\npred_ll &lt;- res$summary.fitted.values[index, \"0.025quant\"]\npred_ul &lt;- res$summary.fitted.values[index, \"0.975quant\"]\ngrid$mean &lt;- NA\ngrid$ll &lt;- NA\ngrid$ul &lt;- NA\n\ngrid$mean[indicespointswithin] &lt;- pred_mean\ngrid$ll[indicespointswithin] &lt;- pred_ll\ngrid$ul[indicespointswithin] &lt;- pred_ul\nAnd plot the predicted intensity\nlevelplot(raster::brick(grid), layout = c(3, 1),\nnames.attr = c(\"Mean\", \"2.5 percentile\", \"97.5 percentile\"))\nWe have to make sure to get the domain for the sampler correct. The INLA code does it above, but inlabru by default does not.\nMore about that https://inlabru-org.github.io/inlabru/articles/2d_lgcp_plotsampling.html and actually the exact problem here: https://groups.google.com/g/r-inla-discussion-group/c/0bBC9bVV-L4 even though the problem was with preferential sampling. In the mesh-process R section above, you can see the manipulation to get the domains correct for INLA. Even with including sampler=domain_sf here, the estimates are not exactly the same as INLA, but closer than it was before.\nips &lt;- fm_int(\n  domain = list(geometry = mesh),\n  samplers = domain_sf\n)\n\nggplot() +\n  geom_fm(data = mesh) +\n  gg(ips, aes(size = weight)) +\n  scale_size_area(max_size = 1)\n# TODO: Make sure I get the same result as inla. Options and mesh are off\n# Oh nice we can name the intercept but then need to subtract 1 to get rid of the default intercept\nformula_inlabru &lt;- geometry ~ -1 + b0(1) + f(geometry, model = spde)\nfit1 &lt;- lgcp(formula_inlabru, data=d, samplers=domain_sf, domain = list(geometry = mesh), \n             options = list(control.inla=list(int.strategy = 'grid', strategy=\"laplace\"),\n                            control.fixed=list(prec = 0.001^2)))\n#                          control.compute=list(config=TRUE),\n#                          control.results=list(return.marginals.random = TRUE,\n#                                               return.marginals.predictor = TRUE),\n#                          control.predictor = list(compute = TRUE)))\n\nsummary(fit1)\n\ninlabru version: 2.12.0\nINLA version: 24.05.01-1\nComponents:\nb0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nf: main = spde(geometry), group = exchangeable(1L), replicate = iid(1L), NULL\nLikelihoods:\n  Family: 'cp'\n    Tag: ''\n    Data class: 'sf', 'data.frame'\n    Response class: 'numeric'\n    Predictor: geometry ~ .\n    Used components: effects[b0, f], latent[]\nTime used:\n    Pre = 1.64, Running = 1.44, Post = 0.62, Total = 3.7 \nFixed effects:\n      mean    sd 0.025quant 0.5quant 0.975quant    mode kld\nb0 -19.747 0.155     -20.05  -19.747    -19.442 -19.747   0\n\nRandom effects:\n  Name    Model\n    f SPDE2 model\n\nModel hyperparameters:\n                mean       sd 0.025quant 0.5quant 0.975quant    mode\nRange for f 13334.56 23573.44     908.26  6720.85   67803.29 2255.42\nStdev for f    13.35    16.48       1.03     8.32      56.18    2.80\n\nDeviance Information Criterion (DIC) ...............: -135688.90\nDeviance Information Criterion (DIC, saturated) ....: NA\nEffective number of parameters .....................: -136646.26\n\nWatanabe-Akaike information criterion (WAIC) ...: 6841.36\nEffective number of parameters .................: 2249.31\n\nMarginal log-Likelihood:  -68809.40 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\npredictions1 &lt;- predict(fit1, newdata=dp, formula = ~ b0 + f)\npredictions2 &lt;- predict(fit1, newdata=dp, formula = ~ f)\nggplot() +\ngeom_sf(data=predictions1, aes(color=mean)) +\n  scale_colour_gradient(low = \"blue\", high = \"yellow\")\n\n\n\n\n\n\n\n# Check the contribution of just the spatial field\nggplot() +\ngeom_sf(data=predictions2, aes(color=mean)) +\n  scale_colour_gradient(low = \"blue\", high = \"yellow\")\nThat concludes my demonstrations using the INLA and inlabru software. This may hopefully serve as a useful reference for someone, at least for myself. As can be seen, the INLA modelling approach is quite flexible and can be used to construct all sorts of models. It isn’t the simplest approach though, so understanding each element can take a while, but the advantage is that once you are comfortable with it, there is a lot that can be done all under the same INLA umbrella.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Implementation of LGCP in INLA and INLABRU</span>"
    ]
  },
  {
    "objectID": "inla_inlabru_lgcp_tutorial_netherlands_data.html#lgcp-in-inla",
    "href": "inla_inlabru_lgcp_tutorial_netherlands_data.html#lgcp-in-inla",
    "title": "4  Implementation of LGCP in INLA and INLABRU",
    "section": "",
    "text": "Simpson, D., J. B. Illian, F. Lindgren, S. H. Sørbye, and H. Rue. 2016. “Going Off Grid: Computationally Efficient Inference for Log-Gaussian Cox Processes.” Biometrika 103 (1): 49–70. https://doi.org/10.1093/biomet/asv064.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Implementation of LGCP in INLA and INLABRU</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Blangiardo, Marta, and Michela Cameletti. 2015. Spatial and\nSpatio-Temporal Bayesian Models with r-INLA. John Wiley & Sons,\nLtd. https://doi.org/https://doi.org/10.1002/9781118950203.\n\n\nDiggle, Peter J., Paula Moraga, Barry Rowlingson, and Benjamin M.\nTaylor. 2013. “Spatial and\nSpatio-Temporal\nLog-Gaussian Cox\nProcesses: Extending the\nGeostatistical Paradigm.”\nStatistical Science 28 (4): 542–63. https://doi.org/10.1214/13-STS441.\n\n\nLindgren, Finn, Håvard Rue, and Johan Lindström. 2011. “An\nExplicit Link Between Gaussian Fields and Gaussian\nMarkov Random Fields: The Stochastic Partial Differential\nEquation Approach.” Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology) 73 (4): 423–98. https://doi.org/10.1111/j.1467-9868.2011.00777.x.\n\n\nMoraga, Paula. 2023. Spatial Statistics for Data\nScience: Theory and Practice with\nR. Chapman & Hall/CRC Data Science Series.\n\n\nRue, Håvard, Sara Martino, and Nicolas Chopin. 2009. “Approximate\nBayesian Inference for Latent Gaussian Models\nby Using Integrated Nested Laplace Approximations.”\nJournal of the Royal Statistical Society: Series B (Statistical\nMethodology) 71 (2): 319–92. https://doi.org/10.1111/j.1467-9868.2008.00700.x.\n\n\nSimpson, D., J. B. Illian, F. Lindgren, S. H. Sørbye, and H. Rue. 2016.\n“Going Off Grid: Computationally Efficient Inference for\nLog-Gaussian Cox Processes.” Biometrika 103\n(1): 49–70. https://doi.org/10.1093/biomet/asv064.",
    "crumbs": [
      "References"
    ]
  }
]