[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Log Gaussian Cox Processes and INLA: Theory and Application",
    "section": "",
    "text": "Preface\nHello!\nThis is a blog post by Max Savery. Go see the next section to learn what it is about.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a blog post about Log-Gaussian Cox Processes (LGCP), mainly. It is a follow up to the earlier blog post I made about Poisson Point Process models. In fact, the LGCP is an extension of the Poisson Point Process, in that a latent correlation structure is included in the point process model. Specifically, the LGCP uses a Gaussian Random Field to characterize the intensity of the point process. Doing so allows us to take into account spatial or temporal correlation across our region or period of interest. Statistically, this is quite advantageous compared to the Poisson Point Process where spatial correlation could only be accounted for via covariates. However, the inclusion of the random field presents mathematical and computational challenges. These challenges are addressed by making use of the Integrated Nested Laplace Approximation (INLA) approach to posterior approximation.\nThe general structure of this post is as follows: I will first introduce the LGCP and accompanying theory, but only briefly, as most of the theory is the same as the Poisson Process. Then, I’ll spend some time covering the INLA method–which involves much more theory–and the accompanying R packages INLA and inlabru. I’ll compare these packages for modelling point processes as well as for continuous measures. At some point I will end the post and continue later with another about fitting multiple likelihood models in INLA, such as for the purpose of accounting for preferential sampling or including multiple datasets. Perhaps, at the end of this blog, you will be in a similar state as represented in the cartoon below.\n\n\n\nhappy face\n\n\nI am writing about this because having a model that incorporates spatial (and/or temporal) correlation is necessary to properly model both point processes (such as the Presence-Only data of the last blog) or continuous measures in spatiotemporal domains (such as air pollution). Not including correlation means that you then assume the covariates included in your model are sufficient to account for spatial fluctuations in the response variable. The problem with this is that is there no direct component in the model that relates the behavior of the process at one location to the behavior at another location. The Gaussian field in the LGCP does exactly this by taking into account the spatial dependence between measurement locations.\nEven though the only thing we want to do is include spatial dependence, this quickly becomes a more technical model than the Poisson process. We have to first learn a whole new method of approximation (INLA) and a new software framework to be able to efficiently implement the LGCP. However, the advantage is the INLA method/software is sufficiently general so that we can tackle a whole set of spatial-temporal modelling problems using the same approach. In fact, the INLA approach allows us to (easily???) work with any continuous, discrete, or point-type data that has latent Gaussian structure, and makes possible the combination of likelihoods of different datasets as long as the underlying processes share a set of parameters. Pretty useful stuff.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "theory.html",
    "href": "theory.html",
    "title": "2  Theory",
    "section": "",
    "text": "2.1 LGCP\nA Poisson point process is a stochastic set of points indexed in a bounded n-dimensional space, where the number of points in the set is a Poisson distributed random variable. The LGCP extends the process by explicitly incorporating spatial or temporal correlation into the definition of the intensity of the process. Diggle et al. (2013) is a good reference for these type of models. I’ll give some details below, and then discuss the INLA approach for actually estimating the model.\nThe Poisson Point Process is characterized by an intensity that can be defined as the expected number of observations per unit area. For a given region \\(A\\), the number of individuals occurring within the region will be Poisson distributed. The mean of the distribution can be defined by the integral of the intensity over the region: \\[\n\\Delta(A) = \\int_A \\lambda(s)ds\n\\] Sadly, the intensity is an unobservable and unknown process, and therefore must be modelled as a function of covariates. The intensity takes the general parameterization as \\[\n\\lambda(s) = \\exp[\\alpha + \\beta'x(s)]\n\\] In the previous blog I discussed incorporating sampling bias, but I will leave that out now. Some version of this will come back when discussing preferential sampling.\nNow, in the LGCP we incorporate a Gaussian Random Field (also described as Gaussian spatial field, spatial random effect, or latent spatial field. I’m not sure exactly when the difference in description leads to a distinctly different spatial model) to incorporate spatial correlation in the model of intensity. \\[\n\\lambda(s) = \\exp[\\alpha + \\beta'x(s) + f_s(s)]\n\\] The spatial effect is denoted as \\(f_s(s)\\) in order to distinguish from temporal contributions. \\(f_s(s)\\) can be thought of as a continuous spatial effect that is evaluated at certain locations.\nThe main thing is that, once we incorporate \\(f_s(s)\\), we are forced to use a covariance structure that depends on the resolution of the grid or mesh that will be used to approximate the continuous spatial effect. Furthermore, we are still stuck with the problem of an intractable integral in the likelihood of the LGCP (as in the case of the Poisson process). Thus, the computational complexity makes this approach prohibitively costly when using MCMC approaches to estimate the model in any region of practical interest. Hence, other approximations to the posterior of the parameters in the LGCP is necessary.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theory</span>"
    ]
  },
  {
    "objectID": "theory.html#inla",
    "href": "theory.html#inla",
    "title": "2  Theory",
    "section": "2.2 INLA",
    "text": "2.2 INLA\nThe Integrated Nested Laplace Approximation has been proposed by Rue, Martino, and Chopin (2009) as a fast way to estimate models with latent Gaussian structure. The LGCP is certainly an instance of models of this class. In the following description of the model, I present it nearly identically to the original paper, and/or from other very well-done sources, including:\n\n\n\n\n\nThere is nothing new under the sun here. I’m just condensing some resources for my own reference. So, taking notation directly from the Rue paper, INLA is concerned with models with an additive predictor that takes the general form \\[\n\\eta_i = \\alpha + \\sum^{n_f}_{j=1}f^{(j)}(u_{ij}) + \\sum^{n_{\\beta}}_{k=1} \\beta_kz_{ki} + \\epsilon_i\n\\] where \\(\\alpha\\) is an intercept, \\(f\\) are unknown functions of \\(\\mathbf{u}\\), \\(\\beta\\) are the typical fixed effects for covariates \\(\\mathbf{z}\\), and \\(\\epsilon_i\\) is the additional noise in the process. Additionally, \\(\\mathbf{x}\\) will refer to the vector of latent Gaussian variables (Gaussian by definition of the prior), and \\(\\bm{\\theta}\\) the vector of hyperparameters that we usually deal with in a Bayesian model. These need not be inherently Gaussian. In the book of Blangiardo and Cameletti (2015), hyperparameters and latent field are denoted by \\(\\psi\\) and \\(\\theta\\) respectively, which is clearer than using \\(\\mathbf{x}\\) to describe the latent field, but here I’m sticking to the original notation.\nStill taking from the notation of the Rue paper, the joint posterior of hyperparameters and latent variables is written as \\[\n\\begin{aligned}\n\\pi(\\mathbf{x},\\mathbf{\\theta}|\\mathbf{y})\\propto\\pi(\\mathbf{\\theta})\\pi(\\mathbf{x}|\\mathbf{\\theta})\\prod_{i\\in I}\\pi(y_i|x_i,\\mathbf{\\theta})\\\\\n\\propto\\pi(\\mathbf{\\theta})|Q(\\mathbf{\\theta})|^{1/2}\\exp\\bigg[-\\frac{1}{2}\\mathbf{x}^TQ(\\mathbf{\\theta})\\mathbf{x}+\\sum_{i\\in I}\\log\\{\\pi(y_i|x_i,\\mathbf{\\theta})\\}\\bigg],\n\\end{aligned}\n\\] where \\(\\pi(y_i|x_i,\\mathbf{\\theta})\\) is the distribution of the response variable and \\(|Q(\\mathbf{\\theta})|^{1/2}\\exp\\bigg[-\\frac{1}{2}\\mathbf{x}^TQ(\\mathbf{\\theta})\\mathbf{x}\\bigg]\\) is the Gaussian prior with a mean of 0 precision matrix \\(Q\\) on the GRMF conditional on \\(\\mathbf{\\theta}\\).\nThe main point of INLA is to make the nested approximations of the posterior marginals of the hyperparameters and (even more importantly) the latent field. The posterior marginal of one component of the hyperparameters is \\[\n\\pi(\\theta_j|\\mathbf{y}) = \\int \\pi(\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta}_{-j}.\n\\] Notice the \\(-j\\) components are integrated out. The posterior marginal of a component of the latent field is \\[\n\\pi(x_i|\\mathbf{y}) = \\int \\pi(x_i|\\mathbf{\\theta},\\mathbf{y})\\pi(\\mathbf{\\theta}|\\mathbf{y})d\\mathbf{\\theta},\n\\] again, sticking to the \\(\\mathbf{x}\\) notation.\nFollowing the procedure of INLA, first \\(\\pi(\\theta|y)\\) will need to be approximated, and then \\(\\pi(x_i|\\theta,y)\\). The integration to approximate \\(\\pi(x_i|y)\\) will be done numerically. The approximations are done with the Laplace Approximation, or a Gaussian-type approximation. To approximate the first quantity \\(\\pi(\\theta|y)\\), a Laplace approximation is used from Tierney and Kadane, 1986: \\[\n\\tilde{\\pi}(\\theta|y) \\propto \\frac{\\pi(x,\\theta,y)}{\\tilde{\\pi}_G(x|\\theta,y)}\\bigg|_{x=x^*(\\theta)} = \\frac{\\pi(y|x,\\theta)\\pi(x|\\theta)\\pi(\\theta)}{\\tilde{\\pi}_G(x|\\theta,y)}\\bigg|_{x=x^*(\\theta)}.\n\\] Interestingly, this approximation uses a NESTED approximation, \\(\\tilde{\\pi}_G(x|\\theta,y)\\). \\(\\tilde{\\pi}_G\\) is the Gaussian approximation to the latent field, which works because we already know the latent field to be prior-ly distributed as Gaussian.\nThe contribution of this paper is to then approximate \\(\\pi(x_i|, \\theta, y)\\) with a Laplace Approximation (LA), beyond just a simple Gaussian. The paper discusses 3 approximations to \\(\\pi(x_i|\\theta, y)\\): the Gaussian, LA, and a simplified LA.\nThe LA approximation is written as \\[\n\\tilde{\\pi}_{LA}(x_i|\\theta,y) \\propto \\frac{\\pi(x,\\theta,y)}{\\tilde{\\pi}_{GG}(x_{-i}|x_i,\\theta,y)}\\bigg|_{x_{-i}=x^*_{-i}(x_i,\\theta)}.\n\\] This can be computationally intensive since it is necessary to recompute \\(\\tilde{\\pi}_{GG}(x_{-i}|x_i,\\theta,y)\\) for each element of \\(x\\) and \\(\\theta\\). Why?\nOnce the approximations of \\(\\tilde{\\pi}(\\theta|y)\\) of \\(\\tilde{\\pi}(x_i|\\theta,y)\\) are found, the marginals for each element \\(x_i\\) can be approximated:\n\\[\n\\tilde{\\pi}(x_i|y) \\approx\\sum\\tilde{\\pi}(x_i|\\theta^{(i)},y)\\tilde{\\pi}(\\theta^{(j)}|y)\\Delta_j\n\\] for integration points \\(\\theta^{(i)}\\) which are found by the exploration of the posterior for \\(\\theta\\). This is done in a few different ways, for example by creating a standardized \\(Z\\) variable around the mode and inverse hessian of \\(\\log \\tilde\\pi}(\\theta|y)\\). There are quite a few more details to this part that I won’t discuss here, but may come up when up using the INLA software itself (choosing the integration scheme, for instance).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theory</span>"
    ]
  },
  {
    "objectID": "theory.html#spde-connection",
    "href": "theory.html#spde-connection",
    "title": "2  Theory",
    "section": "2.3 SPDE connection",
    "text": "2.3 SPDE connection\nThe Stochastic Partial Differential Equation (SPDE) approach is an innovation from (lindgren-2011?) on the INLA method. The approach allows the representation of a continuous Gaussian Field with a Gaussian Markov Random Field. The connection to the (rue-2009?) paper is that (lindgren-2011?) connected the GRMF models of INLA to specific (continuous) GFs. As spatial statisticians, we care about the continuous case more than approximations in discrete space, and SPDE connected the two.\nTo be slightly more specific, it turns out a GF with Matern covariance is a solution to a specific SPDE. The solution to the SPDE is found via a set of basis functions defined on a triangulation of the spatial region. The resulting GRMF is an approximation to the SPDE. So this approach allows us to approximate the solution of the SPDE with a GRMF that corresponds to a specific GF with matern covariance.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theory</span>"
    ]
  },
  {
    "objectID": "theory.html#mesh",
    "href": "theory.html#mesh",
    "title": "2  Theory",
    "section": "2.4 Mesh",
    "text": "2.4 Mesh\nThe mesh is is used to project the discrete GRMF to the continuous GF using some spatial weights defined at the vertices of the mesh (did I say that right?). From the Moraga tutorial above, she says the continuous process is estimated using a weighted average of the process at the vertices of the discrete triangulation mesh.\nThe projection matrix maps the GMRF from the observations to the triangulation nodes. The observation will be the weighted average using the weights and values from the triangulation and projection matrix. \\[\nS(x_i) \\approx \\sum^G_{g=1}A_{ig}S_g\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theory</span>"
    ]
  },
  {
    "objectID": "theory.html#off-the-grid",
    "href": "theory.html#off-the-grid",
    "title": "2  Theory",
    "section": "2.5 Off the grid",
    "text": "2.5 Off the grid\n(simpson-2016?) developed an approach for approximating the likelihood of the LGCP within the INLA framework, using a random field \\[\nZ(s) = \\sum^n_{i=1}z_i\\phi_i(s)\n\\] TODO: 1-10: Think about what simpson did. ## Concluding the section\nThat was a bit of a longer-than-intended dive into INLA. But INLA is definitely a more mathematically complicated method than other approximations, and I wanted to solidify the details for myself. It’s not always that way, sometimes it’s nice just to jump straight into the application, but with INLA it seemed prudent to pick over the details. Now, onto using the software with some of my own data and use-cases.\nAnyway, having covered, the necessary bits of INLA and LGCPs, I’ll now proceed to discuss the modelling procedure using INLA and inlabru. I’ll compare the software, first for a continuous measurement and then for point data using an LGCP!\n\n\n\n\nBlangiardo, Marta, and Michela Cameletti. 2015. John Wiley & Sons, Ltd. https://doi.org/https://doi.org/10.1002/9781118950203.\n\n\nDiggle, Peter J., Paula Moraga, Barry Rowlingson, and Benjamin M. Taylor. 2013. “Spatial and Spatio-Temporal Log-Gaussian Cox Processes: Extending the Geostatistical Paradigm.” Statistical Science 28 (4): 542–63. https://doi.org/10.1214/13-STS441.\n\n\nRue, Håvard, Sara Martino, and Nicolas Chopin. 2009. “Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 71 (2): 319–92. https://doi.org/10.1111/j.1467-9868.2008.00700.x.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theory</span>"
    ]
  },
  {
    "objectID": "inla_inlabru_tutorial_netherlands_data.html",
    "href": "inla_inlabru_tutorial_netherlands_data.html",
    "title": "3  Spatial modelling with INLA and SPDE",
    "section": "",
    "text": "Having covered the need-to-know material for INLA and SPDE, next I’m going to build a spatial model using both INLA and inlabru. Comparing the two packages is useful because, though they can be expected to give the same results in most cases, the implementation is a bit different. Knowing when using the more complicated INLA can be justified is a useful exercise, I think.\nI found https://becarioprecario.bitbucket.io/spde-gitbook/ch-lcox.html and https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html good for details about inla and such specifications. However, this initial tutorial follows that of the book of Paula Moraga https://www.paulamoraga.com/book-spatial/sec-geostatisticaldataSPDE.html, because I found her writing a bit more concise and clear.\nThe model fit here is a simple geostatistical one: \\[\nY_i \\sim N(u_i, \\sigma^2)\\\\\nu_i = \\beta_0\\cdot\\text{temp} + \\beta_1\\cdot\\text{precipitation}_i + S(x_i).\n\\] So it’s a typical Gaussian distributed variable with underlying latent structure. I’m not sure what the most accurate statistical way to say this is; potentially a spatial random effect “modelled as a Gaussian process”. This is what Paula Moraga calls “geostatistical data”, that is, measurements taken at discrete locations but used to describe or estimate a spatially continuous process, such as air pollution. It is very similar to a mixed model with a random effect. In this case the random effect is Gaussian distributed and spatially indexed.\nIt’s a simple model, so let’s get straight to data downloading and processing. I download air pollution data in the Netherlands for the year of 2023 from https://eeadmz1-downloads-webapp.azurewebsites.net/.\n\nlibrary(INLA)\n\nLoading required package: Matrix\n\n\nLoading required package: sp\n\n\nThis is INLA_24.05.01-1 built 2024-05-01 18:49:50 UTC.\n - See www.r-inla.org/contact-us for how to get help.\n - List available models/likelihoods/etc with inla.list.models()\n - Use inla.doc(&lt;NAME&gt;) to access documentation\n\nlibrary(inlabru)\n\nLoading required package: fmesher\n\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(rnaturalearth)\nlibrary(ggplot2)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nlibrary(terra)\n\nterra 1.7.78\n\nlibrary(arrow)\n\n\nAttaching package: 'arrow'\n\n\nThe following object is masked from 'package:terra':\n\n    buffer\n\n\nThe following object is masked from 'package:utils':\n\n    timestamp\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:terra':\n\n    intersect, union\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\nlibrary(readr)\n#data_path &lt;- file.path(\"D:\", \"data\", \"air_quality_data\", \"belgium_eea\", \"E1a\")\ndata_path &lt;- file.path(\"D:\", \"data\", \"air_quality_data\", \"netherlands_eea\", \"E1a\")\nd &lt;- open_dataset(data_path)\nd\n\nFileSystemDataset with 199 Parquet files\n12 columns\nSamplingpoint: string\nPollutant: int32\nStart: timestamp[ns] not null\nEnd: timestamp[ns] not null\nValue: decimal128(38, 18)\nUnit: string\nAggType: string\nValidity: int32 not null\nVerification: int32 not null\nResultTime: timestamp[ns] not null\nDataCapture: decimal128(38, 18)\nFkObservationLog: string\n\n# 8 Nitrogen dioxide\n# 5 Particulate matter &lt; 10 µm\n# 6001  Particulate matter &lt; 2.5 µm\nd %&gt;% group_by(Samplingpoint) |&gt; filter(Pollutant==6001, Value&gt;-1) |&gt; collect() -&gt; df\ndf\n\n# A tibble: 442,873 × 12\n# Groups:   Samplingpoint [53]\n   Samplingpoint   Pollutant Start               End                 Value Unit \n   &lt;chr&gt;               &lt;int&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;\n 1 NL/SPO-NL00007…      6001 2023-01-01 01:00:00 2023-01-01 02:00:00 131.  ug.m…\n 2 NL/SPO-NL00007…      6001 2023-01-01 02:00:00 2023-01-01 03:00:00  86.2 ug.m…\n 3 NL/SPO-NL00007…      6001 2023-01-01 03:00:00 2023-01-01 04:00:00  30.7 ug.m…\n 4 NL/SPO-NL00007…      6001 2023-01-01 04:00:00 2023-01-01 05:00:00  11   ug.m…\n 5 NL/SPO-NL00007…      6001 2023-01-01 05:00:00 2023-01-01 06:00:00   9.1 ug.m…\n 6 NL/SPO-NL00007…      6001 2023-01-01 06:00:00 2023-01-01 07:00:00   5.3 ug.m…\n 7 NL/SPO-NL00007…      6001 2023-01-01 07:00:00 2023-01-01 08:00:00   2.8 ug.m…\n 8 NL/SPO-NL00007…      6001 2023-01-01 08:00:00 2023-01-01 09:00:00   2.3 ug.m…\n 9 NL/SPO-NL00007…      6001 2023-01-01 10:00:00 2023-01-01 11:00:00  -0.6 ug.m…\n10 NL/SPO-NL00007…      6001 2023-01-01 11:00:00 2023-01-01 12:00:00   1.4 ug.m…\n# ℹ 442,863 more rows\n# ℹ 6 more variables: AggType &lt;chr&gt;, Validity &lt;int&gt;, Verification &lt;int&gt;,\n#   ResultTime &lt;dttm&gt;, DataCapture &lt;dbl&gt;, FkObservationLog &lt;chr&gt;\n\n\nAdd locations to the sampling stations.\n\ndf\n\n# A tibble: 442,873 × 12\n# Groups:   Samplingpoint [53]\n   Samplingpoint   Pollutant Start               End                 Value Unit \n   &lt;chr&gt;               &lt;int&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;\n 1 NL/SPO-NL00007…      6001 2023-01-01 01:00:00 2023-01-01 02:00:00 131.  ug.m…\n 2 NL/SPO-NL00007…      6001 2023-01-01 02:00:00 2023-01-01 03:00:00  86.2 ug.m…\n 3 NL/SPO-NL00007…      6001 2023-01-01 03:00:00 2023-01-01 04:00:00  30.7 ug.m…\n 4 NL/SPO-NL00007…      6001 2023-01-01 04:00:00 2023-01-01 05:00:00  11   ug.m…\n 5 NL/SPO-NL00007…      6001 2023-01-01 05:00:00 2023-01-01 06:00:00   9.1 ug.m…\n 6 NL/SPO-NL00007…      6001 2023-01-01 06:00:00 2023-01-01 07:00:00   5.3 ug.m…\n 7 NL/SPO-NL00007…      6001 2023-01-01 07:00:00 2023-01-01 08:00:00   2.8 ug.m…\n 8 NL/SPO-NL00007…      6001 2023-01-01 08:00:00 2023-01-01 09:00:00   2.3 ug.m…\n 9 NL/SPO-NL00007…      6001 2023-01-01 10:00:00 2023-01-01 11:00:00  -0.6 ug.m…\n10 NL/SPO-NL00007…      6001 2023-01-01 11:00:00 2023-01-01 12:00:00   1.4 ug.m…\n# ℹ 442,863 more rows\n# ℹ 6 more variables: AggType &lt;chr&gt;, Validity &lt;int&gt;, Verification &lt;int&gt;,\n#   ResultTime &lt;dttm&gt;, DataCapture &lt;dbl&gt;, FkObservationLog &lt;chr&gt;\n\ndf &lt;- df %&gt;%\n   #mutate(Samplingpoint = str_remove(Samplingpoint, \"BE/\"))\n   mutate(Samplingpoint = str_remove(Samplingpoint, \"NL/\"))\n#station_location_path &lt;- file.path(\"D:\", \"data\", \"air_quality_data\", \"eea_stations_2023\", \"belgium_stations_2023.csv\")\nstation_location_path &lt;- file.path(\"D:\", \"data\", \"air_quality_data\", \"eea_stations_2023\", \"nl_stations_2023.csv\")\nstation_locations &lt;- read_csv(station_location_path)\n\nRows: 4685 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (19): Country, Air Quality Network, Air Quality Network Name, Air Qualit...\ndbl  (8): Year, Air Pollution Level, Data Coverage, Verification, Longitude,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstation_locations &lt;- rename(station_locations, Samplingpoint = 'Sampling Point Id')\nstation_locations %&gt;% \n  distinct(Samplingpoint, Longitude, Latitude) -&gt; unique_locations\nmerged_df &lt;- df %&gt;% \n  left_join(unique_locations %&gt;% select(Samplingpoint, Longitude, Latitude), by = \"Samplingpoint\")\nmerged_df\n\n# A tibble: 442,873 × 14\n# Groups:   Samplingpoint [53]\n   Samplingpoint   Pollutant Start               End                 Value Unit \n   &lt;chr&gt;               &lt;int&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;\n 1 SPO-NL00007_06…      6001 2023-01-01 01:00:00 2023-01-01 02:00:00 131.  ug.m…\n 2 SPO-NL00007_06…      6001 2023-01-01 02:00:00 2023-01-01 03:00:00  86.2 ug.m…\n 3 SPO-NL00007_06…      6001 2023-01-01 03:00:00 2023-01-01 04:00:00  30.7 ug.m…\n 4 SPO-NL00007_06…      6001 2023-01-01 04:00:00 2023-01-01 05:00:00  11   ug.m…\n 5 SPO-NL00007_06…      6001 2023-01-01 05:00:00 2023-01-01 06:00:00   9.1 ug.m…\n 6 SPO-NL00007_06…      6001 2023-01-01 06:00:00 2023-01-01 07:00:00   5.3 ug.m…\n 7 SPO-NL00007_06…      6001 2023-01-01 07:00:00 2023-01-01 08:00:00   2.8 ug.m…\n 8 SPO-NL00007_06…      6001 2023-01-01 08:00:00 2023-01-01 09:00:00   2.3 ug.m…\n 9 SPO-NL00007_06…      6001 2023-01-01 10:00:00 2023-01-01 11:00:00  -0.6 ug.m…\n10 SPO-NL00007_06…      6001 2023-01-01 11:00:00 2023-01-01 12:00:00   1.4 ug.m…\n# ℹ 442,863 more rows\n# ℹ 8 more variables: AggType &lt;chr&gt;, Validity &lt;int&gt;, Verification &lt;int&gt;,\n#   ResultTime &lt;dttm&gt;, DataCapture &lt;dbl&gt;, FkObservationLog &lt;chr&gt;,\n#   Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;\n\nstation_locations_sf &lt;- st_as_sf(station_locations, coords = c(\"Longitude\", \"Latitude\"))\nair_sf &lt;- st_as_sf(merged_df, coords = c(\"Longitude\", \"Latitude\"))\nst_crs(air_sf) &lt;- \"EPSG:4326\"\nair_sf\n\nSimple feature collection with 442873 features and 12 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3.8386 ymin: 50.888 xmax: 6.9195 ymax: 53.3304\nGeodetic CRS:  WGS 84\n# A tibble: 442,873 × 13\n# Groups:   Samplingpoint [53]\n   Samplingpoint   Pollutant Start               End                 Value Unit \n * &lt;chr&gt;               &lt;int&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;\n 1 SPO-NL00007_06…      6001 2023-01-01 01:00:00 2023-01-01 02:00:00 131.  ug.m…\n 2 SPO-NL00007_06…      6001 2023-01-01 02:00:00 2023-01-01 03:00:00  86.2 ug.m…\n 3 SPO-NL00007_06…      6001 2023-01-01 03:00:00 2023-01-01 04:00:00  30.7 ug.m…\n 4 SPO-NL00007_06…      6001 2023-01-01 04:00:00 2023-01-01 05:00:00  11   ug.m…\n 5 SPO-NL00007_06…      6001 2023-01-01 05:00:00 2023-01-01 06:00:00   9.1 ug.m…\n 6 SPO-NL00007_06…      6001 2023-01-01 06:00:00 2023-01-01 07:00:00   5.3 ug.m…\n 7 SPO-NL00007_06…      6001 2023-01-01 07:00:00 2023-01-01 08:00:00   2.8 ug.m…\n 8 SPO-NL00007_06…      6001 2023-01-01 08:00:00 2023-01-01 09:00:00   2.3 ug.m…\n 9 SPO-NL00007_06…      6001 2023-01-01 10:00:00 2023-01-01 11:00:00  -0.6 ug.m…\n10 SPO-NL00007_06…      6001 2023-01-01 11:00:00 2023-01-01 12:00:00   1.4 ug.m…\n# ℹ 442,863 more rows\n# ℹ 7 more variables: AggType &lt;chr&gt;, Validity &lt;int&gt;, Verification &lt;int&gt;,\n#   ResultTime &lt;dttm&gt;, DataCapture &lt;dbl&gt;, FkObservationLog &lt;chr&gt;,\n#   geometry &lt;POINT [°]&gt;\n\n\nThen examine the time series\n\ndistinct_stations &lt;- unique(air_sf$Samplingpoint)[1:12]\nplotting_stations &lt;- air_sf[air_sf$Samplingpoint%in%distinct_stations,]\nplotting_stations\n\nSimple feature collection with 98311 features and 12 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 4.781 ymin: 50.888 xmax: 5.9869 ymax: 52.394\nGeodetic CRS:  WGS 84\n# A tibble: 98,311 × 13\n# Groups:   Samplingpoint [12]\n   Samplingpoint   Pollutant Start               End                 Value Unit \n   &lt;chr&gt;               &lt;int&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;\n 1 SPO-NL00007_06…      6001 2023-01-01 01:00:00 2023-01-01 02:00:00 131.  ug.m…\n 2 SPO-NL00007_06…      6001 2023-01-01 02:00:00 2023-01-01 03:00:00  86.2 ug.m…\n 3 SPO-NL00007_06…      6001 2023-01-01 03:00:00 2023-01-01 04:00:00  30.7 ug.m…\n 4 SPO-NL00007_06…      6001 2023-01-01 04:00:00 2023-01-01 05:00:00  11   ug.m…\n 5 SPO-NL00007_06…      6001 2023-01-01 05:00:00 2023-01-01 06:00:00   9.1 ug.m…\n 6 SPO-NL00007_06…      6001 2023-01-01 06:00:00 2023-01-01 07:00:00   5.3 ug.m…\n 7 SPO-NL00007_06…      6001 2023-01-01 07:00:00 2023-01-01 08:00:00   2.8 ug.m…\n 8 SPO-NL00007_06…      6001 2023-01-01 08:00:00 2023-01-01 09:00:00   2.3 ug.m…\n 9 SPO-NL00007_06…      6001 2023-01-01 10:00:00 2023-01-01 11:00:00  -0.6 ug.m…\n10 SPO-NL00007_06…      6001 2023-01-01 11:00:00 2023-01-01 12:00:00   1.4 ug.m…\n# ℹ 98,301 more rows\n# ℹ 7 more variables: AggType &lt;chr&gt;, Validity &lt;int&gt;, Verification &lt;int&gt;,\n#   ResultTime &lt;dttm&gt;, DataCapture &lt;dbl&gt;, FkObservationLog &lt;chr&gt;,\n#   geometry &lt;POINT [°]&gt;\n\nplotting_stations %&gt;% \n  ggplot(aes(x=Start, y=Value)) + \n    geom_line() + \n    facet_wrap(~ Samplingpoint) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(title = \"Time series of monitoring stations\", x = \"Time, hourly\", y = \"Conc (μg/m³)\")\n\n\n\n\n\n\n\n\n\nair_sf &lt;- air_sf %&gt;% filter(Start==as.POSIXct(\"2023-01-01 16:00:00  \"))\ndim(air_sf)\n\n[1] 52 13\n\nair_sf\n\nSimple feature collection with 52 features and 12 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3.8386 ymin: 50.888 xmax: 6.9195 ymax: 53.3304\nGeodetic CRS:  WGS 84\n# A tibble: 52 × 13\n# Groups:   Samplingpoint [52]\n   Samplingpoint   Pollutant Start               End                 Value Unit \n * &lt;chr&gt;               &lt;int&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;\n 1 SPO-NL00007_06…      6001 2023-01-01 16:00:00 2023-01-01 17:00:00  4.4  ug.m…\n 2 SPO-NL00012_06…      6001 2023-01-01 16:00:00 2023-01-01 17:00:00  2.9  ug.m…\n 3 SPO-NL00014_06…      6001 2023-01-01 16:00:00 2023-01-01 17:00:00  3.8  ug.m…\n 4 SPO-NL00016_06…      6001 2023-01-01 16:00:00 2023-01-01 17:00:00  8    ug.m…\n 5 SPO-NL00017_06…      6001 2023-01-01 16:00:00 2023-01-01 17:00:00  3.5  ug.m…\n 6 SPO-NL00131_06…      6001 2023-01-01 16:00:00 2023-01-01 17:00:00  8.21 ug.m…\n 7 SPO-NL00136_06…      6001 2023-01-01 16:00:00 2023-01-01 17:00:00  1.16 ug.m…\n 8 SPO-NL00230_06…      6001 2023-01-01 16:00:00 2023-01-01 17:00:00  2.98 ug.m…\n 9 SPO-NL00240_06…      6001 2023-01-01 16:00:00 2023-01-01 17:00:00  2.17 ug.m…\n10 SPO-NL00241_06…      6001 2023-01-01 16:00:00 2023-01-01 17:00:00  6.33 ug.m…\n# ℹ 42 more rows\n# ℹ 7 more variables: AggType &lt;chr&gt;, Validity &lt;int&gt;, Verification &lt;int&gt;,\n#   ResultTime &lt;dttm&gt;, DataCapture &lt;dbl&gt;, FkObservationLog &lt;chr&gt;,\n#   geometry &lt;POINT [°]&gt;\n\n\nThen let’s load a map of the Netherlands. The border is from https://service.pdok.nl/kadaster/bestuurlijkegrenzen/atom/bestuurlijke_grenzen.xml.\n\nmap &lt;- st_read(file.path(\"D:\", \"data\", \"maps\", \"netherlands_bestuurlijkegrenzen_2021\", \"bestuurlijkegrenzen.gpkg\"))\n\nMultiple layers are present in data source D:\\data\\maps\\netherlands_bestuurlijkegrenzen_2021\\bestuurlijkegrenzen.gpkg, reading layer `gemeenten'.\nUse `st_layers' to list all layer names and their type in a data source.\nSet the `layer' argument in `st_read' to read a particular layer.\n\n\nWarning in CPL_read_ogr(dsn, layer, query, as.character(options), quiet, :\nautomatically selected the first layer in a data source containing more than\none.\n\n\nReading layer `gemeenten' from data source \n  `D:\\data\\maps\\netherlands_bestuurlijkegrenzen_2021\\bestuurlijkegrenzen.gpkg' \n  using driver `GPKG'\nSimple feature collection with 352 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 10425.16 ymin: 306846.2 xmax: 278026.1 ymax: 621876.3\nProjected CRS: Amersfoort / RD New\n\nmap &lt;- st_union(map)\nmap &lt;- st_as_sf(map)\nmap &lt;- st_transform(map, crs = st_crs(air_sf))\n# raster grid covering map\ngrid &lt;- terra::rast(map, nrows = 100, ncols = 100)\ngrid\n\nclass       : SpatRaster \ndimensions  : 100, 100, 1  (nrow, ncol, nlyr)\nresolution  : 0.03919561, 0.02826056  (x, y)\nextent      : 3.307938, 7.227498, 50.75037, 53.57642  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \n\n# coordinates of all cells\nxy &lt;- terra::xyFromCell(grid, 1:ncell(grid))\n# transform points to a sf object\ndp &lt;- st_as_sf(as.data.frame(xy), coords = c(\"x\", \"y\"),\n                 crs = st_crs(map))\n\n# indices points within the map\nindicespointswithin &lt;- which(st_intersects(dp, map,\n                                           sparse = FALSE))\nlength(indicespointswithin)\n\n[1] 4944\n\n# points within the map\ndp &lt;- st_filter(dp, map)\n\n# plot\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp)\n\n\n\n\n\n\n\n\nMore data process\n\nlibrary(geodata)\n# With geodata library\nsave_path &lt;- file.path(\"D:\", \"data\", \"air_quality_data\", \"aux_variables\")\ncovtemp &lt;- worldclim_global(var = \"tavg\", res = 10,\n                            path = save_path)\ncovprec &lt;- worldclim_global(var = \"prec\", res = 10,\n                            path = save_path)\n# Extract at observed locations\nair_sf$covtemp &lt;- extract(mean(covtemp), st_coordinates(air_sf))[, 1]\nair_sf$covprec &lt;- extract(mean(covprec), st_coordinates(air_sf))[, 1]\n# Extract at prediction locations\ndp$covtemp &lt;- extract(mean(covtemp), st_coordinates(dp))[, 1]\ndp$covprec &lt;- extract(mean(covprec), st_coordinates(dp))[, 1]\n\nggplot() + geom_sf(data = map) +\n  geom_sf(data = air_sf, aes(col = Value)) +\n  scale_color_viridis()\n\n\n\n\n\n\n\nggplot() + geom_sf(data = map) +\n  geom_sf(data = air_sf, aes(col = covtemp)) +\n  scale_color_viridis()\n\n\n\n\n\n\n\nggplot() + geom_sf(data = map) +\n  geom_sf(data = air_sf, aes(col = covprec)) +\n  scale_color_viridis()\n\n\n\n\n\n\n\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp, aes(col = covtemp)) +\n  scale_color_viridis()\n\n\n\n\n\n\n\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp, aes(col = covprec)) +\n  scale_color_viridis()\n\n\n\n\n\n\n\n\nLooks like there is a couple of nans in there. Let’s impute using the mean.\n\ndp &lt;- dp %&gt;% mutate(covprec=ifelse(is.na(covprec), mean(covprec, na.rm=TRUE), covprec),\n                    covtemp=ifelse(is.na(covtemp), mean(covtemp, na.rm=TRUE), covtemp))\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp, aes(col = covtemp)) +\n  scale_color_viridis()\n\n\n\n\n\n\n\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp, aes(col = covprec)) +\n  scale_color_viridis()\n\n\n\n\n\n\n\n\nSaw in the forum https://groups.google.com/g/r-inla-discussion-group/c/z_v2oIh2egs it’s good to work with units of kilometers which is intuitive from previous spatial data work.\n\nst_crs(\"EPSG:3857\")$proj4string\n\n[1] \"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"\n\nprojMercator&lt;-\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0\n+x_0=0 +y_0=0 +k=1 +units=km +nadgrids=@null +wktext +no_defs\"\nair_sf_project &lt;- st_transform(air_sf, crs = projMercator)\ndp &lt;- st_transform(dp, crs = projMercator)\n# Observed coordinates\ncoo &lt;- st_coordinates(air_sf_project)\n\n# Predicted coordinates\ncoop &lt;- st_coordinates(dp)\nsummary(dist(coo)) # summary of distances between locations\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  0.5852  70.0344 109.2560 131.9875 179.8815 472.4047 \n\n\nMesh options are from https://punama.github.io/BDI_INLA/#:~:text=stack%20function.,list%20of%20effects%20(effects). max.edge controls the largest triangle edge length, and providing it with a vector c(inner, outer) sets the max edge for inside the boundary and outside the boundary. The purpose of this is to avoid boundary effects in the estimation of the model, where boundary values have high variance. Lindgren and Rue (2015) suggest to extend the domain by some amount. This is briefly discussed in the book by Blangiardo (2015).\ncutoff is the minimum allowed distance between points. Otherwise, points are replaced by a single vertex. For areas of high clusters of points, this could be useful to reduce redundancy. If no boundary is set the mesh is created on the convex hull of the observations.\nThis was inspired from https://punama.github.io/BDI_INLA/. Thanks! ::: {.cell}\nmesh0 &lt;- inla.mesh.2d(loc = coo, max.edge=c(200, 500))\nmesh1 &lt;- inla.mesh.2d(loc = coo, max.edge=c(200, 500), cutoff=1)\nmesh2 &lt;- inla.mesh.2d(loc = coo, max.edge=c(100, 150), cutoff=1)\nmesh3 &lt;- inla.mesh.2d(loc = coo, max.edge=c(100), cutoff=1)\nmesh4 &lt;- inla.mesh.2d(loc = coo, max.edge=c(500, 750))\nmesh5 &lt;- inla.mesh.2d(loc = coo, max.edge=c(500, 750), cutoff=1)\nplot(mesh0)\n\n\n\n\n\n\n\nplot(mesh1)\n\n\n\n\n\n\n\nplot(mesh2)\n\n\n\n\n\n\n\nplot(mesh3)\n\n\n\n\n\n\n\nplot(mesh4)\n\n\n\n\n\n\n\nplot(mesh5)\n\n\n\n\n\n\n\n# Using mesh0\nmesh &lt;-inla.mesh.2d(loc = coo, max.edge=c(200, 500), crs=st_crs(air_sf_project))\nmesh$n\n\n[1] 280\n\n:::\nNext we construct the A matrix that “A that projects the GRF from the observations to the vertices of the triangulated mesh.” (Moraga 2024). This A matrix has a row for each ovservation, and columns equal to the number of vertices in the mesh. That is shown above, with mesh$n. Below, two different meshes are generated. One for the observation locations, and one for the prediction locations. We need to set these both at once, because to make predictions in INLA we have to have that all pre-specified, unlike with the typical modelling style in R.\nAdditionally, the index allows us to extract fitted values from the model.\nThe smoothness parameter is set to 1, where in the spatial case \\(d=2\\) and \\(\\alpha=\\nu + d/2 = 2\\), seen in the code below.\n\nplot(mesh)\npoints(coo, col = \"red\")\naxis(1)\naxis(2)\n\n\n\n\n\n\n\nspde &lt;- inla.spde2.matern(mesh = mesh, alpha = 2, constr = TRUE)\nindexs &lt;- inla.spde.make.index(\"s\", spde$n.spde)\nlengths(indexs)\n\n      s s.group  s.repl \n    280     280     280 \n\n# Make the projection matrices\nA &lt;- inla.spde.make.A(mesh = mesh, loc = coo)\nAp &lt;- inla.spde.make.A(mesh = mesh, loc = coop)\ndim(A)\n\n[1]  52 280\n\ndim(Ap)\n\n[1] 4944  280\n\n\nThen we have to make the INLA stack. This is done because we need to combine the data for estimation with the prediction data, as well as the projection matrices. It contains the response data, the list of covariate data–here the temperature and precipitation–the projection matrices, and the indices.\n\n# stack for estimation stk.e\nstk.e &lt;- inla.stack(tag = \"est\",\ndata = list(y = air_sf_project$Value), A = list(1, A),\neffects = list(data.frame(b0 = rep(1, nrow(A)),\ncovtemp = air_sf_project$covtemp, covprec = air_sf_project$covprec),\ns = indexs))\n#stk.e\n\n# stack for prediction stk.p\nstk.p &lt;- inla.stack(tag = \"pred\",\ndata = list(y = NA), A = list(1, Ap),\neffects = list(data.frame(b0 = rep(1, nrow(Ap)),\ncovtemp = dp$covtemp, covprec = dp$covprec),\ns = indexs))\n#stk.p\n\n# stk.full has stk.e and stk.p\nstk.full &lt;- inla.stack(stk.e, stk.p)\n\nFinally, we can specify the model in INLA. All the hard work has been done above and at least the model specification in INLA is easier. This model has mean specified by \\[\n\\mu_i = \\beta_0 + \\beta_1 \\times \\text{temp}_i + \\beta_2 \\times \\text{prec}_i + S(x_i),\n\\] so there is some contribution from fixed effects as well as a unknown latent process modelled as a zero-mean Gaussian Random Field with Matern covariance function. This puts us well within INLA territory. The model equation can be seen quite clearly in the formula variable below.\n\nformula &lt;- y ~ 0 + b0 + covtemp + covprec + f(s, model = spde)\nres &lt;- inla(formula, family = \"gaussian\",\n       data = inla.stack.data(stk.full),\n       control.predictor = list(compute = TRUE,\n                                A = inla.stack.A(stk.full)),\n       control.compute = list(return.marginals.predictor = TRUE))\n\nNotice that I am passing a few options to the inla call. Importantly, control.compute. We have a nice description of the control options at https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#sec:controlops. It controls what quantities are actually computed and returned during the INLA estimation. For example, there are a few different information criteria that it can return.\ncontrol.predictor will compute the posterior marginals of the parameters.\nThe options: ::: {.cell}\ninla.set.control.compute.default()\n\n$openmp.strategy\n[1] \"default\"\n\n$hyperpar\n[1] TRUE\n\n$return.marginals\n[1] TRUE\n\n$return.marginals.predictor\n[1] FALSE\n\n$dic\n[1] FALSE\n\n$mlik\n[1] TRUE\n\n$cpo\n[1] FALSE\n\n$po\n[1] FALSE\n\n$waic\n[1] FALSE\n\n$residuals\n[1] FALSE\n\n$q\n[1] FALSE\n\n$config\n[1] FALSE\n\n$likelihood.info\n[1] FALSE\n\n$smtp\nNULL\n\n$graph\n[1] FALSE\n\n$internal.opt\nNULL\n\n$save.memory\nNULL\n\n$control.gcpo\n$enable\n[1] FALSE\n\n$num.level.sets\n[1] -1\n\n$size.max\n[1] 32\n\n$strategy\n[1] \"posterior\" \"prior\"    \n\n$groups\nNULL\n\n$selection\nNULL\n\n$group.selection\nNULL\n\n$friends\nNULL\n\n$weights\nNULL\n\n$verbose\n[1] FALSE\n\n$epsilon\n[1] 0.005\n\n$prior.diagonal\n[1] 1e-04\n\n$correct.hyperpar\n[1] TRUE\n\n$keep\nNULL\n\n$remove\nNULL\n\n$remove.fixed\n[1] TRUE\n\nattr(,\"class\")\n[1] \"ctrl_gcpo\"        \"inla_ctrl_object\"\n\nattr(,\"class\")\n[1] \"ctrl_compute\"     \"inla_ctrl_object\"\n\n::: Once the model is fit, we can inspect the fixed parameters and estimated latent field, as well as the hyperparameters for the latent field. ::: {.cell}\nres$summary.fixed\n\n                mean         sd  0.025quant     0.5quant 0.975quant\nb0      -15.29769449 21.3472624 -57.7595559 -15.17245556 26.4583704\ncovtemp   2.17003514  2.0949918  -1.8002534   2.09513247  6.5392273\ncovprec  -0.01550201  0.2034683  -0.4214077  -0.01404086  0.3818577\n                mode          kld\nb0      -15.17140288 4.819848e-08\ncovtemp   2.09359434 3.231952e-07\ncovprec  -0.01411427 5.717319e-08\n\n# Latent field is here\n# res$summary.random$s\n# res$summary.hyperpar\n\nindex &lt;- inla.stack.index(stack = stk.full, tag = \"pred\")$data\npred_mean &lt;- res$summary.fitted.values[index, \"mean\"]\npred_ll &lt;- res$summary.fitted.values[index, \"0.025quant\"]\npred_ul &lt;- res$summary.fitted.values[index, \"0.975quant\"]\ngrid$mean &lt;- NA\ngrid$ll &lt;- NA\ngrid$ul &lt;- NA\n\nlength(pred_mean)\n\n[1] 4944\n\nlength(indicespointswithin)\n\n[1] 4944\n\ngrid$mean[indicespointswithin] &lt;- pred_mean\ngrid$ll[indicespointswithin] &lt;- pred_ll\ngrid$ul[indicespointswithin] &lt;- pred_ul\n  \nsummary(grid) # negative values for the lower limit\n\n      mean             ll               ul        \n Min.   :1.338   Min.   :-4.722   Min.   : 3.733  \n 1st Qu.:3.626   1st Qu.: 0.359   1st Qu.: 6.530  \n Median :4.665   Median : 1.462   Median : 7.813  \n Mean   :4.595   Mean   : 1.368   Mean   : 7.874  \n 3rd Qu.:5.553   3rd Qu.: 2.405   3rd Qu.: 9.087  \n Max.   :8.316   Max.   : 6.753   Max.   :12.860  \n NA's   :5056    NA's   :5056     NA's   :5056    \n\nlibrary(rasterVis)\n\nLoading required package: lattice\n\nlevelplot(grid, layout = c(1, 3),\nnames.attr = c(\"Mean\", \"2.5 percentile\", \"97.5 percentile\"))\n\n\n\n\n\n\n\n:::\nWhen compute=TRUE in control.predictor, we can also obtain the output below. https://www.paulamoraga.com/book-geospatial/sec-inla.html is good for info about this. ::: {.cell}\nres$summary.fitted.values\nres$summary.linear.predictor\n::: as well as marginals.linear.predictor and marginals.fitted.values.\nFrom https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#model-fitting-strategies we can also see the differences using the different fitting strategies, set by the control.inla argument.\nWe can change both the Gaussian approximation strategy for the posterior full conditional distributions, as well as the integration strategy used to integrate out the \\(\\theta_{-k}\\) parameters to get the marginal distribution for \\(\\theta_k\\).\nThe \"grid\" option is the most costly, compared to the central composite design (\"ccd\"). In the empirical bayes option (\"eb\"), the posterior mode is used as the integration point.\n\napprox_strategy &lt;- c(\"gaussian\", \"simplified.laplace\", \"laplace\")\nint_strategy &lt;- c(\"ccd\", \"grid\", \"eb\")\nmodels &lt;- c(\"iid\", \"matern\")\nfits &lt;- matrix(nrow=length(approx_strategy)*length(int_strategy)*length(models), ncol=3)\nfits_marginals &lt;- vector(mode=\"list\", length=length(approx_strategy)*length(int_strategy)*length(models))\nindex_f &lt;- 0\nmodel_names &lt;- c()\nfor (m in models){\n  for(a in approx_strategy){\n    for (i in int_strategy){\n      index_f &lt;- index_f + 1\n      if (m==\"matern\"){\n        formula_approx &lt;- y ~ 0 + b0 + covtemp + covprec + f(s, model = spde)\n      }\n      else{\n        formula_approx &lt;- y ~ 0 + b0 + covtemp + covprec + f(s, model = \"iid\")\n      }\n      print(paste(a, \", \", i, \", \", m))\n      model_names &lt;- c(model_names, paste(a, \", \", i, \", \", m))\n      fit_approx &lt;- inla(formula, family = \"gaussian\",\n         data = inla.stack.data(stk.full),\n         control.inla = list(strategy = a, int.strategy = i),\n         control.compute = list(cpo = TRUE, dic = TRUE, waic = TRUE),\n         control.predictor = list(compute = TRUE, A = inla.stack.A(stk.full)))\n      fits[index_f,] &lt;- fit_approx$summary.fixed$mean\n      fits_marginals[[index_f]] &lt;- fit_approx$marginals.fixed$b0\n  }\n  }\n}\n\n[1] \"gaussian ,  ccd ,  iid\"\n[1] \"gaussian ,  grid ,  iid\"\n[1] \"gaussian ,  eb ,  iid\"\n[1] \"simplified.laplace ,  ccd ,  iid\"\n[1] \"simplified.laplace ,  grid ,  iid\"\n[1] \"simplified.laplace ,  eb ,  iid\"\n[1] \"laplace ,  ccd ,  iid\"\n[1] \"laplace ,  grid ,  iid\"\n[1] \"laplace ,  eb ,  iid\"\n[1] \"gaussian ,  ccd ,  matern\"\n[1] \"gaussian ,  grid ,  matern\"\n[1] \"gaussian ,  eb ,  matern\"\n[1] \"simplified.laplace ,  ccd ,  matern\"\n[1] \"simplified.laplace ,  grid ,  matern\"\n[1] \"simplified.laplace ,  eb ,  matern\"\n[1] \"laplace ,  ccd ,  matern\"\n[1] \"laplace ,  grid ,  matern\"\n[1] \"laplace ,  eb ,  matern\"\n\n\nLet’s compare the means of the parameters ::: {.cell}\nfits_df &lt;- as.data.frame(fits)\nnames(fits_df) &lt;- c(\"b0\", \"covtemp\", \"covprec\")\nfits_df$models &lt;- model_names\nfits_df\n\n          b0  covtemp     covprec                               models\n1  -13.85789 2.049883 -0.01973532               gaussian ,  ccd ,  iid\n2  -15.29769 2.170035 -0.01550201              gaussian ,  grid ,  iid\n3  -13.11329 1.992509 -0.02262044                gaussian ,  eb ,  iid\n4  -13.85789 2.049883 -0.01973532     simplified.laplace ,  ccd ,  iid\n5  -15.29769 2.170035 -0.01550201    simplified.laplace ,  grid ,  iid\n6  -13.11329 1.992509 -0.02262044      simplified.laplace ,  eb ,  iid\n7  -13.85789 2.049883 -0.01973532                laplace ,  ccd ,  iid\n8  -15.29769 2.170035 -0.01550201               laplace ,  grid ,  iid\n9  -13.11329 1.992509 -0.02262044                 laplace ,  eb ,  iid\n10 -13.85789 2.049883 -0.01973532            gaussian ,  ccd ,  matern\n11 -15.29769 2.170035 -0.01550201           gaussian ,  grid ,  matern\n12 -13.11329 1.992509 -0.02262044             gaussian ,  eb ,  matern\n13 -13.85789 2.049883 -0.01973532  simplified.laplace ,  ccd ,  matern\n14 -15.29769 2.170035 -0.01550201 simplified.laplace ,  grid ,  matern\n15 -13.11329 1.992509 -0.02262044   simplified.laplace ,  eb ,  matern\n16 -13.85789 2.049883 -0.01973532             laplace ,  ccd ,  matern\n17 -15.29769 2.170035 -0.01550201            laplace ,  grid ,  matern\n18 -13.11329 1.992509 -0.02262044              laplace ,  eb ,  matern\n\n::: Also compare the posteriors themselves ::: {.cell}\n# Combine dataframes\ndf_list &lt;- lapply(fits_marginals, function(m) {\n  data.frame(x = m[,1], y = m[,2]) \n})\nnew_df &lt;- do.call(rbind, df_list) \n# Add an ID column\nrow_counts &lt;- sapply(fits_marginals, nrow)\nids &lt;- factor(rep(1:length(fits_marginals), each = row_counts))\n\nWarning in rep(1:length(fits_marginals), each = row_counts): first element used\nof 'each' argument\n\nnew_df$id &lt;- ids\nlength(ids)\n\n[1] 774\n\nnew_df &lt;- new_df[1:86,]\nggplot(new_df, aes(x = x, y = y, colour = id)) + \n  geom_line() \n\n\n\n\n\n\n\n::: Wow so with this data set there is not really much different between meshes or with estimation methods.\nAnother useful feature is the set of functions that can be used to manipulate the marginal distributions. Might be useful to compute posterior quantities such as KL divergence: https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#sec:marginals\nAgain from https://www.paulamoraga.com/book-geospatial/sec-inla.html is helpful.\n\ninla.mmarginal(res$marginals.fixed$b0)\n\n[1] -14.90955\n\n# Should be the same?\nres$summary.fixed$mode[1]\n\n[1] -15.1714\n\n\nNext, let’s write that model in INLABRU and check if the estimates are the same. Then, we also look at how to fit the LGCP in INLA/INLABRU.\nWne can use the same mesh and spde function as we used before. And also the same formula! (I think). But what is especially nice about INLABRU is that we don’t have to set up the stack with the projection matrices. We just model the response variable as a function of the covariates in the dataset we set up earlier and the locations of the observations. In INLABRU, we can use the sf dataset object directly.\nINLA ::: {.cell}\nformula &lt;- Value ~ Intercept(1) + covtemp + covprec + f(geometry, model = spde)\n# Fit the model for inlabru\nfit &lt;- bru(formula, data = air_sf_project, family = \"gaussian\")\n# Summarize the results\nsummary(fit)\n\ninlabru version: 2.12.0\nINLA version: 24.05.01-1\nComponents:\nIntercept: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\ncovtemp: main = linear(covtemp), group = exchangeable(1L), replicate = iid(1L), NULL\ncovprec: main = linear(covprec), group = exchangeable(1L), replicate = iid(1L), NULL\nf: main = spde(geometry), group = exchangeable(1L), replicate = iid(1L), NULL\nLikelihoods:\n  Family: 'gaussian'\n    Tag: ''\n    Data class: 'sf', 'grouped_df', 'tbl_df', 'tbl', 'data.frame'\n    Response class: 'numeric'\n    Predictor: Value ~ .\n    Used components: effects[Intercept, covtemp, covprec, f], latent[]\nTime used:\n    Pre = 1.72, Running = 0.754, Post = 0.346, Total = 2.82 \nFixed effects:\n             mean     sd 0.025quant 0.5quant 0.975quant    mode kld\nIntercept -15.298 21.347    -57.760  -15.172     26.458 -15.171   0\ncovtemp     2.170  2.095     -1.800    2.095      6.539   2.094   0\ncovprec    -0.016  0.203     -0.421   -0.014      0.382  -0.014   0\n\nRandom effects:\n  Name    Model\n    f SPDE2 model\n\nModel hyperparameters:\n                                          mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations  0.308 0.080      0.177    0.299\nTheta1 for f                             1.928 0.531      0.908    1.920\nTheta2 for f                            -4.134 0.692     -5.533   -4.122\n                                        0.975quant   mode\nPrecision for the Gaussian observations      0.489  0.283\nTheta1 for f                                 2.999  1.883\nTheta2 for f                                -2.808 -4.070\n\nDeviance Information Criterion (DIC) ...............: 231.65\nDeviance Information Criterion (DIC, saturated) ....: 71.69\nEffective number of parameters .....................: 16.69\n\nWatanabe-Akaike information criterion (WAIC) ...: 232.14\nEffective number of parameters .................: 14.00\n\nMarginal log-Likelihood:  -145.00 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n::: That gives nearly the same estimates as from INLA. ::: {.cell}\nmap_prj &lt;- st_transform(map, crs = projMercator)\npredictions1 &lt;- predict(fit, newdata=dp, formula = ~ Intercept + covtemp + covprec + f)\npredictions2 &lt;- predict(fit, newdata=dp, formula = ~ f)\nggplot() +\ngeom_sf(data=predictions1, aes(color=mean)) +\n  scale_colour_gradient(low = \"blue\", high = \"yellow\")\n\n\n\n\n\n\n\n# Check the contribution of just the spatial field\nggplot() +\ngeom_sf(data=predictions2, aes(color=mean)) +\n  scale_colour_gradient(low = \"blue\", high = \"yellow\")\n\n\n\n\n\n\n\n::: That concludes this section, showing how to fit spatial model in INLA and inlabru. There are a few things more to do: simulating data and seeing how well INLA recovers the parameters, and fitting a temporal model in both packages. But I’ll leave these for later.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial modelling with INLA and SPDE</span>"
    ]
  },
  {
    "objectID": "inla_inlabru_lgcp_tutorial_netherlands_data.html",
    "href": "inla_inlabru_lgcp_tutorial_netherlands_data.html",
    "title": "4  Implementation of LGCP in INLA and INLABRU",
    "section": "",
    "text": "4.1 LGCP in INLA introduction\nHaving completed an examination of Air Pollutoin modelling in INLA and INLABRU, it is time to move on to modelling the point process\nUsing data from GBIF.org (10 January 2025) GBIF Occurrence Download https://doi.org/10.15468/dl.p5cy6n\nFollowing https://www.paulamoraga.com/book-spatial/point-process-modeling.html for the INLA intro.\nlibrary(INLA)\n\nLoading required package: Matrix\n\n\nLoading required package: sp\n\n\nThis is INLA_24.05.01-1 built 2024-05-01 18:49:50 UTC.\n - See www.r-inla.org/contact-us for how to get help.\n - List available models/likelihoods/etc with inla.list.models()\n - Use inla.doc(&lt;NAME&gt;) to access documentation\n\nlibrary(inlabru)\n\nLoading required package: fmesher\n\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(ggplot2)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nlibrary(terra)\n\nterra 1.7.78\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:terra':\n\n    intersect, union\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\nTODO: Fix issue reading data. StillIMage is doing something\nfile_name &lt;- file.path(\"D:\", \"data\", \"gbif_observation_org_butterflies\", \"gbif_butterfly_observation-org\", \"gbif_subset_netherlands_lepidoptera.csv\")\nd &lt;- read_delim(file_name, delim='\\t', col_types=cols(infraspecificEpithet=col_character()))\nd %&gt;% filter(species==\"Lasiommata megera\") -&gt; d\ndim(d)\n\n[1] 2705   50\n\nsum(is.na(d))\n\n[1] 45001\n\nd &lt;- st_as_sf(d, coords = c(\"decimalLongitude\", \"decimalLatitude\"))\nst_crs(d) &lt;- \"EPSG:4326\"\nst_crs(\"EPSG:3857\")$proj4string\n\n[1] \"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"\n\nprojMercator &lt;- st_crs(\"EPSG:3857\")$proj4string\nprojMercator\n\n[1] \"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"\n\n# Observed coordinates\nd &lt;- st_transform(d, crs = projMercator)\nd\n\nSimple feature collection with 2705 features and 48 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 411882.1 ymin: 6577190 xmax: 740274.6 ymax: 7066673\nProjected CRS: WGS 84 / Pseudo-Mercator\n# A tibble: 2,705 × 49\n       gbifID datasetKey    occurrenceID kingdom phylum class order family genus\n *      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;\n 1 4160322107 8a863029-f43… https://obs… Animal… Arthr… Inse… Lepi… Nymph… Para…\n 2 4409008265 8a863029-f43… https://obs… Animal… Arthr… Inse… Lepi… Nymph… Para…\n 3 4462065068 8a863029-f43… https://obs… Animal… Arthr… Inse… Lepi… Nymph… Para…\n 4 4160750394 8a863029-f43… https://obs… Animal… Arthr… Inse… Lepi… Nymph… Para…\n 5 4159129901 8a863029-f43… https://obs… Animal… Arthr… Inse… Lepi… Nymph… Para…\n 6 4410120094 8a863029-f43… https://obs… Animal… Arthr… Inse… Lepi… Nymph… Para…\n 7 4410216900 8a863029-f43… https://obs… Animal… Arthr… Inse… Lepi… Nymph… Para…\n 8 4409871777 8a863029-f43… https://obs… Animal… Arthr… Inse… Lepi… Nymph… Para…\n 9 4409311703 8a863029-f43… https://obs… Animal… Arthr… Inse… Lepi… Nymph… Para…\n10 4158114004 8a863029-f43… https://obs… Animal… Arthr… Inse… Lepi… Nymph… Para…\n# ℹ 2,695 more rows\n# ℹ 40 more variables: species &lt;chr&gt;, infraspecificEpithet &lt;chr&gt;,\n#   taxonRank &lt;chr&gt;, scientificName &lt;chr&gt;, verbatimScientificName &lt;chr&gt;,\n#   verbatimScientificNameAuthorship &lt;lgl&gt;, countryCode &lt;chr&gt;, locality &lt;chr&gt;,\n#   stateProvince &lt;chr&gt;, occurrenceStatus &lt;chr&gt;, individualCount &lt;dbl&gt;,\n#   publishingOrgKey &lt;chr&gt;, coordinateUncertaintyInMeters &lt;dbl&gt;,\n#   coordinatePrecision &lt;lgl&gt;, elevation &lt;lgl&gt;, elevationAccuracy &lt;lgl&gt;, …\n?st_layers\n\nstarting httpd help server ... done\n\nlayers &lt;- st_layers(file.path(\"D:\", \"data\", \"maps\", \"netherlands_bestuurlijkegrenzen_2021\", \"bestuurlijkegrenzen.gpkg\"))\n#print(str(layers))\nmap &lt;- st_read(file.path(\"D:\", \"data\", \"maps\", \"netherlands_bestuurlijkegrenzen_2021\", \"bestuurlijkegrenzen.gpkg\"), layer = \"landsgrens\")\n\nReading layer `landsgrens' from data source \n  `D:\\data\\maps\\netherlands_bestuurlijkegrenzen_2021\\bestuurlijkegrenzen.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 10425.16 ymin: 306846.2 xmax: 278026.1 ymax: 621876.3\nProjected CRS: Amersfoort / RD New\n\nmap &lt;- st_union(map)\nmap &lt;- st_as_sf(map)\nplot(map)\n\n\n\n\n\n\n\n# Damn it, there's a little isolated spec in the map!\nborder_polygon &lt;- st_cast(map, \"POLYGON\")\nborder_polygon &lt;- st_as_sfc(border_polygon)\ngeos &lt;- lapply(border_polygon, function(x) x[1])\nfor (g in geos){\n  plot(st_polygon(g))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Get the border polygon\nborder_final &lt;- st_polygon(geos[[1]])\n# We still need sf object\nborder_final &lt;- st_sfc(border_final, crs=st_crs(map))\nborder_final &lt;- st_as_sf(border_final)\nplot(border_final)\n\n\n\n\n\n\n\nmap &lt;- border_final\nmap &lt;- st_transform(map, crs = projMercator)\ncoo &lt;- st_coordinates(d)\nggplot() + geom_sf(data = map) +\n  geom_sf(data = d) + coord_sf(datum = projMercator)\n# raster grid covering map\ngrid &lt;- terra::rast(map, nrows = 50, ncols = 50)\n# coordinates of all cells\nxy &lt;- terra::xyFromCell(grid, 1:ncell(grid))\n# transform points to a sf object\ndp &lt;- st_as_sf(as.data.frame(xy), coords = c(\"x\", \"y\"),\n               crs = st_crs(map))\n\n# indices points within the map\nindicespointswithin &lt;- which(st_intersects(dp, map,\n                                           sparse = FALSE))\n\n# points within the map\ndp &lt;- st_filter(dp, map)\n\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp) + coord_sf(datum = projMercator)\n\n\n\n\n\n\n\ncoop &lt;- st_coordinates(dp)\nHmm, why do I need such a large unit? Something with mercator? Oh, am I in meters?\nmap\n\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 368237.9 ymin: 6577255 xmax: 804561.4 ymax: 7090341\nProjected CRS: WGS 84 / Pseudo-Mercator\n                               x\n1 POLYGON ((669987.2 6579597,...\n\nloc.d &lt;- cbind(st_coordinates(map)[, 1], st_coordinates(map)[, 2])\n#mesh &lt;- inla.mesh.2d(loc=coo, max.edge = c(50000, 100000))\n#mesh &lt;- inla.mesh.2d(loc.domain=loc.d)\nmesh &lt;- inla.mesh.2d(loc.domain = loc.d, max.edge = c(50000, 100000), crs=crs(d))\nmesh\n\nfm_mesh_2d object:\n  CRS:\n    LegacyPROJ4:    +proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\n    WKT: (only shown with verbose = TRUE)\n  Manifold: R2\n  V / E / T:    326 / 930 / 605\n  Euler char.:  1\n  Constraints:  45 boundary edges (1 group: 0), 56 interior edges (1 group: 0)\n  Bounding box: (243337.2,929462.1) x (6452354,7215241)\n  Basis d.o.f.: 326\n\nplot(mesh)\npoints(coo, col = \"red\")\naxis(1)\naxis(2)\n\n\n\n\n\n\n\n(nv &lt;- mesh$n)\n\n[1] 326\n\n(n &lt;- nrow(coo))\n\n[1] 2705\nspde &lt;- inla.spde2.matern(mesh = mesh, alpha = 2, constr = TRUE)\nbook.mesh.dual &lt;- function(mesh) {\n    if (mesh$manifold=='R2') {\n        ce &lt;- t(sapply(1:nrow(mesh$graph$tv), function(i)\n            colMeans(mesh$loc[mesh$graph$tv[i, ], 1:2])))\n        library(parallel)\n        pls &lt;- mclapply(1:mesh$n, function(i) {\n            p &lt;- unique(Reduce('rbind', lapply(1:3, function(k) {\n            j &lt;- which(mesh$graph$tv[,k]==i)\n            if (length(j)&gt;0) \n            return(rbind(ce[j, , drop=FALSE],\n            cbind(mesh$loc[mesh$graph$tv[j, k], 1] +\n            mesh$loc[mesh$graph$tv[j, c(2:3,1)[k]], 1], \n            mesh$loc[mesh$graph$tv[j, k], 2] +\n            mesh$loc[mesh$graph$tv[j, c(2:3,1)[k]], 2])/2))\n            else return(ce[j, , drop=FALSE])\n            })))\n            j1 &lt;- which(mesh$segm$bnd$idx[,1]==i)\n            j2 &lt;- which(mesh$segm$bnd$idx[,2]==i)\n            if ((length(j1)&gt;0) | (length(j2)&gt;0)) {\n            p &lt;- unique(rbind(mesh$loc[i, 1:2], p,\n            mesh$loc[mesh$segm$bnd$idx[j1, 1], 1:2]/2 +\n            mesh$loc[mesh$segm$bnd$idx[j1, 2], 1:2]/2, \n            mesh$loc[mesh$segm$bnd$idx[j2, 1], 1:2]/2 +\n            mesh$loc[mesh$segm$bnd$idx[j2, 2], 1:2]/2))\n            yy &lt;- p[,2]-mean(p[,2])/2-mesh$loc[i, 2]/2\n            xx &lt;- p[,1]-mean(p[,1])/2-mesh$loc[i, 1]/2\n            }\n            else {\n            yy &lt;- p[,2]-mesh$loc[i, 2]\n            xx &lt;- p[,1]-mesh$loc[i, 1]\n            }\n            Polygon(p[order(atan2(yy,xx)), ])\n        })\n        return(SpatialPolygons(lapply(1:mesh$n, function(i)\n            Polygons(list(pls[[i]]), i))))\n    }\n    else stop(\"It only works for R2!\")\n}\ndmesh &lt;- book.mesh.dual(mesh)\nplot(dmesh)\naxis(1)\naxis(2)\nWe then do something a little tricky. The mesh is larger than the domain that the points were observed in or the study region. So the intersections between the polygons in the mesh and the locations in \\(D\\) are computed\nplot(loc.d)\n\n\n\n\n\n\n\ndomain.polys &lt;- Polygons(list(Polygon(loc.d)), '0')\ndomainSP &lt;- SpatialPolygons(list(domain.polys))\nplot(domainSP)\n\n\n\n\n\n\n\ndomain_sf &lt;- st_as_sf(domainSP)\ncrs(domain_sf)\n\n[1] \"\"\n\ndomain_sf &lt;- st_set_crs(domain_sf, projMercator)\nst_crs(domain_sf)\n\nCoordinate Reference System:\n  User input: +proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\nplot(domain_sf)\n\n\n\n\n\n\n\nmesh_sf &lt;- st_as_sf(dmesh)\nmesh_sf &lt;- st_set_crs(mesh_sf, projMercator)\n# Check if the mesh polygons overlap with any of the locations \nw &lt;- sapply(1:length(dmesh), function(i) {\n  if(length(st_intersects(mesh_sf[i,], domain_sf)[[1]])&gt;0){\n    return(sf::st_area(sf::st_intersection(mesh_sf[i, ], domain_sf)))\n  }\n  else return(0)\n})\nsum(w)\n\n[1] 111060620373\n\nst_area(map)\n\n111060620373 [m^2]\nplot(mesh)\nplot(domain_sf, add=T, col=\"green\")\npoints(mesh$loc[which(w &gt; 0), 1:2], col = \"black\", pch = 20)\npoints(mesh$loc[which(w == 0), 1:2], col = \"red\", pch = 20)\nTODO: January 13th. Okay, now i’m stuck again. Finish later\ny.pp &lt;- rep(0:1, c(nv, n))\ne.pp &lt;- c(w, rep(0, n))\n# Projection matrix for the integration points (mesh vertices)\nA.int &lt;- Diagonal(nv, rep(1, nv))\n# Projection matrix for observed points (event locations)\nA.y &lt;- inla.spde.make.A(mesh = mesh, loc = coo)\n# Projection matrix for mesh vertices and event locations\nA.pp &lt;- rbind(A.int, A.y)\n\n# We also create the projection matrix Ap.pp for the prediction locations.\nAp.pp &lt;- inla.spde.make.A(mesh = mesh, loc = coop)\n# stack for estimation\nstk.e.pp &lt;- inla.stack(tag = \"est.pp\",\ndata = list(y = y.pp, e = e.pp), \nA = list(1, A.pp),\neffects = list(list(b0 = rep(1, nv + n)), list(s = 1:nv)))\n\n# stack for prediction stk.p\nstk.p.pp &lt;- inla.stack(tag = \"pred.pp\",\ndata = list(y = rep(NA, nrow(coop)), e = rep(0, nrow(coop))),\nA = list(1, Ap.pp),\neffects = list(data.frame(b0 = rep(1, nrow(coop))),\n               list(s = 1:nv)))\n\n# stk.full has stk.e and stk.p\nstk.full.pp &lt;- inla.stack(stk.e.pp, stk.p.pp)\nformula &lt;- y ~ 0 + b0 + f(s, model = spde)\nres &lt;- inla(formula,  family = 'poisson',\n  data = inla.stack.data(stk.full.pp),\n  control.inla=list(int.strategy = 'grid', strategy=\"laplace\"),\n  control.predictor = list(compute = TRUE, link = 1,\n    A = inla.stack.A(stk.full.pp)),\n    E = inla.stack.data(stk.full.pp)$e)\nres$summary.fixed\n\n       mean       sd 0.025quant 0.5quant 0.975quant    mode          kld\nb0 1.039501 1.065052  -1.049386 1.039649   3.127548 1.03965 5.801275e-11\n\nres$summary.hyperpar\n\n                  mean          sd 0.025quant   0.5quant 0.975quant       mode\nTheta1 for s   8.14040 0.001247988   8.137666   8.140495   8.142519   8.140964\nTheta2 for s -11.45034 0.009720901 -11.466856 -11.451076 -11.429055 -11.454715\n\nindex &lt;- inla.stack.index(stk.full.pp, tag = \"pred.pp\")$data\npred_mean &lt;- res$summary.fitted.values[index, \"mean\"]\npred_ll &lt;- res$summary.fitted.values[index, \"0.025quant\"]\npred_ul &lt;- res$summary.fitted.values[index, \"0.975quant\"]\ngrid$mean &lt;- NA\ngrid$ll &lt;- NA\ngrid$ul &lt;- NA\n\ngrid$mean[indicespointswithin] &lt;- pred_mean\ngrid$ll[indicespointswithin] &lt;- pred_ll\ngrid$ul[indicespointswithin] &lt;- pred_ul\nThen plot the predicted intensity\nplot(grid)\ngeom_sf(data=grid, aes(color=mean)) +\n  scale_colour_gradient(low = \"blue\", high = \"yellow\")\nWe have to make sure to get the domain for the sampler correct. The INLA code does it above, but inlabru by default does not.\nMore about that https://inlabru-org.github.io/inlabru/articles/2d_lgcp_plotsampling.html and actually the exact problem here: https://groups.google.com/g/r-inla-discussion-group/c/0bBC9bVV-L4 even though the problem was with preferential sampling. In the mesh-process R section above, you can see the manipulation to get the domains correct for INLA. Even with including sampler=domain_sf here, the estimates are not exactly the same as INLA, but closer than it was before.\nTODO: What is this domain stuff for exactly? The integration? Should be equation 3 of Simpson (2016).\n# TODO: Make sure I get the same result as inla. Options and mesh are off\n# Oh nice we can name the intercept but then need to subtract 1 to get rid of the default intercept\nformula_inlabru &lt;- geometry ~ b0(1) - 1 + f(geometry, model = spde)\nfit1 &lt;- lgcp(formula_inlabru, data=d, sampler=domain_sf, domain = list(geometry = mesh), \n             options = list(control.inla=list(int.strategy = 'ccd', strategy=\"laplace\")))\n#                          control.compute=list(config=TRUE),\n#                          control.results=list(return.marginals.random = TRUE,\n#                                               return.marginals.predictor = TRUE),\n#                          control.predictor = list(compute = TRUE)))\n\nsummary(fit1)\n\ninlabru version: 2.12.0\nINLA version: 24.05.01-1\nComponents:\nb0: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\nf: main = spde(geometry), group = exchangeable(1L), replicate = iid(1L), NULL\nLikelihoods:\n  Family: 'cp'\n    Tag: ''\n    Data class: 'sf', 'data.frame'\n    Response class: 'numeric'\n    Predictor: geometry ~ .\n    Used components: effects[b0, f], latent[]\nTime used:\n    Pre = 0.691, Running = 1.47, Post = 0.175, Total = 2.33 \nFixed effects:\n      mean  sd 0.025quant 0.5quant 0.975quant   mode kld\nb0 -21.688 1.4    -24.492  -21.662    -19.041 -21.66   0\n\nRandom effects:\n  Name    Model\n    f SPDE2 model\n\nModel hyperparameters:\n               mean    sd 0.025quant 0.5quant 0.975quant   mode\nTheta1 for f   8.85 0.106       8.66     8.85       9.07   8.83\nTheta2 for f -12.74 0.873     -14.74   -12.63     -11.41 -12.07\n\nDeviance Information Criterion (DIC) ...............: -126820.99\nDeviance Information Criterion (DIC, saturated) ....: NA\nEffective number of parameters .....................: -127802.62\n\nWatanabe-Akaike information criterion (WAIC) ...: 7199.81\nEffective number of parameters .................: 2414.72\n\nMarginal log-Likelihood:  -64580.42 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\nTODO: Oh wow, why is b0 so different between inla and inlabru.\npredictions1 &lt;- predict(fit1, newdata=dp, formula = ~ b0 + f)\npredictions2 &lt;- predict(fit1, newdata=dp, formula = ~ f)\nggplot() +\ngeom_sf(data=predictions1, aes(color=mean)) +\n  scale_colour_gradient(low = \"blue\", high = \"yellow\")\n\n\n\n\n\n\n\n# Check the contribution of just the spatial field\nggplot() +\ngeom_sf(data=predictions2, aes(color=mean)) +\n  scale_colour_gradient(low = \"blue\", high = \"yellow\")\nThen, we can move onto multiple likelihoods and inlabru",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Implementation of LGCP in INLA and INLABRU</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Blangiardo, Marta, and Michela Cameletti. 2015. John Wiley & Sons,\nLtd. https://doi.org/https://doi.org/10.1002/9781118950203.\n\n\nDiggle, Peter J., Paula Moraga, Barry Rowlingson, and Benjamin M.\nTaylor. 2013. “Spatial and\nSpatio-Temporal\nLog-Gaussian Cox\nProcesses: Extending the\nGeostatistical Paradigm.”\nStatistical Science 28 (4): 542–63. https://doi.org/10.1214/13-STS441.\n\n\nRue, Håvard, Sara Martino, and Nicolas Chopin. 2009. “Approximate\nBayesian Inference for Latent Gaussian Models\nby Using Integrated Nested Laplace Approximations.”\nJournal of the Royal Statistical Society: Series B (Statistical\nMethodology) 71 (2): 319–92. https://doi.org/10.1111/j.1467-9868.2008.00700.x.",
    "crumbs": [
      "References"
    ]
  }
]