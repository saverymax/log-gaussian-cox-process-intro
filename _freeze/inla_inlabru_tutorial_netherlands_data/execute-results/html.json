{
  "hash": "c8b93dd66ccb46ee8c014dac9e233df7",
  "result": {
    "markdown": "\n# Spatial modelling with INLA and SPDE\n\n## INLA\n\nHaving covered the need-to-know material for INLA and SPDE, next I'm going to build a spatial model using both `INLA` and `inlabru`. Comparing the two packages is useful because, though they can be expected to give the same results in most cases, the implementation is a bit different. Knowing when using the more complicated `INLA` can be justified is a useful exercise, I think. The template for this code using `INLA` follows the air pollution example from the geostatistical chapter in Paula Moraga's Spatial Statistics book, <https://www.paulamoraga.com/book-spatial/sec-geostatisticaldataSPDE.html>. The difference is that I will use air pollution data from the Netherlands, compare `INLA` and `inlabru`, and consider a few extra technical details. For the `inlabru` code I follow the examples the authors provide at their site for the package: <https://inlabru-org.github.io/inlabru/articles/>.\n\nThe model fit here is a simple geostatistical one. I'll be using air pollution data in the Netherlands. The model can be written as\n$$\nY_i \\sim N(u_i, \\sigma^2),\n$$ \nwith mean $\\mu_i$ parameterized as \n$$\nu_i = \\beta_0 + \\beta_1\\cdot\\text{temperature}_i + \\beta_2\\cdot\\text{precipitation}_i + S(x_i).\n$$\nSo it's a typical Gaussian distributed variable with underlying latent structure $S(x_i)$ that is spatially indexed. It represents measurements taken at discrete locations but used to describe or estimate a spatially continuous process, such as air pollution.\n\nLet's get straight to data downloading and processing. I downloaded air pollution data in the Belgium and Netherlands for the year of 2023 from <https://eeadmz1-downloads-webapp.azurewebsites.net/>. For now I just focus on the Netherlands data.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(INLA)\nlibrary(inlabru)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(terra)\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\n```\n:::\n\n In the dataset, 8 is the id for\tNitrogen dioxide, 5\tis for Particulate matter < 10 µm, and 6001 for Particulate matter < 2.5 µm. I focused on Particulate matter < 2.5 µm, referred to as PM2.5.\n \n\n::: {.cell}\n\n```{.r .cell-code}\n#data_path <- file.path(\"D:\", \"data\", \"air_quality_data\", \"belgium_eea\", \"E1a\")\ndata_path <- file.path(\"D:\", \"data\", \"air_quality_data\", \"netherlands_eea\", \"E1a\")\nd <- open_dataset(data_path)\nd %>% group_by(Samplingpoint) |> filter(Pollutant==6001, Value>-1) |> collect() -> df\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 442,873 × 12\n# Groups:   Samplingpoint [53]\n   Samplingpoint   Pollutant Start               End                 Value Unit \n   <chr>               <int> <dttm>              <dttm>              <dbl> <chr>\n 1 NL/SPO-NL00007…      6001 2023-01-01 01:00:00 2023-01-01 02:00:00 131.  ug.m…\n 2 NL/SPO-NL00007…      6001 2023-01-01 02:00:00 2023-01-01 03:00:00  86.2 ug.m…\n 3 NL/SPO-NL00007…      6001 2023-01-01 03:00:00 2023-01-01 04:00:00  30.7 ug.m…\n 4 NL/SPO-NL00007…      6001 2023-01-01 04:00:00 2023-01-01 05:00:00  11   ug.m…\n 5 NL/SPO-NL00007…      6001 2023-01-01 05:00:00 2023-01-01 06:00:00   9.1 ug.m…\n 6 NL/SPO-NL00007…      6001 2023-01-01 06:00:00 2023-01-01 07:00:00   5.3 ug.m…\n 7 NL/SPO-NL00007…      6001 2023-01-01 07:00:00 2023-01-01 08:00:00   2.8 ug.m…\n 8 NL/SPO-NL00007…      6001 2023-01-01 08:00:00 2023-01-01 09:00:00   2.3 ug.m…\n 9 NL/SPO-NL00007…      6001 2023-01-01 10:00:00 2023-01-01 11:00:00  -0.6 ug.m…\n10 NL/SPO-NL00007…      6001 2023-01-01 11:00:00 2023-01-01 12:00:00   1.4 ug.m…\n# ℹ 442,863 more rows\n# ℹ 6 more variables: AggType <chr>, Validity <int>, Verification <int>,\n#   ResultTime <dttm>, DataCapture <dbl>, FkObservationLog <chr>\n```\n:::\n:::\n\nNext add the the locations of the measuring stations to the dataset. The air pollution observations include an station identifier, but the coordinates of each of these stations is not included. It comes in a separate dataset. \n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- df %>%\n   #mutate(Samplingpoint = str_remove(Samplingpoint, \"BE/\"))\n   mutate(Samplingpoint = str_remove(Samplingpoint, \"NL/\"))\n#station_location_path <- file.path(\"D:\", \"data\", \"air_quality_data\", \"eea_stations_2023\", \"belgium_stations_2023.csv\")\nstation_location_path <- file.path(\"D:\", \"data\", \"air_quality_data\", \"eea_stations_2023\", \"nl_stations_2023.csv\")\nstation_locations <- read_csv(station_location_path)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 4685 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (19): Country, Air Quality Network, Air Quality Network Name, Air Qualit...\ndbl  (8): Year, Air Pollution Level, Data Coverage, Verification, Longitude,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nstation_locations <- rename(station_locations, Samplingpoint = 'Sampling Point Id')\nstation_locations %>% \n  distinct(Samplingpoint, Longitude, Latitude) -> unique_locations\nmerged_df <- df %>% \n  left_join(unique_locations %>% select(Samplingpoint, Longitude, Latitude), by = \"Samplingpoint\")\nstation_locations_sf <- st_as_sf(station_locations, coords = c(\"Longitude\", \"Latitude\"))\nair_sf <- st_as_sf(merged_df, coords = c(\"Longitude\", \"Latitude\"))\nst_crs(air_sf) <- \"EPSG:4326\"\n```\n:::\n\nThen examine the time series\n\n::: {.cell}\n\n```{.r .cell-code}\ndistinct_stations <- unique(air_sf$Samplingpoint)[1:12]\nplotting_stations <- air_sf[air_sf$Samplingpoint%in%distinct_stations,]\nplotting_stations %>% \n  ggplot(aes(x=Start, y=Value)) + \n    geom_line() + \n    facet_wrap(~ Samplingpoint) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(title = \"Time series of monitoring stations\", x = \"Time, hourly\", y = \"Conc (μg/m³)\")\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/plot-time-series-1.png){width=672}\n:::\n:::\n\nThe above figure shows 12 of the stations observations over 2023. The data is currently at the hourly resolution.\n\n::: {.cell}\n\n:::\n\nNext, load a map of the Netherlands. The border is from <https://service.pdok.nl/kadaster/bestuurlijkegrenzen/atom/bestuurlijke_grenzen.xml>. A number of processing steps are done here, namely to generate some grid points within the map border. These points will be used as locations to make spatial predictions later on.\n\n::: {.cell}\n\n```{.r .cell-code}\nmap <- st_read(file.path(\"D:\", \"data\", \"maps\", \"netherlands_bestuurlijkegrenzen_2021\", \"bestuurlijkegrenzen.gpkg\"))\nmap <- st_union(map)\nmap <- st_as_sf(map)\nmap <- st_transform(map, crs = st_crs(air_sf))\n# raster grid covering map\ngrid <- terra::rast(map, nrows = 100, ncols = 100)\n# coordinates of all cells\nxy <- terra::xyFromCell(grid, 1:ncell(grid))\n# transform points to a sf object\ndp <- st_as_sf(as.data.frame(xy), coords = c(\"x\", \"y\"),\n                 crs = st_crs(map))\n\n# indices points within the map\nindicespointswithin <- which(st_intersects(dp, map,\n                                           sparse = FALSE))\n# points within the map\ndp <- st_filter(dp, map)\n```\n:::\n\nLet's check the map and grid of prediction points.\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp)\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/plot-map-1.png){width=672}\n:::\n:::\n\nWe can see the grid of spatial points is at quite a high resolution.\n\nThe next step is to download temperature and precipitation data using the `geodata` package. \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(geodata)\n# With geodata library\nsave_path <- file.path(\"D:\", \"data\", \"air_quality_data\", \"aux_variables\")\ncovtemp <- worldclim_global(var = \"tavg\", res = 10,\n                            path = save_path)\ncovprec <- worldclim_global(var = \"prec\", res = 10,\n                            path = save_path)\n# Extract at observed locations\nair_sf$covtemp <- extract(mean(covtemp), st_coordinates(air_sf))[, 1]\nair_sf$covprec <- extract(mean(covprec), st_coordinates(air_sf))[, 1]\n# Extract at prediction locations\ndp$covtemp <- extract(mean(covtemp), st_coordinates(dp))[, 1]\ndp$covprec <- extract(mean(covprec), st_coordinates(dp))[, 1]\n```\n:::\n\nWe can then take a look at the air pollution and the covariates at the measuring stations locations.\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + geom_sf(data = map) +\n  geom_sf(data = air_sf, aes(col = Value)) +\n  ggtitle(\"Air pollution at measuring stations\") +\n  scale_color_viridis()\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/view-covars-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot() + geom_sf(data = map) +\n  geom_sf(data = air_sf, aes(col = covtemp)) +\n  ggtitle(\"Temperature at measuring stations\") +\n  scale_color_viridis()\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/view-covars-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot() + geom_sf(data = map) +\n  geom_sf(data = air_sf, aes(col = covprec)) +\n  ggtitle(\"Precipitation at measuring stations\") +\n  scale_color_viridis()\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/view-covars-3.png){width=672}\n:::\n:::\n\nThe above code also extracted the covariate data at each prediction point. We can see what that looks like as well:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp, aes(col = covtemp)) +\n  ggtitle(\"Temperature at prediction points\") +\n  scale_color_viridis()\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/view-pred-data-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp, aes(col = covprec)) +\n  ggtitle(\"Precipitation at prediction points\") +\n  scale_color_viridis()\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/view-pred-data-2.png){width=672}\n:::\n:::\n\nNext, mean impute out the NANs in the data. A better imputation method would certainly be better, wouldn't it?\n\n::: {.cell}\n\n```{.r .cell-code}\ndp <- dp %>% mutate(covprec=ifelse(is.na(covprec), mean(covprec, na.rm=TRUE), covprec),\n                    covtemp=ifelse(is.na(covtemp), mean(covtemp, na.rm=TRUE), covtemp))\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp, aes(col = covtemp)) +\n  scale_color_viridis()\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/imputation-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot() + geom_sf(data = map) +\n  geom_sf(data = dp, aes(col = covprec)) +\n  scale_color_viridis()\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/imputation-2.png){width=672}\n:::\n:::\n\nI saw in the INLA forum, <https://groups.google.com/g/r-inla-discussion-group/c/z_v2oIh2egs>, that it can be helpful to work with units of kilometers. In my experience, this has also held true, but it also takes some finagling and mindfulness depending on the data you are working with.\n\n::: {.cell}\n\n```{.r .cell-code}\nprojMercator<-\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0\n+x_0=0 +y_0=0 +k=1 +units=km +nadgrids=@null +wktext +no_defs\"\nair_sf_project <- st_transform(air_sf, crs = projMercator)\ndp <- st_transform(dp, crs = projMercator)\n# Observed coordinates\ncoo <- st_coordinates(air_sf_project)\n\n# Predicted coordinates\ncoop <- st_coordinates(dp)\n#summary(dist(coo)) # summary of distances between locations\n```\n:::\n\n\nThat gives us enough data and more importantly, motivation, to make a model. Now I can get into the `INLA` implementation. First: the mesh. I mentioned the mesh in the theory discussion. The mesh enables us to compute the discretized approximation to the GF using a GRMF. \n\n`INLA` provides the function to create the mesh. Here I'm using `inla.mesh.2d`. Let's take a look at a few meshes created with different parameter options. \n\n::: {.cell}\n\n```{.r .cell-code}\nmesh0 <- inla.mesh.2d(loc = coo, max.edge=c(200, 500))\nmesh1 <- inla.mesh.2d(loc = coo, max.edge=c(200, 500), cutoff=1)\nmesh2 <- inla.mesh.2d(loc = coo, max.edge=c(200, 500), cutoff=0.3)\nmesh3 <- inla.mesh.2d(loc = coo, max.edge=c(100, 150), cutoff=1)\nmesh4 <- inla.mesh.2d(loc = coo, max.edge=c(100, 150), cutoff=0.3)\nmesh5 <- inla.mesh.2d(loc = coo, max.edge=c(100), cutoff=1)\nmesh6 <- inla.mesh.2d(loc = coo, max.edge=c(100), cutoff=0.3)\nplot(mesh0)\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/inla-mesh-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(mesh1)\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/inla-mesh-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(mesh2)\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/inla-mesh-3.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(mesh3)\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/inla-mesh-4.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(mesh4)\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/inla-mesh-5.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(mesh5)\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/inla-mesh-6.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(mesh6)\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/inla-mesh-7.png){width=672}\n:::\n\n```{.r .cell-code}\n# Using mesh0\nmesh <-inla.mesh.2d(loc = coo, max.edge=c(200, 500), crs=st_crs(air_sf_project))\n```\n:::\n\nThere are a number of options to play with: `max.edge` controls the largest triangle edge length, and providing it with a vector `c(inner, outer)` sets the max edge for inside the boundary and outside the boundary. The purpose of this is to avoid boundary effects in the estimation of the model, where boundary values have high variance. @lindgren2015 suggests to extend the domain by some amount. \n\n`cutoff` is the minimum allowed distance between points. Otherwise, points are replaced by a single vertex. For areas of high clusters of points, this could be useful to reduce redundancy. If no `boundary` is set the mesh is created on the convex hull of the observations.\n\n<https://punama.github.io/BDI_INLA/> is also a good source for this. Thanks!\n\nNext we construct the A matrix that \"A that projects the GRF from the observations to the vertices of the triangulated mesh\" (@moraga2023). The A matrix has a row for each observation, and columns equal to the number of vertices in the mesh. The number of vertices in our mesh can be checked with `mesh$n`. Below, two different meshes are generated. One for the observation locations, and one for the prediction locations. We need to set these both at once, because to make predictions in INLA we have to have that all pre-specified, unlike with the typical modelling style in R. That was why I created the grid of prediction points initially.\n\nThe `inla.spde2.matern` function is used to build the SPDE model with matern covariance, using the mesh. The smoothness parameter $\\nu$ for the SPDE is implicitly set to 1, where in the spatial case $d=2$ and $\\alpha=\\nu + d/2 = 2$, seen in the code below. In the model itself the Matern covariance function parameters will be estimated; more on that later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mesh)\npoints(coo, col = \"red\")\naxis(1)\naxis(2)\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/inla-A-1.png){width=672}\n:::\n\n```{.r .cell-code}\nspde <- inla.spde2.matern(mesh = mesh, alpha = 2, constr = TRUE)\nindexs <- inla.spde.make.index(\"s\", spde$n.spde)\nlengths(indexs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      s s.group  s.repl \n    280     280     280 \n```\n:::\n\n```{.r .cell-code}\n# Make the projection matrices\nA <- inla.spde.make.A(mesh = mesh, loc = coo)\nAp <- inla.spde.make.A(mesh = mesh, loc = coop)\ndim(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  52 280\n```\n:::\n\n```{.r .cell-code}\ndim(Ap)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4944  280\n```\n:::\n:::\n\nThe index allows us to extract fitted values from the model, which has a length equal to the number of vertices in the mesh. `s.group` and `s.repl` (replicate) are related to how the dependence structure is specified in the model.\n\nThen we have to make the INLA stack. This is done because we need to combine the data for estimation with the prediction data, as well as the projection matrices. It contains the response data, the list of covariate data--here the temperature and precipitation, the projection matrices, and the indices. Though this is a bit technical, it actually adds a lot of flexibility to `INLA` because we can manipulate the stack based on the data structure that we want. For example, when modelling multiple likelihoods in a single model, these will be combined all within the stack.\n\n::: {.cell}\n\n```{.r .cell-code}\n# stack for estimation stk.e\nstk.e <- inla.stack(tag = \"est\",\ndata = list(y = air_sf_project$Value), A = list(1, A),\neffects = list(data.frame(b0 = rep(1, nrow(A)),\ncovtemp = air_sf_project$covtemp, covprec = air_sf_project$covprec),\ns = indexs))\n#stk.e\n\n# stack for prediction stk.p\nstk.p <- inla.stack(tag = \"pred\",\ndata = list(y = NA), A = list(1, Ap),\neffects = list(data.frame(b0 = rep(1, nrow(Ap)),\ncovtemp = dp$covtemp, covprec = dp$covprec),\ns = indexs))\n#stk.p\n\n# stk.full has stk.e and stk.p\nstk.full <- inla.stack(stk.e, stk.p)\n```\n:::\n\nFinally, we can specify the model in INLA. All the hard work has been done above and at least the model specification in INLA is easier. This model has its mean specified by\n$$\n\\mu_i = \\beta_0 + \\beta_1 \\cdot \\text{temp}_i + \\beta_2 \\cdot \\text{prec}_i + S(x_i),\n$$\nso there is some contribution from fixed effects as well as a latent process modelled as a zero-mean Gaussian Random Field with Matern covariance function. This puts us well within INLA territory. The model equation can be seen quite clearly in the `formula` variable below. We include 0 to remove the intercept that is added by default because we specifically refer to $\\beta_0$ in the stack and then need to do so in the model equation.\n\n::: {.cell}\n\n```{.r .cell-code}\nformula <- y ~ 0 + b0 + covtemp + covprec + f(s, model = spde)\nres <- inla(formula, family = \"gaussian\",\n       data = inla.stack.data(stk.full),\n       control.inla=list(lincomb.derived.correlation.matrix=TRUE),\n       control.predictor = list(compute = TRUE,\n                                A = inla.stack.A(stk.full)),\n       control.compute = list(return.marginals.predictor = TRUE))\n```\n:::\n\nNotice that I am passing a few options to the `INLA` call. Importantly, `control.compute`. We have a nice description of the control options at <https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#sec:controlops>. It controls what quantities are actually computed and returned during the INLA estimation. For example, there are a few different information criteria that it can returned. `control.predictor` will compute the posterior marginals of the parameters. Also useful is `lincomb.derived.correlation.matrix=TRUE` or `control.fixed = list(correlation.matrix=TRUE)` to retrieve the covariance matrix for the fixed effects. All the options can be seen with the command `inla.set.control.compute.default()`\n\nOnce the model is fit, we can inspect the fixed parameters, the estimated latent field, as well as the hyperparameters for the latent field.\n\n::: {.cell}\n\n```{.r .cell-code}\nres$summary.fixed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                mean         sd  0.025quant     0.5quant 0.975quant\nb0      -15.29769449 21.3472625 -57.7595559 -15.17245556 26.4583704\ncovtemp   2.17003514  2.0949918  -1.8002534   2.09513247  6.5392273\ncovprec  -0.01550201  0.2034683  -0.4214077  -0.01404086  0.3818577\n                mode          kld\nb0      -15.17140288 4.819848e-08\ncovtemp   2.09359434 3.231952e-07\ncovprec  -0.01411427 5.717319e-08\n```\n:::\n\n```{.r .cell-code}\n# Latent field is here but it will print out a lot of values\n# res$summary.random$s\nres$summary.hyperpar\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                              mean         sd 0.025quant\nPrecision for the Gaussian observations  0.3076177 0.07974476  0.1774270\nTheta1 for s                             1.9282907 0.53127489  0.9078608\nTheta2 for s                            -4.1344657 0.69219969 -5.5329805\n                                          0.5quant 0.975quant       mode\nPrecision for the Gaussian observations  0.2986888  0.4890943  0.2825065\nTheta1 for s                             1.9197803  2.9993397  1.8830630\nTheta2 for s                            -4.1223427 -2.8081708 -4.0698031\n```\n:::\n:::\n\nThis shows us the relevant statistics for our intercept and parameters for the covariates.\n\nOur predictions at the points in our spatial field have already been made, so we need to extract the estimated values:\n\n::: {.cell}\n\n```{.r .cell-code}\nindex <- inla.stack.index(stack = stk.full, tag = \"pred\")$data\npred_mean <- res$summary.fitted.values[index, \"mean\"]\npred_ll <- res$summary.fitted.values[index, \"0.025quant\"]\npred_ul <- res$summary.fitted.values[index, \"0.975quant\"]\ngrid$mean <- NA\ngrid$ll <- NA\ngrid$ul <- NA\ngrid$mean[indicespointswithin] <- pred_mean\ngrid$ll[indicespointswithin] <- pred_ll\ngrid$ul[indicespointswithin] <- pred_ul\n  \nsummary(grid) # negative values for the lower limit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mean             ll               ul        \n Min.   :1.338   Min.   :-4.722   Min.   : 3.733  \n 1st Qu.:3.626   1st Qu.: 0.359   1st Qu.: 6.530  \n Median :4.665   Median : 1.462   Median : 7.813  \n Mean   :4.595   Mean   : 1.368   Mean   : 7.874  \n 3rd Qu.:5.553   3rd Qu.: 2.405   3rd Qu.: 9.087  \n Max.   :8.316   Max.   : 6.753   Max.   :12.860  \n NA's   :5056    NA's   :5056     NA's   :5056    \n```\n:::\n:::\n\nThen create a plot of the predictions:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rasterVis)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n```{.r .cell-code}\nlevelplot(grid, layout = c(1, 3),\nnames.attr = c(\"Mean\", \"2.5 percentile\", \"97.5 percentile\"))\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/plot-inla-preds-1.png){width=672}\n:::\n:::\n\nThis shows the mean and the 95% credible interval surrounding it.\n\nWhen `compute=TRUE` in `control.predictor`, we can also obtain the quantities below. <https://www.paulamoraga.com/book-geospatial/sec-inla.html> describes this.\n\n::: {.cell}\n\n```{.r .cell-code}\nres$summary.fitted.values\nres$summary.linear.predictor\nmarginals.linear.predictor\nmarginals.fitted.values\n```\n:::\n\nbut it's a lot of output so I won't show it.\n\nFrom <https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#model-fitting-strategies>, we can also see the differences using the different fitting strategies, set by the `control.inla` argument. We can change both the Gaussian approximation strategy for the posterior full conditional distributions, as well as the integration strategy used to integrate out the $\\theta_{-i}$ parameters to get the marginal distribution for $\\theta_i$. The `\"grid\"` option is the most costly, compared to the central composite design (`\"ccd\"`). In the empirical bayes option (`\"eb\"`), the posterior mode is used as the integration point.\n\n\n::: {.cell}\n\n```{.r .cell-code}\napprox_strategy <- c(\"gaussian\", \"simplified.laplace\", \"laplace\")\nint_strategy <- c(\"ccd\", \"grid\", \"eb\")\nmodels <- c(\"iid\", \"matern\")\nfits <- matrix(nrow=length(approx_strategy)*length(int_strategy)*length(models), ncol=3)\nfits_marginals_b <- vector(mode=\"list\", length=length(approx_strategy)*length(int_strategy)*length(models))\nfits_marginals_temp <- vector(mode=\"list\", length=length(approx_strategy)*length(int_strategy)*length(models))\nfits_marginals_prec <- vector(mode=\"list\", length=length(approx_strategy)*length(int_strategy)*length(models))\nindex_f <- 0\nmodel_names <- c()\nfor (m in models){\n  for(a in approx_strategy){\n    for (i in int_strategy){\n      index_f <- index_f + 1\n      if (m==\"matern\"){\n        formula_approx <- y ~ 0 + b0 + covtemp + covprec + f(s, model = spde)\n      }\n      else{\n        formula_approx <- y ~ 0 + b0 + covtemp + covprec + f(s, model = \"iid\")\n      }\n      print(paste(a, \", \", i, \", \", m))\n      model_names <- c(model_names, paste(a, \", \", i, \", \", m))\n      fit_approx <- inla(formula, family = \"gaussian\",\n         data = inla.stack.data(stk.full),\n         control.inla = list(strategy = a, int.strategy = i),\n         control.compute = list(cpo = TRUE, dic = TRUE, waic = TRUE),\n         control.predictor = list(compute = TRUE, A = inla.stack.A(stk.full)))\n      fits[index_f,] <- fit_approx$summary.fixed$mean\n      fits_marginals_b[[index_f]] <- fit_approx$marginals.fixed$b0\n      fits_marginals_temp[[index_f]] <- fit_approx$marginals.fixed$covtemp\n      fits_marginals_prec[[index_f]] <- fit_approx$marginals.fixed$covprec\n  }\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"gaussian ,  ccd ,  iid\"\n[1] \"gaussian ,  grid ,  iid\"\n[1] \"gaussian ,  eb ,  iid\"\n[1] \"simplified.laplace ,  ccd ,  iid\"\n[1] \"simplified.laplace ,  grid ,  iid\"\n[1] \"simplified.laplace ,  eb ,  iid\"\n[1] \"laplace ,  ccd ,  iid\"\n[1] \"laplace ,  grid ,  iid\"\n[1] \"laplace ,  eb ,  iid\"\n[1] \"gaussian ,  ccd ,  matern\"\n[1] \"gaussian ,  grid ,  matern\"\n[1] \"gaussian ,  eb ,  matern\"\n[1] \"simplified.laplace ,  ccd ,  matern\"\n[1] \"simplified.laplace ,  grid ,  matern\"\n[1] \"simplified.laplace ,  eb ,  matern\"\n[1] \"laplace ,  ccd ,  matern\"\n[1] \"laplace ,  grid ,  matern\"\n[1] \"laplace ,  eb ,  matern\"\n```\n:::\n:::\n\nThis prints out all the combinations of approximation, integration strategy and correlation structure.\n\nNow we can compare the means of the parameters per different model set-up.\n\n::: {.cell}\n\n```{.r .cell-code}\nfits_df <- as.data.frame(fits)\nnames(fits_df) <- c(\"b0\", \"covtemp\", \"covprec\")\nfits_df$models <- model_names\nfits_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          b0  covtemp     covprec                               models\n1  -13.85789 2.049883 -0.01973532               gaussian ,  ccd ,  iid\n2  -15.29769 2.170035 -0.01550201              gaussian ,  grid ,  iid\n3  -13.11329 1.992509 -0.02262044                gaussian ,  eb ,  iid\n4  -13.85789 2.049883 -0.01973532     simplified.laplace ,  ccd ,  iid\n5  -15.29769 2.170035 -0.01550201    simplified.laplace ,  grid ,  iid\n6  -13.11329 1.992509 -0.02262044      simplified.laplace ,  eb ,  iid\n7  -13.85789 2.049883 -0.01973532                laplace ,  ccd ,  iid\n8  -15.29769 2.170035 -0.01550201               laplace ,  grid ,  iid\n9  -13.11329 1.992509 -0.02262044                 laplace ,  eb ,  iid\n10 -13.85789 2.049883 -0.01973532            gaussian ,  ccd ,  matern\n11 -15.29769 2.170035 -0.01550201           gaussian ,  grid ,  matern\n12 -13.11329 1.992509 -0.02262044             gaussian ,  eb ,  matern\n13 -13.85789 2.049883 -0.01973532  simplified.laplace ,  ccd ,  matern\n14 -15.29769 2.170035 -0.01550201 simplified.laplace ,  grid ,  matern\n15 -13.11329 1.992509 -0.02262044   simplified.laplace ,  eb ,  matern\n16 -13.85789 2.049883 -0.01973532             laplace ,  ccd ,  matern\n17 -15.29769 2.170035 -0.01550201            laplace ,  grid ,  matern\n18 -13.11329 1.992509 -0.02262044              laplace ,  eb ,  matern\n```\n:::\n:::\n\nIt's also possible to compare the posteriors themselves. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine dataframes\ndf_list_1 <- lapply(fits_marginals_b, function(m) {\n  data.frame(x = m[,1], y = m[,2]) \n})\ndf_list_2 <- lapply(fits_marginals_temp, function(m) {\n  data.frame(x = m[,1], y = m[,2]) \n})\ndf_list_3 <- lapply(fits_marginals_prec, function(m) {\n  data.frame(x = m[,1], y = m[,2]) \n})\nnew_df_1 <- do.call(rbind, df_list_1) \nnew_df_2 <- do.call(rbind, df_list_2) \nnew_df_3 <- do.call(rbind, df_list_3) \n# Add an ID column\nrow_counts <- sapply(fits_marginals_b, nrow)\nids <- factor(rep(1:length(fits_marginals_b), each = row_counts))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in rep(1:length(fits_marginals_b), each = row_counts): first element\nused of 'each' argument\n```\n:::\n\n```{.r .cell-code}\nnew_df_1$id <- ids\nnew_df_2$id <- ids\nnew_df_3$id <- ids\nggplot(new_df_1, aes(x = x, y = y, colour = id)) + \n  geom_line() + ggtitle(\"Posterior of B0\")\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/compare-marginals-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(new_df_2, aes(x = x, y = y, colour = id)) + \n  geom_line() + ggtitle(\"Posterior of Temperature parameter\")\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/compare-marginals-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(new_df_3, aes(x = x, y = y, colour = id)) + \n  geom_line() + ggtitle(\"Posterior of Precipitation parameter\")\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/compare-marginals-3.png){width=672}\n:::\n:::\n\nSo with this data set there is not really much different between estimation methods, regardless of the options used. I suppose that is a good sign?\n\nAnother useful feature is the set of functions that can be used to manipulate the marginal distributions: <https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#sec:marginals>. for example,\n\n::: {.cell}\n\n```{.r .cell-code}\ninla.mmarginal(res$marginals.fixed$b0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -14.90955\n```\n:::\n\n```{.r .cell-code}\nplot(res$marginals.fixed$b0, type = \"l\", xlab = \"b0\", ylab = \"Density\")\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/marginal-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# This should give about the same answer:\nres$summary.fixed$mode[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -15.1714\n```\n:::\n:::\n\n\n## Inlabru\n\nNext, let's write that model in `inlabru` and check if the estimates are the same. We can use the same mesh and spde function as we used before. And also the same formula! But what is especially nice about `inlabru` is that we don't have to set up the stack with the projection matrices. We just model the response variable as a function of the covariates in the dataset we set up earlier and the locations of the observations. In `inlabru` we can use the `sf` dataset object directly. Because of this, it's amazingly simple to fit the model, compared to `INLA`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula <- Value ~ Intercept(1) + covtemp + covprec + f(geometry, model = spde)\n# Fit the model for inlabru\nfit <- bru(formula, data = air_sf_project, family = \"gaussian\")\n# Summarize the results\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninlabru version: 2.12.0\nINLA version: 24.05.01-1\nComponents:\nIntercept: main = linear(1), group = exchangeable(1L), replicate = iid(1L), NULL\ncovtemp: main = linear(covtemp), group = exchangeable(1L), replicate = iid(1L), NULL\ncovprec: main = linear(covprec), group = exchangeable(1L), replicate = iid(1L), NULL\nf: main = spde(geometry), group = exchangeable(1L), replicate = iid(1L), NULL\nLikelihoods:\n  Family: 'gaussian'\n    Tag: ''\n    Data class: 'sf', 'grouped_df', 'tbl_df', 'tbl', 'data.frame'\n    Response class: 'numeric'\n    Predictor: Value ~ .\n    Used components: effects[Intercept, covtemp, covprec, f], latent[]\nTime used:\n    Pre = 1.77, Running = 0.732, Post = 0.347, Total = 2.85 \nFixed effects:\n             mean     sd 0.025quant 0.5quant 0.975quant    mode kld\nIntercept -15.298 21.347    -57.760  -15.172     26.458 -15.171   0\ncovtemp     2.170  2.095     -1.800    2.095      6.539   2.094   0\ncovprec    -0.016  0.203     -0.421   -0.014      0.382  -0.014   0\n\nRandom effects:\n  Name\t  Model\n    f SPDE2 model\n\nModel hyperparameters:\n                                          mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations  0.308 0.080      0.177    0.299\nTheta1 for f                             1.928 0.531      0.908    1.920\nTheta2 for f                            -4.134 0.692     -5.533   -4.122\n                                        0.975quant   mode\nPrecision for the Gaussian observations      0.489  0.283\nTheta1 for f                                 2.999  1.883\nTheta2 for f                                -2.808 -4.070\n\nDeviance Information Criterion (DIC) ...............: 231.65\nDeviance Information Criterion (DIC, saturated) ....: 71.69\nEffective number of parameters .....................: 16.69\n\nWatanabe-Akaike information criterion (WAIC) ...: 232.14\nEffective number of parameters .................: 14.00\n\nMarginal log-Likelihood:  -145.00 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n:::\n:::\n\nFortunately, that gives nearly the same estimates as from  `INLA`  There's also a lot of additional useful output, including the spatial variance $\\tau$ and  spatial range $\\kappa$ for the spatial field, respectively referred to as `theta1` and `theta2` by default (which are a log transformation of the parameters). We can also see the information criteria. We could have gotten this same output from `INLA` with `summary(fit)`.\n\nThen let's plot the predictions. In `inlabru`, we don't have to make predictions at the same time as the model fitting, which is convenient.\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_prj <- st_transform(map, crs = projMercator)\npredictions1 <- predict(fit, newdata=dp, formula = ~ Intercept + covtemp + covprec + f)\npredictions2 <- predict(fit, newdata=dp, formula = ~ f)\nggplot() +\ngeom_sf(data=predictions1, aes(color=mean)) +\n  scale_colour_gradient(low = \"blue\", high = \"yellow\")\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/plot-field-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Check the contribution of just the spatial field\nggplot() +\ngeom_sf(data=predictions2, aes(color=mean)) +\n  scale_colour_gradient(low = \"blue\", high = \"yellow\")\n```\n\n::: {.cell-output-display}\n![](inla_inlabru_tutorial_netherlands_data_files/figure-html/plot-field-2.png){width=672}\n:::\n:::\n\nLooks very nice. Of course, there are a lot more diagnostics we could consider, including looking at the posterior distributions of all parameters and hyperparameters and checking the information criteria. We could also consider the effect of setting priors on the SPDE parameters. This will end up making a difference for more complex models. I could also simulate data and see how well INLA recovers the parameters. There is also the matter of fitting a temporal model in both packages. \n\nBut for now, that concludes this section, showing how to fit spatial model in `INLA` and `inlabru`. The next section shows how to do this for a point process.",
    "supporting": [
      "inla_inlabru_tutorial_netherlands_data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}